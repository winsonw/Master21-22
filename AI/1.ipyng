 5/1:
# Let's import some data.
import numpy as np
import matplotlib.pyplot as plt
 
data_income = np.genfromtxt('data/income_data.csv', delimiter=',') ## Import income data and save to variable.

X = data_income[:, 0] ## Extract an array of the ages from the data.
Y = data_income[:, 1] ## Extract an array of the salaries from the data.

def plot_data(X, Y):
    plt.figure() # create a figure
    plt.scatter(X, Y, c='r') # plot the data in color=red
    plt.xlabel('Age (Years)')
    plt.ylabel('Salary (£K / Year)')
    plt.show()
    
plot_data(ages, salaries)
 5/2:
# Let's import some data.
import numpy as np
import matplotlib.pyplot as plt
 
data_income = np.genfromtxt('data/income_data.csv', delimiter=',') ## Import income data and save to variable.

X = data_income[:, 0] ## Extract an array of the ages from the data.
Y = data_income[:, 1] ## Extract an array of the salaries from the data.

def plot_data(X, Y):
    plt.figure() # create a figure
    plt.scatter(X, Y, c='r') # plot the data in color=red
    plt.xlabel('Age (Years)')
    plt.ylabel('Salary (£K / Year)')
    plt.show()
    
plot_data(ages, salaries)
 5/3:
# Let's import some data.
import numpy as np
import matplotlib.pyplot as plt
 
data_income = np.genfromtxt('data/income_data.csv', delimiter=',') ## Import income data and save to variable.

X = data_income[:, 0] ## Extract an array of the ages from the data.
Y = data_income[:, 1] ## Extract an array of the salaries from the data.

def plot_data(X, Y):
    plt.figure() # create a figure
    plt.scatter(X, Y, c='r') # plot the data in color=red
    plt.xlabel('Age (Years)')
    plt.ylabel('Salary (£K / Year)')
    plt.show()
    
plot_data(X, Y)
 5/4:
class LinearHypothesis:
    def __init__(self): #initalize parameters 
        self.w = np.random.randn() ## randomly initialise weight
        self.b = np.random.randn() ## randomly initialise bias
        
    def __call__(self, X): # how do we calculate output from an input in our model?
        ypred = self.w * X + self.b ## make a prediction
        return ypred # return prediction
    
    def update_params(self, new_w, new_b):
        self.w = new_w ## set this instance's weights to the new weight value passed to the function
        self.b = new_b ## do the same for the bias
 5/5:
H = LinearHypothesis() # instantiate our linear model
y_hat = H(X) # make prediction
print('Input:',X, '\n')
print('W:', H.w, 'B:', H.b, '\n')
print('Prediction:', y_hat, '\n')
 5/6:
H = LinearHypothesis() # instantiate our linear model
y_hat = H(X) # make prediction
print('Input:',X, '\n')
print('W:', H.w, 'B:', H.b, '\n')
print('Prediction:', y_hat, '\n')
 9/1:
atom1 = ('H', (0.00, 0.00, 0.00))
atom2 = ('H', (0.00, 0.00, 0.74))

print(atom1)
print(atom2)
 9/2:
from ase           import Atom  as ASE_Atom_obj
from ase           import Atoms as ASE_Atoms_obj
from ase.visualize import view  as ASE_view

atoms=[]
atoms.append(['H', (0.00, 0.00, 0.00)])
atoms.append(['H', (0.00, 0.00, 0.74)])

print(atoms)
ASE_atoms=[]

for i in range(2):
    ASE_atoms.append(ASE_Atom_obj(atoms[i][0],atoms[i][1]))
print(ASE_atoms)
print(ASE_atoms[0])
print(ASE_atoms[1])
    
mol_H2=ASE_Atoms_obj([ASE_atoms[0],ASE_atoms[1]])

from ase.visualize import view as ASE_view
    ASE_view(mol_H2)


from ase.visualize import view as ASE_view
ASE_view(mol_H2)
 9/3:
from ase           import Atom  as ASE_Atom_obj
from ase           import Atoms as ASE_Atoms_obj
from ase.visualize import view  as ASE_view

atoms=[]
atoms.append(['H', (0.00, 0.00, 0.00)])
atoms.append(['H', (0.00, 0.00, 0.74)])

print(atoms)
ASE_atoms=[]

for i in range(2):
    ASE_atoms.append(ASE_Atom_obj(atoms[i][0],atoms[i][1]))
print(ASE_atoms)
print(ASE_atoms[0])
print(ASE_atoms[1])
    
mol_H2=ASE_Atoms_obj([ASE_atoms[0],ASE_atoms[1]])

from ase.visualize import view as ASE_view
    ASE_view(mol_H2)


from ase.visualize import view as ASE_view
ASE_view(mol_H2)
 9/4:
atom1 = ('H', (0.00, 0.00, 0.00))
atom2 = ('H', (0.00, 0.00, 0.74))

from ase import Atom as ASE_Atom_obj

ASE_atom1 = ASE_Atom_obj(atom1[0],atom1[1])
print(ASE_atom1.symbol)
print(ASE_atom1.position)
print(ASE_atom1.x,ASE_atom1.y, ASE_atom1.z )
print(ASE_atom1.mass)
 9/5:
atoms=[]
atoms.append(['H', (0.00, 0.00, 0.00)])
atoms.append(['H', (0.00, 0.00, 0.74)])

print("The atom 1 is: ",atoms[:][0])
print("The atom 2 is: ",atoms[:][1])
 9/6:
atoms=[]
atoms.append(['H', (0.00, 0.00, 0.00)])
atoms.append(['H', (0.00, 0.00, 0.74)])

for i in range(2):
    print("The atom ", i , " is: ", atoms[:][i])
 9/7:
atoms=[]
atoms.append(['H', (0.00, 0.00, 0.00)])
atoms.append(['H', (0.00, 0.00, 0.74)])

for i in range(2):
    print("The atom", i , " is: ", atoms[:][i])
 9/8:
atoms=[]
atoms.append(['H', (0.00, 0.00, 0.00)])
atoms.append(['H', (0.00, 0.00, 0.74)])

for i in range(2):
    print("The atom", i , "is: ", atoms[:][i])
10/1:  help(ASE_Atom_obj)
10/2:
from ase import Atom as ASE_Atom_obj

atoms=[]
atoms.append(['H', (0.00, 0.00, 0.00)])
atoms.append(['H', (0.00, 0.00, 0.74)])


ASE_atoms=[]

for i in range(2):
    ASE_atoms.append(ASE_Atom_obj(atoms[i][0],atoms[i][1])) 
    print(ASE_atoms[i].symbol)
    print(ASE_atoms[i].position)
    print(ASE_atoms[i].x,ASE_atoms[i].y, ASE_atoms[i].z )
    print(ASE_atoms[i].mass)
14/1:
atom1 = ('H', (0.00, 0.00, 0.00))
atom2 = ('H', (0.00, 0.00, 0.74))

from ase import Atom as ASE_Atom_obj

ASE_atom1 = ASE_Atom_obj(atom1[0],atom1[1])
print(ASE_atom1.symbol)
print(ASE_atom1.position)
print(ASE_atom1.x,ASE_atom1.y, ASE_atom1.z )
print(ASE_atom1.mass)

ASE_atom2 = ASE_Atom_obj(atom2[0],atom2[1])
print(ASE_atom2.symbol)
print(ASE_atom2.position)
print(ASE_atom2.x,ASE_atom1.y, ASE_atom2.z )
print(ASE_atom2.mass)
14/2:  help(ASE_Atom_obj)
14/3:  help(ASE_Atom_obj)
14/4:
from ase import Atom as ASE_Atom_obj

atoms=[]
atoms.append(['H', (0.00, 0.00, 0.00)])
atoms.append(['H', (0.00, 0.00, 0.74)])


ASE_atoms=[]

for i in range(2):
    ASE_atoms.append(ASE_Atom_obj(atoms[i][0],atoms[i][1])) 
    print(ASE_atoms[i].symbol)
    print(ASE_atoms[i].position)
    print(ASE_atoms[i].x,ASE_atoms[i].y, ASE_atoms[i].z )
    print(ASE_atoms[i].mass)
14/5:
from ase           import Atom  as ASE_Atom_obj
from ase           import Atoms as ASE_Atoms_obj
from ase.visualize import view  as ASE_view

atoms=[]
atoms.append(['H', (0.00, 0.00, 0.00)])
atoms.append(['H', (0.00, 0.00, 0.74)])

print(atoms)
ASE_atoms=[]

for i in range(2):
    ASE_atoms.append(ASE_Atom_obj(atoms[i][0],atoms[i][1]))
print(ASE_atoms)
print(ASE_atoms[0])
print(ASE_atoms[1])
    
mol_H2=ASE_Atoms_obj([ASE_atoms[0],ASE_atoms[1]])

from ase.visualize import view as ASE_view
    ASE_view(mol_H2)
14/6:
from ase           import Atom  as ASE_Atom_obj
from ase           import Atoms as ASE_Atoms_obj
from ase.visualize import view  as ASE_view

atoms=[]
atoms.append(['H', (0.00, 0.00, 0.00)])
atoms.append(['H', (0.00, 0.00, 0.74)])

print(atoms)
ASE_atoms=[]

for i in range(2):
    ASE_atoms.append(ASE_Atom_obj(atoms[i][0],atoms[i][1]))
print(ASE_atoms)
print(ASE_atoms[0])
print(ASE_atoms[1])
    
mol_H2=ASE_Atoms_obj([ASE_atoms[0],ASE_atoms[1]])

from ase.visualize import view as ASE_view
ASE_view(mol_H2)
14/7:
from ase           import Atom  as ASE_Atom_obj
from ase           import Atoms as ASE_Atoms_obj
from ase.visualize import view  as ASE_view

atoms=[]
atoms.append(['H', (0.00, 0.00, 0.00)])
atoms.append(['H', (0.00, 0.00, 0.74)])

print(atoms)
ASE_atoms=[]

for i in range(2):
    ASE_atoms.append(ASE_Atom_obj(atoms[i][0],atoms[i][1]))
print(ASE_atoms)
print(ASE_atoms[0])
print(ASE_atoms[1])
    
mol_H2=ASE_Atoms_obj([ASE_atoms[0],ASE_atoms[1]])

from ase.visualize import view as ASE_view
ASE_view(mol_H2,viewer='x3d')
13/1:
from ase.build import molecule as ASE_Molecule_builder

ASE_mol_H2 = ASE_Molecule_builder('H2')
13/2:
from ase.build import molecule as ASE_Molecule_builder

ASE_mol_H2 = ASE_Molecule_builder('H2')

print("The coordinates are:", ASE_mol_H2.get_positions())
print("The atomic numbers are:", ASE_mol_H2.get_atomic_numbers())
13/3: print("The coordinates of the center of mass are:", ASE_Molecule_builder('H2').get_center_of_mass())
13/4: print("The coordinates of the center of mass are:", ASE_mol_H2.get_center_of_mass())
13/5: print("The coordinates of the center of mass are:", ASE_mol_H2.get_center_of_mass())
13/6: print("The distance between atom 0 and atom 1 is:" ASE_mol_H2.get_distance(0,1))
13/7: print("The distance between atom 0 and atom 1 is:", ASE_mol_H2.get_distance(0,1))
13/8:
from ase.build import molecule as ASE_Molecule_builder
ASE_mol_H2 = ASE_Molecule_builder('H2')

from ase.visualize import view as ASE_view
ASE_view(ASE_mol_H2)
13/9:
from ase.build import molecule as ASE_Molecule_builder

ASE_mol_HF = ASE_Molecule_builder('HF')
print("The coordinates are:", ASE_mol_H2.get_positions())
print("The distance between atom 0 and atom 1 is:", ASE_mol_HF.get_distance(0,1))
print("The coordinates of the center of mass are:", ASE_mol_HF.get_center_of_mass())
13/10:
from ase.build import molecule as ASE_Molecule_builder

ASE_mol_HF = ASE_Molecule_builder('HF')
print("The coordinates are:", ASE_mol_HF.get_positions())
print("The distance between atom 0 and atom 1 is:", ASE_mol_HF.get_distance(0,1))
print("The coordinates of the center of mass are:", ASE_mol_HF.get_center_of_mass())
13/11:
from ase.build import molecule as ASE_Molecule_builder

ASE_mol_H2O = ASE_Molecule_builder('H2O')
print("The coordinates are:", ASE_mol_H2O.get_positions())
print("The distance between atom 0 and atom 1 is:", ASE_mol_H2O.get_distance(0,1))
13/12:
from ase.build import molecule as ASE_Molecule_builder

ASE_mol_H2O = ASE_Molecule_builder('H2O')
print("The distance between atom 0 and atom 1 is:", ASE_mol_H2O.get_distance())
13/13:
from ase.build import molecule as ASE_Molecule_builder

ASE_mol_H2O = ASE_Molecule_builder('H2O')
print("The distance between atom 0 and atom 1 is:", ASE_mol_H2O.get_distance(0,1))
print("The distance between atom 0 and atom 1 is:", ASE_mol_H2O.get_distance(1,2))
print("The distance between atom 0 and atom 1 is:", ASE_mol_H2O.get_distance(0,2))
13/14:
from ase.build import molecule as ASE_Molecule_builder

ASE_mol_H2O = ASE_Molecule_builder('H2O')
print("The distance between atom 0 and atom 1 is:", ASE_mol_H2O.get_distance(0,1))
print("The distance between atom 1 and atom 2 is:", ASE_mol_H2O.get_distance(1,2))
print("The distance between atom 0 and atom 2 is:", ASE_mol_H2O.get_distance(0,2))
13/15:
from ase.build import molecule as ASE_Molecule_builder

ASE_mol_H2O = ASE_Molecule_builder('H2O')
print("The distance between atom 0 and atom 1 is:", ASE_mol_H2O.get_distance(0,1))
print("The distance between atom 1 and atom 2 is:", ASE_mol_H2O.get_distance(1,2))
print("The distance between atom 0 and atom 2 is:", ASE_mol_H2O.get_distance(0,2))
print(ASE_mol_H2O.get_angle(1,0,2))
13/16:
from ase.build import molecule as ASE_Molecule_builder

ASE_mol_H2O = ASE_Molecule_builder('H2O')
print("The distance between atom 0 and atom 1 is:", ASE_mol_H2O.get_distance(0,1))
print("The distance between atom 1 and atom 2 is:", ASE_mol_H2O.get_distance(1,2))
print("The distance between atom 0 and atom 2 is:", ASE_mol_H2O.get_distance(0,2))
print(ASE_mol_H2O.get_angle(1,0,2))

print(ASE_mol_H20.get_center_of_mass)
13/17:
from ase.build import molecule as ASE_Molecule_builder

ASE_mol_H2O = ASE_Molecule_builder('H2O')
print("The distance between atom 0 and atom 1 is:", ASE_mol_H2O.get_distance(0,1))
print("The distance between atom 1 and atom 2 is:", ASE_mol_H2O.get_distance(1,2))
print("The distance between atom 0 and atom 2 is:", ASE_mol_H2O.get_distance(0,2))
print(ASE_mol_H2O.get_angle(1,0,2))

print(ASE_mol_H2O.get_center_of_mass)
13/18:
from ase.build import molecule as ASE_Molecule_builder

ASE_mol_H2O = ASE_Molecule_builder('H2O')
print("The distance between atom 0 and atom 1 is:", ASE_mol_H2O.get_distance(0,1))
print("The distance between atom 1 and atom 2 is:", ASE_mol_H2O.get_distance(1,2))
print("The distance between atom 0 and atom 2 is:", ASE_mol_H2O.get_distance(0,2))
print(ASE_mol_H2O.get_angle(1,0,2))

print(ASE_mol_H2O.get_center_of_mass())
13/19:
ASE_mol_NH3 = ASE_Molecule_builder('NH3')
print(ASE_mol_NH3.get_atomic_numbers())
13/20:
ASE_mol_NH3 = ASE_Molecule_builder('NH3')
print(ASE_mol_NH3.get_angle(1,0,2))
13/21:
ASE_mol_NH3 = ASE_Molecule_builder('NH3')
print(ASE_mol_NH3.get_angle(1,0,2))
print(ASE_mol_NH3.get_angle(1,0,2))

print(ASE_mol_NH3.get_center_of_mass())
13/22:
ASE_mol_NH3 = ASE_Molecule_builder('NH3')
print(ASE_mol_NH3.get_angle(1,0,2))
print(ASE_mol_NH3.get_angle(1,0,2))
13/23:
ASE_mol_PH3 = ASE_Molecule_builder('PH3')
print(ASE_mol_PH3.get_angle(1,0,2))
15/1:
from ase.build import molecule as ASE_Molecule_builder

ASE_mol_H2 = ASE_Molecule_builder('H2')

print("The coordinates are:", ASE_mol_H2.get_positions())
print("The atomic numbers are:", ASE_mol_H2.get_atomic_numbers())
15/2:
from pyscf import gto 

#OPTIONAL: uncomment this line to find the python type.
#type(gto)
16/1:
from pyscf import gto 

#OPTIONAL: uncomment this line to find the python type.
#type(gto)
16/2:
def ASE_geom_to_pyscf_geom(ASE_geom):
    return [[atom.symbol,atom.position] for atom in ASE_geom]


pyscf_mol_H2 = ASE_geom_to_pyscf_geom(ASE_mol_H2)
16/3:
def ASE_geom_to_pyscf_geom(ASE_geom):
    return [[atom.symbol,atom.position] for atom in ASE_geom]


pyscf_mol_H2 = ASE_geom_to_pyscf_geom(ASE_Molecule_builder('H2'))
16/4:
def ASE_geom_to_pyscf_geom(ASE_geom):
    return [[atom.symbol,atom.position] for atom in ASE_geom]

from ase.build import molecule as ASE_Molecule_builder

ASE_mol_H2 = ASE_Molecule_builder('H2')

pyscf_mol_H2 = ASE_geom_to_pyscf_geom(ASE_mol_H2)
16/5:
def ASE_geom_to_pyscf_geom(ASE_geom):
    return [[atom.symbol,atom.position] for atom in ASE_geom]

from ase.build import molecule as ASE_Molecule_builder

ASE_mol_H2 = ASE_Molecule_builder('H2')

pyscf_mol_H2 = ASE_geom_to_pyscf_geom(ASE_mol_H2)

print(pyscf_mol_H2)
16/6:
#basis set
mol.basis = "sto-3g"
print("pyscf molecule module set: basis set selected")
16/7:
#geometry
#Add code
mol.atom = pyscf_mol_H2
16/8: mol = gto.Mole()
16/9:
#geometry
#Add code
mol.atom = pyscf_mol_H2
16/10:
#basis set
mol.basis = "sto-3g"
print("pyscf molecule module set: basis set selected")
16/11:
#building mol
mol.build()
print(mol.atom_coords())
print(mol.atom_coords(unit="Angstrom"))
16/12:
#Hamiltonian
### mean field class in pyscf
#Add code 
from pyscf import scf
mf_class=scf.RHF(mol)  
print("pyscf mean field module: hamiltonian selected")
16/13:
#Add code
#execution
E_0=mf_class.run() 
or
E_0=mf_class.kernel()
16/14:
#Add code
#execution
E_0=mf_class.kernel()
22/1: torch.__version__
22/2:
# These are the libraries will be used for this lab.

import torch 
import numpy as np 
import pandas as pd

import matplotlib.pyplot as plt
%matplotlib inline
24/1:
print('PyDev console: using IPython 7.13.0\n')

import sys; print('Python %s on %s' % (sys.version, sys.platform))
sys.path.extend(['/Users/winson/Library/Mobile Documents/com~apple~CloudDocs/2020summer/CASIA/week3/pytorch-dqn-master'])
26/1:
print('PyDev console: using IPython 7.12.0\n')

import sys; print('Python %s on %s' % (sys.version, sys.platform))
sys.path.extend(['/Users/winson/Desktop/pytorch-dqn-master_rl_large'])
26/2: runfile('/Users/winson/Desktop/pytorch-dqn-master_rl_large/ram.py', wdir='/Users/winson/Desktop/pytorch-dqn-master_rl_large')
27/1:
print('PyDev console: using IPython 7.12.0\n')

import sys; print('Python %s on %s' % (sys.version, sys.platform))
sys.path.extend(['/Users/winson/Desktop/pytorch-dqn-master'])
27/2: runfile('/Users/winson/Desktop/pytorch-dqn-master/ram.py', wdir='/Users/winson/Desktop/pytorch-dqn-master')
28/1:
print('PyDev console: using IPython 7.12.0\n')

import sys; print('Python %s on %s' % (sys.version, sys.platform))
sys.path.extend(['/Users/winson/Desktop/pytorch-dqn-master'])
28/2: runfile('/Users/winson/Desktop/pytorch-dqn-master/main.py', wdir='/Users/winson/Desktop/pytorch-dqn-master')
29/1:
print('PyDev console: using IPython 7.12.0\n')

import sys; print('Python %s on %s' % (sys.version, sys.platform))
sys.path.extend(['/Users/winson/Desktop/pytorch-dqn-master'])
29/2: runfile('/Users/winson/Desktop/pytorch-dqn-master/main.py', wdir='/Users/winson/Desktop/pytorch-dqn-master')
30/1:
print('PyDev console: using IPython 7.12.0\n')

import sys; print('Python %s on %s' % (sys.version, sys.platform))
sys.path.extend(['/Users/winson/Desktop/pytorch-dqn-master'])
31/1:
print('PyDev console: using IPython 7.12.0\n')

import sys; print('Python %s on %s' % (sys.version, sys.platform))
sys.path.extend(['/Users/winson/Desktop/pytorch-dqn-master'])
32/1:
print('PyDev console: using IPython 7.12.0\n')

import sys; print('Python %s on %s' % (sys.version, sys.platform))
sys.path.extend(['/Users/winson/Desktop/summer_dqn/pytorch-dqn-master'])
32/2: runfile('/Users/winson/Desktop/summer_dqn/pytorch-dqn-master/main.py', wdir='/Users/winson/Desktop/summer_dqn/pytorch-dqn-master')
33/1:
print('PyDev console: using IPython 7.12.0\n')

import sys; print('Python %s on %s' % (sys.version, sys.platform))
sys.path.extend(['/Users/winson/Desktop/summer_dqn/pytorch-dqn-master'])
33/2: runfile('/Users/winson/Desktop/summer_dqn/pytorch-dqn-master/main.py', wdir='/Users/winson/Desktop/summer_dqn/pytorch-dqn-master')
34/1:
print('PyDev console: using IPython 7.12.0\n')

import sys; print('Python %s on %s' % (sys.version, sys.platform))
sys.path.extend(['/Users/winson/Desktop/summer_dqn/pytorch-dqn-master'])
34/2: runfile('/Users/winson/Desktop/summer_dqn/pytorch-dqn-master/main.py', wdir='/Users/winson/Desktop/summer_dqn/pytorch-dqn-master')
35/1:
print('PyDev console: using IPython 7.12.0\n')

import sys; print('Python %s on %s' % (sys.version, sys.platform))
sys.path.extend(['/Users/winson/Desktop/summer_dqn/pytorch-dqn-master'])
35/2: runfile('/Users/winson/Desktop/summer_dqn/pytorch-dqn-master/main.py', wdir='/Users/winson/Desktop/summer_dqn/pytorch-dqn-master')
36/1:
print('PyDev console: using IPython 7.12.0\n')

import sys; print('Python %s on %s' % (sys.version, sys.platform))
sys.path.extend(['/Users/winson/Desktop/summer_dqn/pytorch-dqn-master'])
36/2: runfile('/Users/winson/Desktop/summer_dqn/pytorch-dqn-master/main.py', wdir='/Users/winson/Desktop/summer_dqn/pytorch-dqn-master')
37/1:
print('PyDev console: using IPython 7.12.0\n')

import sys; print('Python %s on %s' % (sys.version, sys.platform))
sys.path.extend(['/Users/winson/Desktop/summer_dqn/pytorch-dqn-master'])
37/2: runfile('/Users/winson/Desktop/summer_dqn/pytorch-dqn-master/main.py', wdir='/Users/winson/Desktop/summer_dqn/pytorch-dqn-master')
38/1:
print('PyDev console: using IPython 7.12.0\n')

import sys; print('Python %s on %s' % (sys.version, sys.platform))
sys.path.extend(['/Users/winson/Desktop/summer_dqn/pytorch-dqn-master'])
39/1:
print('PyDev console: using IPython 7.12.0\n')

import sys; print('Python %s on %s' % (sys.version, sys.platform))
sys.path.extend(['/Users/winson/Desktop/summer_dqn/pytorch-dqn-master'])
39/2: runfile('/Users/winson/Desktop/summer_dqn/pytorch-dqn-master/main.py', wdir='/Users/winson/Desktop/summer_dqn/pytorch-dqn-master')
40/1:
print('PyDev console: using IPython 7.12.0\n')

import sys; print('Python %s on %s' % (sys.version, sys.platform))
sys.path.extend(['/Users/winson/Desktop/summer_dqn/pytorch-dqn-master'])
40/2: runfile('/Users/winson/Desktop/summer_dqn/pytorch-dqn-master/main.py', wdir='/Users/winson/Desktop/summer_dqn/pytorch-dqn-master')
42/1: conda install pyserial
42/2:
import serial
import serial.tools.list_ports
import time
%pylab inline
from scipy.optimize import curve_fit
42/3:
ports = serial.tools.list_ports.comports()

for p in ports:
    print(p.device)
print (len(ports), 'ports found')
42/4: ser = serial.Serial('/dev/cu.usbmodem14101', baudrate=9600, timeout=2)
44/1:
def number_of_vowels( string ):
    vowel_count = 0
    for c in string:
        if c in "aeiouAEIOU":
            vowel_count += 1
    return vowel_count
44/2:
def number_of_vowels( string ):
    vowel_count = 0
    for c in string:
        if c in "aeiouAEIOU":
            vowel_count += 1
    return vowel_count
44/3:
# Run this cell to test number_of_vowels function
# The A0_Warm_Up_tests.py program must be in the same folder as this file.
from A0_Warm_Up_tests import do_tests
do_tests(number_of_vowels)
44/4:
## Edit this cell to give your answer for Q1

def number_of_distinct_vowels( string ):
    vowels = set()
    for c in string.upper():
        if c in "aeiouAEIOU":
            vowels.add(c)
    return len(vowels)
44/5:
# Run this cell to test the number_of_distinct_vowels function
# The A0_Warm_Up_tests.py program must be in the same folder as this file.
from A0_Warm_Up_tests import do_tests
do_tests(number_of_distinct_vowels)
44/6:
## Edit this cell to give your answer for Q2

def password_strength( password ):
    if len(password) < 8:
        strength = "WEAK"
    else:
    return strength  # should return the strength ("WEAK", "STRONG" or "MEDIUM")
44/7:
## Edit this cell to give your answer for Q2

def password_strength( password ):
    if len(password) < 8:
        strength = "WEAK"
    else:
        strength = "MEDIUM"
        
        lower = 0
        upper = 0
        number = 0
        if len(password) >= 11:
    return strength  # should return the strength ("WEAK", "STRONG" or "MEDIUM")
44/8:
## Edit this cell to give your answer for Q2

def password_strength( password ):
    if len(password) < 8:
        strength = "WEAK"
    else:
        strength = "MEDIUM"
        lower = 0
        upper = 0
        number = 0
        if len(password) >= 11:
            for c in password:
                if c.isdigit():
                    number += 1
                elif c.isupper():
                    upper += 1
                elif c.islower():
                    lower += 1
            if number >= 1 and upper >= 1 and lower >= 1:
                strength = "STRONG"
    return strength  # should return the strength ("WEAK", "STRONG" or "MEDIUM")
44/9:
# code to test the password_strength function
# The A0_Warm_Up_tests.py program must be in the same folder as this file.
from A0_Warm_Up_tests import do_tests
do_tests( password_strength )
44/10:
## Edit this cell to give your answer for Q3

def megabyte_bars_cost( n ):
    six = n // 6
    remine = n % 6
    cost = 5 * six + 1.25 * remine
    if cost >= 20 :
        cost *= 0.9
    return cost  ## change this to return the cost of bying n bars.
44/11:
# Run this cell to test the megabyte_bars_cost function
# The A0_Warm_Up_tests.py program must be in the same folder as this file.
from A0_Warm_Up_tests import do_tests
do_tests(megabyte_bars_cost)
44/12:
from A0_Warm_Up_tests import do_tests, tests_version

functions = [
              number_of_distinct_vowels,
              password_strength,
              megabyte_bars_cost
            ]

marks = 0
max_marks = 0
for function in functions:
    mark, max = do_tests(function)
    marks += mark
    max_marks += max
    
print("\nFINAL TOTAL =", marks, " (of", str(max_marks) + ")")
print("\nTests version:", tests_version())
45/1:
## Q2(a) answer code cell
## Modify this function definition to fulfill the given requirements.
## Expected code length: less than 10 lines.

def anagrams( s1, s2 ):
    if len(s1) == len(s2):
        c_s1 = 0
        c_s2 = 0
        for index in len(s1)
            if s1[index].isupper():
                c_s1 += 1
            if s2[index].isupper():
                c_s2 += 1
        s_s1 = sorted(s1)
        s_s2 = sorted(s2)
        return s_s1 == s_s2
    return False
45/2:
from A1_P4DS2_Python_Programming_tests import do_tests, do_all_tests, tests_version

tests_version()
45/3:
## Q2(a) answer code cell
## Modify this function definition to fulfill the given requirements.
## Expected code length: less than 10 lines.

def anagrams( s1, s2 ):
    if len(s1) == len(s2):
        c_s1 = 0
        c_s2 = 0
        for index in len(s1):
            if s1[index].isupper():
                c_s1 += 1
            if s2[index].isupper():
                c_s2 += 1
        s_s1 = sorted(s1)
        s_s2 = sorted(s2)
        return s_s1 == s_s2
    return False
45/4:
# Run this cell to test your is_english_word function
# The testing module have been imported (see above)
# You must also have run the cell with your anagrams definition.
do_tests( anagrams )
45/5:
## Q2(a) answer code cell
## Modify this function definition to fulfill the given requirements.
## Expected code length: less than 10 lines.

def anagrams( s1, s2 ):
    if len(s1) == len(s2):
        c_s1 = 0
        c_s2 = 0
        for index in range(len(s1)):
            if s1[index].isupper():
                c_s1 += 1
            if s2[index].isupper():
                c_s2 += 1
        s_s1 = sorted(s1)
        s_s2 = sorted(s2)
        return s_s1 == s_s2
    return False
45/6:
# Run this cell to test your is_english_word function
# The testing module have been imported (see above)
# You must also have run the cell with your anagrams definition.
do_tests( anagrams )
45/7:
## Q2(a) answer code cell
## Modify this function definition to fulfill the given requirements.
## Expected code length: less than 10 lines.

def anagrams( s1, s2 ):
    if len(s1) == len(s2):
        c_s1 = 0
        c_s2 = 0
        for index in range(len(s1)):
            if s1[index].isupper():
                c_s1 += 1
            if s2[index].isupper():
                c_s2 += 1
        s_s1 = sorted(s1.upper())
        s_s2 = sorted(s2.upper())
        
        return (c_s1 == c_s2 and s_s1 == s_s2)
    return False
45/8:
# Run this cell to test your is_english_word function
# The testing module have been imported (see above)
# You must also have run the cell with your anagrams definition.
do_tests( anagrams )
45/9:
## Q2(c) answer code cell
## Modify this function definition to fulfill the given requirements
## Expected code length: less than 10 lines.

def is_palindrome(s):
    f_index = 0
    b_index = len(s)
    while f_index < b_index:
        if ! s[f_index].isalpha():
            f_index += 1
        elif !s[b_index].isaphla():
            b_index -= 1
        elif s[f_index] != s[b_index]:
    return True
45/10:
## Q2(c) answer code cell
## Modify this function definition to fulfill the given requirements
## Expected code length: less than 10 lines.

def is_palindrome(s):
    f_index = 0
    b_index = len(s)
    while f_index < b_index:
        if ! s[f_index].isalpha():
            f_index += 1
        elif !s[b_index].isaphla():
            b_index -= 1
        elif s[f_index] != s[b_index]:
            return False
    return True
45/11:
## Q2(c) answer code cell
## Modify this function definition to fulfill the given requirements
## Expected code length: less than 10 lines.

def is_palindrome(s):
    f_index = 0
    b_index = len(s)
    while f_index < b_index:
        if !s[f_index].isalpha():
            f_index += 1
        elif !s[b_index].isaphla():
            b_index -= 1
        elif s[f_index] != s[b_index]:
            return False
    return True
45/12:
## Q2(c) answer code cell
## Modify this function definition to fulfill the given requirements
## Expected code length: less than 10 lines.

def is_palindrome(s):
    f_index = 0
    b_index = len(s)
    while f_index < b_index:
        if not s[f_index].isalpha():
            f_index += 1
        elif not s[b_index].isaphla():
            b_index -= 1
        elif s[f_index] != s[b_index]:
            return False
    return True
45/13:
# Run this cell to test your is_palindrome function
# The tests module must have been imported (see above).
do_tests(is_palindrome)
45/14:
## Q2(c) answer code cell
## Modify this function definition to fulfill the given requirements
## Expected code length: less than 10 lines.

def is_palindrome(s):
    f_index = 0
    b_index = len(s) - 1
    while f_index < b_index:
        if not s[f_index].isalpha():
            f_index += 1
        elif not s[b_index].isaphla():
            b_index -= 1
        elif s[f_index] != s[b_index]:
            return False
    return True
45/15:
## Q2(c) answer code cell
## Modify this function definition to fulfill the given requirements
## Expected code length: less than 10 lines.

def is_palindrome(s):
    f_index = 0
    b_index = len(s) - 1
    while f_index < b_index:
        if not s[f_index].isalpha():
            f_index += 1
        elif not s[b_index].isaphla():
            b_index -= 1
        elif s[f_index] != s[b_index]:
            return False
    return True
45/16:
# Run this cell to test your is_palindrome function
# The tests module must have been imported (see above).
do_tests(is_palindrome)
45/17:
## Q2(c) answer code cell
## Modify this function definition to fulfill the given requirements
## Expected code length: less than 10 lines.

def is_palindrome(s):
    f_index = 0
    b_index = len(s) - 1
    while f_index < b_index:
        if not s[f_index].isalpha():
            f_index += 1
        elif not s[b_index].isalpha():
            b_index -= 1
        elif s[f_index] != s[b_index]:
            return False
    return True
45/18:
# Run this cell to test your is_palindrome function
# The tests module must have been imported (see above).
do_tests(is_palindrome)
45/19:
## Q2(c) answer code cell
## Modify this function definition to fulfill the given requirements
## Expected code length: less than 10 lines.

def is_palindrome(s):
    f_index = 0
    b_index = len(s) - 1
    s = s.lower()
    while f_index < b_index:
        if not s[f_index].isalpha():
            f_index += 1
        elif not s[b_index].isalpha():
            b_index -= 1
        elif s[f_index] != s[b_index]:
            return False
    return True
45/20:
# Run this cell to test your is_palindrome function
# The tests module must have been imported (see above).
do_tests(is_palindrome)
46/1:
## Q2(c) answer code cell
## Modify this function definition to fulfill the given requirements
## Expected code length: less than 10 lines.

def is_palindrome(s):
    f_index = 0
    b_index = len(s) - 1
    s = s.lower()
    while f_index < b_index:
        if not s[f_index].isalpha():
            f_index += 1
        elif not s[b_index].isalpha():
            b_index -= 1
        elif s[f_index] != s[b_index]:
            return False
    return True
46/2:
# Run this cell to test your is_palindrome function
# The tests module must have been imported (see above).
do_tests(is_palindrome)
46/3:
from A1_P4DS2_Python_Programming_tests import do_tests, do_all_tests, tests_version

tests_version()
46/4:
## Q2(a) answer code cell
## Modify this function definition to fulfill the given requirements.
## Expected code length: less than 10 lines.

def anagrams( s1, s2 ):
    if len(s1) == len(s2):
        c_s1 = 0
        c_s2 = 0
        for index in range(len(s1)):
            if s1[index].isupper():
                c_s1 += 1
            if s2[index].isupper():
                c_s2 += 1
        s_s1 = sorted(s1.upper())
        s_s2 = sorted(s2.upper())
        
        return (c_s1 == c_s2 and s_s1 == s_s2)
    return False
46/5:
# Run this cell to test your is_english_word function
# The testing module have been imported (see above)
# You must also have run the cell with your anagrams definition.
do_tests( anagrams )
46/6:
## Q2(c) answer code cell
## Modify this function definition to fulfill the given requirements
## Expected code length: less than 10 lines.

def is_palindrome(s):
    f_index = 0
    b_index = len(s) - 1
    s = s.lower()
    while f_index < b_index:
        if not s[f_index].isalpha():
            f_index += 1
        elif not s[b_index].isalpha():
            b_index -= 1
        elif s[f_index] != s[b_index]:
            return False
    return True
46/7:
# Run this cell to test your is_palindrome function
# The tests module must have been imported (see above).
do_tests(is_palindrome)
46/8:
from A1_P4DS2_Python_Programming_tests import do_tests, do_all_tests, tests_version

tests_version()
47/1:
from A1_P4DS2_Python_Programming_tests import do_tests, do_all_tests, tests_version

tests_version()
47/2:
## Q2(a) answer code cell
## Modify this function definition to fulfill the given requirements.
## Expected code length: less than 10 lines.

def anagrams( s1, s2 ):
    if len(s1) == len(s2):
        c_s1 = 0
        c_s2 = 0
        for index in range(len(s1)):
            if s1[index].isupper():
                c_s1 += 1
            if s2[index].isupper():
                c_s2 += 1
        s_s1 = sorted(s1.upper())
        s_s2 = sorted(s2.upper())
        
        return (c_s1 == c_s2 and s_s1 == s_s2)
    return False
47/3:
# Run this cell to test your is_english_word function
# The testing module have been imported (see above)
# You must also have run the cell with your anagrams definition.
do_tests( anagrams )
47/4:
## Q2(c) answer code cell
## Modify this function definition to fulfill the given requirements
## Expected code length: less than 10 lines.

def is_palindrome(s):
    f_index = 0
    b_index = len(s) - 1
    s = s.lower()
    while f_index < b_index:
        if not s[f_index].isalpha():
            f_index += 1
        elif not s[b_index].isalpha():
            b_index -= 1
        elif s[f_index] != s[b_index]:
            return False
    return True
47/5:
# Run this cell to test your is_palindrome function
# The tests module must have been imported (see above).
do_tests(is_palindrome)
47/6:
## Q2(c) answer code cell
## Modify this function definition to fulfill the given requirements
## Expected code length: less than 10 lines.

def is_palindrome(s):
    f_index = 0
    b_index = len(s) - 1
    s = s.lower()
    while f_index < b_index:
        if not s[f_index].isalpha():
            f_index += 1
        elif not s[b_index].isalpha():
            b_index -= 1
        elif s[f_index] != s[b_index]:
            return False
        else :
            f_index += 1
            b_index -= 1
    return True
47/7:
# Run this cell to test your is_palindrome function
# The tests module must have been imported (see above).
do_tests(is_palindrome)
47/8:
def get_english_words():
    with open( "english_words.txt" ) as f:
         words = f.readlines()
    words = { word.strip() for
             word in words }
    return words

ENGLISH_WORDS = get_english_words()
47/9:
## Modify this function definition to fulfill the given requirements.
## Expected code length: less than 15 lines.

def is_english_word(s):
    plain = s.lower()
    if plain == s or plain.title() == s or plain.upper() == s:
        if plain in ENGLISH_WORDS:
            return True
    return False
47/10:
# Run this cell to test your is_english_word function
# The testing module must have been imported (see above)
do_tests(is_english_word)
47/11:
def get_english_words():
    with open( "english_words.txt" ) as f:
         words = f.readlines()
    words = { word.strip() for
             word in words }
    return words

ENGLISH_WORDS = get_english_words()
print(len(ENGLISH_WORDS))
47/12:
## Q2(b) answer code cell
## Modify this function definition to fulfill the given requirements.
## Expected code length: less than 5 lines.

def find_all_anagrams(s):
    return [p for p in ENGLISH_WORDS if anagrams(s,p)]
47/13:
# Run this cell to test your find_all_anagrams function
# The testing module must have been imported (see above)
do_tests( find_all_anagrams )
47/14:
## Q2(b) answer code cell
## Modify this function definition to fulfill the given requirements.
## Expected code length: less than 5 lines.

def find_all_anagrams(s):
    return [p for p in ENGLISH_WORDS if anagrams(s,p) and s != p]
47/15:
# Run this cell to test your find_all_anagrams function
# The testing module must have been imported (see above)
do_tests( find_all_anagrams )
47/16:
## Q2(b) answer code cell
## Modify this function definition to fulfill the given requirements.
## Expected code length: less than 5 lines.

def find_all_anagrams(s):
    if is_english_word(s):
        s = s.lower()
        return [p for p in ENGLISH_WORDS if anagrams(s,p) and s != p]
    return []
47/17:
# Run this cell to test your find_all_anagrams function
# The testing module must have been imported (see above)
do_tests( find_all_anagrams )
47/18:
## Q2(b) answer code cell
## Modify this function definition to fulfill the given requirements.
## Expected code length: less than 5 lines.

def find_all_anagrams(s):
    l = []
    s = s.lower()
    l = [p for p in ENGLISH_WORDS if anagrams(s,p) and s != p]
    return l
47/19:
# Run this cell to test your find_all_anagrams function
# The testing module must have been imported (see above)
do_tests( find_all_anagrams )
47/20:
## Q2(b) answer code cell
## Modify this function definition to fulfill the given requirements.
## Expected code length: less than 5 lines.

def find_all_anagrams(s):
    l = []
    s = s.lower()
    l = [p for p in ENGLISH_WORDS if anagrams(s,p) and s != p]
    return l.sort()
47/21:
# Run this cell to test your find_all_anagrams function
# The testing module must have been imported (see above)
do_tests( find_all_anagrams )
47/22:
## Q2(b) answer code cell
## Modify this function definition to fulfill the given requirements.
## Expected code length: less than 5 lines.

def find_all_anagrams(s):
    l = []
    s = s.lower()
    l = [p for p in ENGLISH_WORDS if anagrams(s,p) and s != p]
    return sorted(l)
47/23:
# Run this cell to test your find_all_anagrams function
# The testing module must have been imported (see above)
do_tests( find_all_anagrams )
47/24:
## Q2(b) answer code cell
## Modify this function definition to fulfill the given requirements.
## Expected code length: less than 5 lines.

def find_all_anagrams(s):
    s = s.lower()
    return sorted([p for p in ENGLISH_WORDS if anagrams(s,p) and s != p])
47/25:
# Run this cell to test your find_all_anagrams function
# The testing module must have been imported (see above)
do_tests( find_all_anagrams )
47/26:
## Q2(d) answer code cell
## Modify this function definition to fulfill the given requirements.
## Expected code length: less than 5 lines.

def find_palindromes_of_length(n):
    return sorted([p for p in ENGLISH_WORDS if len(p) == n and is_palindrome(p)])
47/27:
# Run this cell to test your find_palindromes_of_length
# The testing module must have been imported (see above)
do_tests( find_palindromes_of_length )
47/28:
## Edit this cell to give your answer for Q2(b)

def password_strength( password ):
    if is_english_word(password) or " " in password or "\t" in password or "\n" in password:
        return "ILLEGAL"
    index = 0
    while index < len(password):
    if len(password) < 8
    return "MEDIUM" 
    # should return the strength ("ILLEGAL", WEAK", "STRONG" or "MEDIUM")
47/29:
## Edit this cell to give your answer for Q2(b)

def password_strength( password ):
    if is_english_word(password) or " " in password or "\t" in password or "\n" in password:
        return "ILLEGAL"
    index = 0
    while index < len(password) and not password[index].isdigit():
        index += 1
    if len(password) < 8 or (is_english_word(password[:index])):
    return "MEDIUM" 
    # should return the strength ("ILLEGAL", WEAK", "STRONG" or "MEDIUM")
47/30:
## Edit this cell to give your answer for Q2(b)

def password_strength( password ):
    if is_english_word(password) or " " in password or "\t" in password or "\n" in password:
        return "ILLEGAL"
    index = 0
    while index < len(password) and not password[index].isdigit():
        index += 1
    if len(password) < 8 or (is_english_word(password[:index]) and password[index].isdigit()):
        return "WEAK"
    return "MEDIUM" 
    # should return the strength ("ILLEGAL", WEAK", "STRONG" or "MEDIUM")
47/31:
# Run this cell to test the password_strength function
# The testing module must have been imported (see above)
do_tests( password_strength )
47/32:
## Edit this cell to give your answer for Q2(b)

def password_strength( password ):
    if is_english_word(password) or " " in password or "\t" in password or "\n" in password:
        return "ILLEGAL"
    index = 0
    while index < len(password) and not password[index].isdigit():
        index += 1
    if len(password) < 8 or (is_english_word(password[:index]) and password[index:].isdigit()):
        return "WEAK"
    return "MEDIUM" 
    # should return the strength ("ILLEGAL", WEAK", "STRONG" or "MEDIUM")
47/33:
# Run this cell to test the password_strength function
# The testing module must have been imported (see above)
do_tests( password_strength )
47/34:
## Edit this cell to give your answer for Q2(b)

def password_strength( password ):
    if is_english_word(password) or " " in password or "\t" in password or "\n" in password:
        return "ILLEGAL"
    index = 0
    while index < len(password) and not password[index].isdigit():
        index += 1
    if len(password) < 8 or (is_english_word(password[:index]) and password[index:].isdigit()):
        return "WEAK"
    if len(password) >= 12:
        number = False
        upper = False
        lower = False
        special = False
        for c in password:
            if c.isdigit():
                number = True
            elif c.isupper():
                upper = True
            elif c.islower():
                lower = True
            elif c in "~`!@#$%^&*(){}[]|\/:;\";<>.?":
                special = True
        if number and upper and lower and special:
            strength = "STRONG"
    return "MEDIUM" 
    # should return the strength ("ILLEGAL", WEAK", "STRONG" or "MEDIUM")
47/35:
# Run this cell to test the password_strength function
# The testing module must have been imported (see above)
do_tests( password_strength )
47/36:
## Edit this cell to give your answer for Q2(b)

def password_strength( password ):
    if is_english_word(password) or " " in password or "\t" in password or "\n" in password:
        return "ILLEGAL"
    index = 0
    while index < len(password) and not password[index].isdigit():
        index += 1
    if len(password) < 8 or (is_english_word(password[:index]) and password[index:].isdigit()):
        return "WEAK"
    if len(password) >= 12:
        number = False
        upper = False
        lower = False
        special = False
        for c in password:
            if c.isdigit():
                number = True
            elif c.isupper():
                upper = True
            elif c.islower():
                lower = True
            elif c in "~`!@#$%^&*(){}[]|\/:;\";<>.?":
                special = True
        print(number, upper, lower, special)
        if number and upper and lower and special:
            strength = "STRONG"
    return "MEDIUM" 
    # should return the strength ("ILLEGAL", WEAK", "STRONG" or "MEDIUM")
47/37:
# Run this cell to test the password_strength function
# The testing module must have been imported (see above)
do_tests( password_strength )
47/38:
## Edit this cell to give your answer for Q2(b)

def password_strength( password ):
    if is_english_word(password) or " " in password or "\t" in password or "\n" in password:
        return "ILLEGAL"
    index = 0
    while index < len(password) and not password[index].isdigit():
        index += 1
    if len(password) < 8 or (is_english_word(password[:index]) and password[index:].isdigit()):
        return "WEAK"
    if len(password) >= 12:
        number = False
        upper = False
        lower = False
        special = False
        for c in password:
            if c.isdigit():
                number = True
            elif c.isupper():
                upper = True
            elif c.islower():
                lower = True
            elif c in "~`!@#$%^&*(){}[]|\/:;\";<>.?":
                special = True
        if number and upper and lower and special:
            return "STRONG"
    return "MEDIUM" 
    # should return the strength ("ILLEGAL", WEAK", "STRONG" or "MEDIUM")
47/39:
# Run this cell to test the password_strength function
# The testing module must have been imported (see above)
do_tests( password_strength )
47/40:
## Edit this function definition to give your answer
def available_features( max_cost, holiday_data):
    af = set()
    for data in holiday_data:
        if data[1] <= max_cost:
            af.update(data[2])
    return sorted(list(af))
47/41:
# Run this cell to test the recommend_holidays function
# The testing module must have been imported (see above)
do_tests(available_features)
47/42:
## Edit this function definition to give your answer
def recommend_holidays( max_cost, attributes, holiday_data):
    r_h = []
    for data in holiday_data:
        if data[1] <= max_cost:
            contains_all = True
            for a in attributes:
            af.append(data[0])
    return sorted(r_h)
47/43:
## Edit this function definition to give your answer
def recommend_holidays( max_cost, attributes, holiday_data):
    r_h = []
    for data in holiday_data:
        if data[1] <= max_cost:
            contains_all = True
            for a in attributes:
                if a not in data[3]:
            af.append(data[0])
    return sorted(r_h)
47/44:
## Edit this function definition to give your answer
def recommend_holidays( max_cost, attributes, holiday_data):
    r_h = []
    for data in holiday_data:
        if data[1] <= max_cost:
            contains_all = True
            for a in attributes:
                if a not in data[3]:
                    contains_all = False
            if contains_all:
                af.append(data[0])
    return sorted(r_h)
47/45:
# Run this cell to test the recommend_holidays function
# The testing module must have been imported (see above)
do_tests(recommend_holidays)
47/46:
## Edit this function definition to give your answer
def recommend_holidays( max_cost, attributes, holiday_data):
    r_h = []
    for data in holiday_data:
        if data[1] <= max_cost:
            contains_all = True
            for a in attributes:
                if a not in data[2]:
                    contains_all = False
            if contains_all:
                af.append(data[0])
    return sorted(r_h)
47/47:
# Run this cell to test the recommend_holidays function
# The testing module must have been imported (see above)
do_tests(recommend_holidays)
47/48:
## Edit this function definition to give your answer
def recommend_holidays( max_cost, attributes, holiday_data):
    r_h = []
    for data in holiday_data:
        if data[1] <= max_cost:
            contains_all = True
            for a in attributes:
                if a not in data[2]:
                    contains_all = False
            if contains_all:
                r_h.append(data[0])
    return sorted(r_h)
47/49:
# Run this cell to test the recommend_holidays function
# The testing module must have been imported (see above)
do_tests(recommend_holidays)
47/50:
## Testing module must have been imported (see first code cell above)
do_all_tests()
47/51:
## Edit this function definition to give your answer
def recommend_holidays( max_cost, attributes, holiday_data):
    r_h = []
    for data in holiday_data:
        if data[1] <= max_cost:
            contains_all = True
            for a in attributes:
                if a not in data[2]:
                    contains_all = False
            if contains_all:
                r_h.append(data[0])
    return sorted(r_h)
47/52:
# Run this cell to test the recommend_holidays function
# The testing module must have been imported (see above)
do_tests(recommend_holidays)
47/53:
## Testing module must have been imported (see first code cell above)
do_all_tests()
47/54:
from A1_P4DS2_Python_Programming_tests import do_tests, do_all_tests, tests_version

tests_version()
47/55:
## Q2(a) answer code cell
## Modify this function definition to fulfill the given requirements.
## Expected code length: less than 10 lines.

def anagrams( s1, s2 ):
    if len(s1) == len(s2):
        c_s1 = 0
        c_s2 = 0
        for index in range(len(s1)):
            if s1[index].isupper():
                c_s1 += 1
            if s2[index].isupper():
                c_s2 += 1
        s_s1 = sorted(s1.upper())
        s_s2 = sorted(s2.upper())
        
        return (c_s1 == c_s2 and s_s1 == s_s2)
    return False
47/56:
# Run this cell to test your is_english_word function
# The testing module have been imported (see above)
# You must also have run the cell with your anagrams definition.
do_tests( anagrams )
47/57:
## Q2(c) answer code cell
## Modify this function definition to fulfill the given requirements
## Expected code length: less than 10 lines.

def is_palindrome(s):
    f_index = 0
    b_index = len(s) - 1
    s = s.lower()
    while f_index < b_index:
        if not s[f_index].isalpha():
            f_index += 1
        elif not s[b_index].isalpha():
            b_index -= 1
        elif s[f_index] != s[b_index]:
            return False
        else :
            f_index += 1
            b_index -= 1
    return True
47/58:
# Run this cell to test your is_palindrome function
# The tests module must have been imported (see above).
do_tests(is_palindrome)
47/59:
def get_english_words():
    with open( "english_words.txt" ) as f:
         words = f.readlines()
    words = { word.strip() for
             word in words }
    return words

ENGLISH_WORDS = get_english_words()
47/60:
## Modify this function definition to fulfill the given requirements.
## Expected code length: less than 15 lines.

def is_english_word(s):
    plain = s.lower()
    if plain == s or plain.title() == s or plain.upper() == s:
        if plain in ENGLISH_WORDS:
            return True
    return False
47/61:
# Run this cell to test your is_english_word function
# The testing module must have been imported (see above)
do_tests(is_english_word)
47/62:
## Q2(b) answer code cell
## Modify this function definition to fulfill the given requirements.
## Expected code length: less than 5 lines.

def find_all_anagrams(s):
    s = s.lower()
    return sorted([p for p in ENGLISH_WORDS if anagrams(s,p) and s != p])
47/63:
# Run this cell to test your find_all_anagrams function
# The testing module must have been imported (see above)
do_tests( find_all_anagrams )
47/64:
## Q2(d) answer code cell
## Modify this function definition to fulfill the given requirements.
## Expected code length: less than 5 lines.

def find_palindromes_of_length(n):
    return sorted([p for p in ENGLISH_WORDS if len(p) == n and is_palindrome(p)])
47/65:
# Run this cell to test your find_palindromes_of_length
# The testing module must have been imported (see above)
do_tests( find_palindromes_of_length )
47/66:
## Edit this cell to give your answer for Q2(b)

def password_strength( password ):
    if is_english_word(password) or " " in password or "\t" in password or "\n" in password:
        return "ILLEGAL"
    index = 0
    while index < len(password) and not password[index].isdigit():
        index += 1
    if len(password) < 8 or (is_english_word(password[:index]) and password[index:].isdigit()):
        return "WEAK"
    if len(password) >= 12:
        number = False
        upper = False
        lower = False
        special = False
        for c in password:
            if c.isdigit():
                number = True
            elif c.isupper():
                upper = True
            elif c.islower():
                lower = True
            elif c in "~`!@#$%^&*(){}[]|\/:;\";<>.?":
                special = True
        if number and upper and lower and special:
            return "STRONG"
    return "MEDIUM" 
    # should return the strength ("ILLEGAL", WEAK", "STRONG" or "MEDIUM")
47/67:
# Run this cell to test the password_strength function
# The testing module must have been imported (see above)
do_tests( password_strength )
47/68:
HOLIDAYS_EG = [ ["Scarborough",  45,  ["beach"]], 
                ["Whitby",       60,  ["beach", "culture"]],
                ["Barcelona",   320,  ["beach", "culture", "hot"]], 
                ["Corfu",       300,  ["beach", "hot"]],
                ["Paris",       250,  ["culture"]],
                ["Rome",        300,  ["culture", "hot"]],
                ["Switzerland", 450,  ["culture", "mountains"]],
                ["California",  750,  ["beach", "hot", "mountains"]],      
             ]
47/69:
## Edit this function definition to give your answer
def available_features( max_cost, holiday_data):
    af = set()
    for data in holiday_data:
        if data[1] <= max_cost:
            af.update(data[2])
    return sorted(list(af))
47/70:
# Run this cell to test the recommend_holidays function
# The testing module must have been imported (see above)
do_tests(available_features)
47/71:
## Edit this function definition to give your answer
def recommend_holidays( max_cost, attributes, holiday_data):
    r_h = []
    for data in holiday_data:
        if data[1] <= max_cost:
            contains_all = True
            for a in attributes:
                if a not in data[2]:
                    contains_all = False
            if contains_all:
                r_h.append(data[0])
    return sorted(r_h)
47/72:
# Run this cell to test the recommend_holidays function
# The testing module must have been imported (see above)
do_tests(recommend_holidays)
47/73:
## Testing module must have been imported (see first code cell above)
do_all_tests()
54/1: import pandas as pd
54/2:
df = pd.DataFrame({
    "title": ["Tangled Web",
              "Close Up",
              "Foundations",
              "Professional Secrets",
              "5 Times 5"],
    "author": ["Eric Mead",
               "David Stone",
               "Eberhard Riese",
               "Geoffrey Durham",
               "Richard Kaufman"],
    "price": [40, 23.1, 70, 295, 34.65]
})

df
54/3: df.index
54/4: df.columns
54/5: df["title"]
54/6: df["title"][1]
54/7: df.head(2)
54/8: df.describe()
54/9: df.info()
54/10: df.shape
54/11:
title_series = df["title"]

for i in title_series:
    print('The title of this book is {}'.format(i))
54/12:
for i in df:
    print(i)
54/13:
for col_name, col_data in df.items():
    print('**New column: {}**'.format(col_name))
    for i in col_data:
        print(i)
54/14:
for row_name, row_data in df.iterrows():
    print("**New row: {}**".format(row_name))
    for i in row_data:
        print(i)
54/15:
for row in df.itertuples(name="Book"):
    print(row)
54/16:
for row in df.itertuples(name="Book"):
    print(row.title)
54/17: df[["author", "title"]]
54/18: df.iloc[2:-1]
55/1:
## WC_DF Initialisation

import pandas  ## This is the module for creating and manipulating DataFrames

WC_DF = pandas.read_csv("worldcities.csv")
55/2:
top_5_cities = WC_DF["city"][:5]   ## selects the first 5 items of the "city" column.
top_5_cities
55/3:
top_5_cities.index = list("abcde")
top_5_cities
55/4: top_5_cities.index
55/5: WC_DF["city"][:5].values
55/6: WC_DF.columns
55/7: list(WC_DF.columns)
55/8: WC_DF.iloc[1]
55/9:
for i, row in WC_DF.iterrows():
    print(i, row['city_ascii'], row['lat'], row['lng'])
    if i >3: break
55/10: WC_DF.sort_values(by=["country"], ascending=True)[:10] # Sorts countries by alphabet
55/11:
filtered_DF = WC_DF[ WC_DF['capital'] == 'admin'] # This keeps only administrative capitals
filtered_DF.head()
55/12:
# Question 1a answer cell

def non_ascii_cities():
    return set(WC_DF[ WC_DF['city'] != WC_DF['city_ascii']])
    # Modify to return a set of all non-ascii city names in the world_cities data
    
non_ascii_cities()
55/13:
# Question 1a answer cell

def non_ascii_cities():
    return set(WC_DF[ WC_DF['city'] != WC_DF['city_ascii']].head())
    # Modify to return a set of all non-ascii city names in the world_cities data
    
non_ascii_cities()
55/14:
# Question 1a answer cell

def non_ascii_cities():
    return set(WC_DF[ WC_DF['city'] == WC_DF['city_ascii']].head())
    # Modify to return a set of all non-ascii city names in the world_cities data
    
non_ascii_cities()
55/15:
# Question 1a answer cell

def non_ascii_cities():
    
    return set()
    # Modify to return a set of all non-ascii city names in the world_cities data
    
non_ascii_cities()
[row['city'] for i, row in WC_DF.iterrows() if row['city']!= row['city_ascii']]
55/16:
# Question 1a answer cell

def non_ascii_cities():
    
    return set([row['city'] for i, row in WC_DF.iterrows() if row['city']!= row['city_ascii']])
    # Modify to return a set of all non-ascii city names in the world_cities data
    
non_ascii_cities()
55/17:
# Question 1b answer cell

def num_cities_occurring_n_times(n):
    counts = dict()
    for city in WC_DF["city"]:
        if city in counts:
            counts[city] += 1
        else:
            counts[city] == 1
    return [key for key,value in counts:]
        
    # Modify to return a value according to the specification given above
55/18:
# Question 1b answer cell

def num_cities_occurring_n_times(n):
    counts = dict()
    for city in WC_DF["city"]:
        if city in counts:
            counts[city] += 1
        else:
            counts[city] == 1
    return [key for key,value in counts if value == n]
    # Modify to return a value according to the specification given above
55/19:
# Question 1b answer cell

def num_cities_occurring_n_times(n):
    counts = dict()
    for city in WC_DF["city"]:
        if city in counts:
            counts[city] += 1
        else:
            counts[city] == 1
    return [key for key,value in counts if value == n]
    # Modify to return a value according to the specification given above
    
num_cities_occurring_n_times(2)
55/20:
# Question 1b answer cell

def num_cities_occurring_n_times(n):
    counts = dict()
    for city in WC_DF["city"]:
        if city in counts:
            counts[city] += 1
        else:
            counts[city] = 1
    return [key for key,value in counts if value == n]
    # Modify to return a value according to the specification given above
    
num_cities_occurring_n_times(2)
55/21:
# Question 1b answer cell

def num_cities_occurring_n_times(n):
    counts = dict()
    for city in WC_DF["city"]:
        if city in counts:
            counts[city] += 1
        else:
            counts[city] = 1
    return [city for city in counts if counts[city] == n]
    # Modify to return a value according to the specification given above
    
num_cities_occurring_n_times(2)
55/22:
# Question 1b answer cell

def num_cities_occurring_n_times(n):
    counts = dict()
    for city in WC_DF["city"]:
        if city in counts:
            counts[city] += 1
        else:
            counts[city] = 1
    return [city for city in counts if counts[city] == n]
    # Modify to return a value according to the specification given above
    
num_cities_occurring_n_times(10)
55/23:
# Question 1b answer cell

def num_cities_occurring_n_times(n):
    counts = dict()
    for city in WC_DF["city"]:
        if city in counts:
            counts[city] += 1
        else:
            counts[city] = 1
    return [city for city in counts if counts[city] == n]
    # Modify to return a value according to the specification given above
    
num_cities_occurring_n_times(11)
55/24:
# Question 1b answer cell

def num_cities_occurring_n_times(n):
    counts = dict()
    for city in WC_DF["city"]:
        if city in counts:
            counts[city] += 1
        else:
            counts[city] = 1
    return [city for city in counts if counts[city] == n]
    # Modify to return a value according to the specification given above
    
num_cities_occurring_n_times(13)
55/25:
# Question 1b answer cell

def num_cities_occurring_n_times(n):
    counts = dict()
    for city in WC_DF["city"]:
        if city in counts:
            counts[city] += 1
        else:
            counts[city] = 1
    return [city for city in counts if counts[city] == n]
    # Modify to return a value according to the specification given above
    
num_cities_occurring_n_times(14)
55/26:
# Question 1a answer cell

def non_ascii_cities():
    return set([row['city'] for i, row in WC_DF.iterrows() if row['city']!= row['city_ascii']])
    # Modify to return a set of all non-ascii city names in the world_cities data
55/27:
# Question 1b answer cell

def num_cities_occurring_n_times(n):
    counts = dict()
    for city in WC_DF["city"]:
        if city in counts:
            counts[city] += 1
        else:
            counts[city] = 1
    return [city for city in counts if counts[city] == n]
    # Modify to return a value according to the specification given above
55/28:
def country_num_cities_dict():
    counts = dict()
    for country in WC_DF["country"]:
        if country in counts:
            counts[country] += 1
        else:
            counts[country] = 1
    return count

country_num_cities_dict()
55/29:
def country_num_cities_dict():
    counts = dict()
    for country in WC_DF["country"]:
        if country in counts:
            counts[country] += 1
        else:
            counts[country] = 1
    return counts

country_num_cities_dict()
55/30:
def country_num_cities_dict():
    counts = dict()
    for country in WC_DF["country"]:
        if country in counts:
            counts[country] += 1
        else:
            counts[country] = 1
    return counts
55/31:
# Question 1d answer cell

def largest_cities_dataframe(n):
    return WC_DF.sort_values(by=["population"], ascending=False)[:n]
    # Modify to return a list of the n cities with the largest population
    
largest_cities_dataframe(10)
55/32:
# Question 1d answer cell

def largest_cities_dataframe(n):
    return WC_DF.sort_values(by=["population"], ascending=False)[:n]
    # Modify to return a list of the n cities with the largest population
    
largest_cities_dataframe(15)
55/33:
# Question 1d answer cell

def largest_cities_dataframe(n):
    return WC_DF.sort_values(by=["population"], ascending=False)[:n]
55/34:
# Question 1d answer cell

def largest_cities_dataframe(n):
    return WC_DF.sort_values(by=["population"], ascending=False)[:n]

largest_cities_dataframe(10)
55/35:
# Complete question 1d in this cell

def big_cities_in_country(country, population): # country is a string argument
    sorted_df = WC_DF.sort_values(by=["population"], ascending=True)
    return sorted_df[sorted_df['country'] == country and sorted_df['population'] >= population]
big_cities_in_country("Japan",0)
55/36:
# Complete question 1d in this cell

def big_cities_in_country(country, population): # country is a string argument
    sorted_df = WC_DF.sort_values(by=["population"], ascending=True)
    sorted_country = sorted_df[sorted_df['country'] == country]
    return sorted_country[sorted_country['population'] >= population]
big_cities_in_country("Japan",0)
55/37:
# Complete question 1d in this cell

def big_cities_in_country(country, population): # country is a string argument
    sorted_df = WC_DF.sort_values(by=["population"], ascending=True)
    sorted_country = sorted_df[sorted_df['country'] == country]
    return sorted_country[sorted_country['population'] >= population]
big_cities_in_country("Japan",100000)
55/38:
# Complete question 1d in this cell

def big_cities_in_country(country, population): # country is a string argument
    sorted_df = WC_DF.sort_values(by=["population"], ascending=True)
    sorted_country = sorted_df[sorted_df['country'] == country]
    return sorted_country[sorted_country['population'] >= population]
big_cities_in_country("Japan",1000000)
55/39:
# Complete question 1d in this cell

def big_cities_in_country(country, population): # country is a string argument
    sorted_df = WC_DF.sort_values(by=["population"], ascending=True)
    sorted_country = sorted_df[sorted_df['country'] == country]
    return sorted_country[sorted_country['population'] >= population]
big_cities_in_country("Japan",15000000)
55/40:
# Complete question 1d in this cell

def big_cities_in_country(country, population): # country is a string argument
    sorted_df = WC_DF.sort_values(by=["population"], ascending=True)
    sorted_country = sorted_df[sorted_df['country'] == country]
    return sorted_country[sorted_country['population'] >= population]
55/41:
## Question 1e Answer Code Cell

def country_total_cities_population(country):
    country_df = WC_DF[WC_DF['country'] == country]
    population = 0
    for p in country_df["population"]:
        population += p
    return population

country_total_cities_population("Japan")
55/42:
## Question 1e Answer Code Cell

def country_total_cities_population(country):
    country_df = WC_DF[WC_DF['country'] == country]
    population = 0
    for p in country_df["population"]:
        if ! p!=p:
        population += p
    return population

country_total_cities_population("Japan")
55/43:
## Question 1e Answer Code Cell

def country_total_cities_population(country):
    country_df = WC_DF[WC_DF['country'] == country]
    population = 0
    for p in country_df["population"]:
        if not p!=p:
        population += p
    return population

country_total_cities_population("Japan")
55/44:
## Question 1e Answer Code Cell

def country_total_cities_population(country):
    country_df = WC_DF[WC_DF['country'] == country]
    population = 0
    for p in country_df["population"]:
        if not p!=p:
            population += p
    return population

country_total_cities_population("Japan")
55/45:
## Question 1e Answer Code Cell

def country_total_cities_population(country):
    country_df = WC_DF[WC_DF['country'] == country]
    population = 0
    for p in country_df["population"]:
        if not p!=p:
            population += p
    return population

country_total_cities_population("China")
55/46:
## Question 1e Answer Code Cell

def country_total_cities_population(country):
    country_df = WC_DF[WC_DF['country'] == country]
    population = 0
    for p in country_df["population"]:
        if not p!=p:
            population += p
    return population
55/47:
# Q2a answer code cell

import pandas    ## This is the module for creating and manupulating DataFrames

# Here we have assigned the url of the quake datasource to the global variable 
# 'QUAKE_SOURCE' for your convenience.
QUAKE_SOURCE = ( "http://earthquake.usgs.gov/" +
                 "earthquakes/feed/v1.0/summary/all_day.csv" )


QUAKE_DF = pandas.read_csv(QUAKE_SOURCE)  ## Modify this line to import the data using Pandas
55/48:
## If QUAKE_DF is a DataFrame, show the first 5 rows
try:
    if type(QUAKE_DF) == pandas.DataFrame:
        display(QUAKE_DF.head())
    else:
        print("QUAKE_DF is not a DataFrame")
except:
    print("QUAKE_DF has not been assigned a value")
55/49:
def show_deep_quakes( depth ):
    # make deep_quakes DataFrame by selecting rows from QUAKE_DF
    deep_quakes = QUAKE_DF[ QUAKE_DF["depth"] >= depth ]  ## This is how you select rows by a condition
                                                          ## on one of the column values.
        
    print("Number of quakes of depth {} or deeper:".format(depth), 
           len(deep_quakes.index))     ## This finds the number of rows of the deep_quakes DataFrame
    
    display(deep_quakes.sort_values("depth", ascending=False))  ## Sort by descending depth value
55/50:
try:
    show_deep_quakes(100)
except:
    print("Probably QUAKE_DF not correctly set")
55/51:
try:
    QUAKE_DF["depth"].max()
except:
    print("Probably QUAKE_DF not correctly set")
55/52:
try:
    QUAKE_DF["depth"].min()
except:
    print("Probably QUAKE_DF not correctly set")
55/53:
try:
    print(QUAKE_DF["depth"].max())
except:
    print("Probably QUAKE_DF not correctly set")
55/54:
try:
    QUAKE_DF["depth"].max()
except:
    print("Probably QUAKE_DF not correctly set")
55/55:
try:
    QUAKE_DF["depth"].min()
except:
    print("Probably QUAKE_DF not correctly set")
55/56:
# Q2a answer code cell

import pandas    ## This is the module for creating and manupulating DataFrames

# Here we have assigned the url of the quake datasource to the global variable 
# 'QUAKE_SOURCE' for your convenience.
QUAKE_SOURCE = ( "http://earthquake.usgs.gov/" +
                 "earthquakes/feed/v1.0/summary/all_day.csv" )


QUAKE_DF = pandas.read_csv(QUAKE_SOURCE)  ## Modify this line to import the data using Pandas
55/57:
## If QUAKE_DF is a DataFrame, show the first 5 rows
try:
    if type(QUAKE_DF) == pandas.DataFrame:
        display(QUAKE_DF.head())
    else:
        print("QUAKE_DF is not a DataFrame")
except:
    print("QUAKE_DF has not been assigned a value")
55/58:
def show_deep_quakes( depth ):
    # make deep_quakes DataFrame by selecting rows from QUAKE_DF
    deep_quakes = QUAKE_DF[ QUAKE_DF["depth"] >= depth ]  ## This is how you select rows by a condition
                                                          ## on one of the column values.
        
    print("Number of quakes of depth {} or deeper:".format(depth), 
           len(deep_quakes.index))     ## This finds the number of rows of the deep_quakes DataFrame
    
    display(deep_quakes.sort_values("depth", ascending=False))  ## Sort by descending depth value
55/59:
# Complete question 2b answer cell

def powerful_quakes(mag):
    deep_quakes = QUAKE_DF[ QUAKE_DF["magnitude"] >= mag ]
    display(deep_quakes.sort_values("depth", ascending=False))
    return deep_quakes

powerful_quakes(5)
55/60:
# Complete question 2b answer cell

def powerful_quakes(mag):
    deep_quakes = QUAKE_DF[ QUAKE_DF["mag"] >= mag ]
    display(deep_quakes.sort_values("depth", ascending=False))
    return deep_quakes

powerful_quakes(5)
55/61:
# Complete question 2b answer cell

def powerful_quakes(mag):
    deep_quakes = QUAKE_DF[ QUAKE_DF["mag"] >= mag ]
    return deep_quakes

powerful_quakes(5)
55/62:
# Question 2c answer cell

def most_powerful_n_quakes(n):
    count = 0
    sorted_df = QUAKE_DF.sort_values(by=["mag"], ascending=True)
    return sorted_df
    # Edit this function to make it return a DataFrame of 
    # the 'top n' quakes of the all_day.csv file

most_powerful_n_quakes(1)
55/63:
# Question 2c answer cell

def most_powerful_n_quakes(n):
    count = 0
    sorted_df = QUAKE_DF.sort_values(by=["mag"], ascending=False)
    return sorted_df
    # Edit this function to make it return a DataFrame of 
    # the 'top n' quakes of the all_day.csv file

most_powerful_n_quakes(1)
55/64:
# Question 2c answer cell

def most_powerful_n_quakes(n):
    sorted_df = QUAKE_DF.sort_values(by=["mag"], ascending=False)
    while (sorted_df["mag"][n] == sorted_df["mag"]):
        n+=1
    return sorted_df[:n]

most_powerful_n_quakes(2)
55/65:
# Question 2c answer cell

def most_powerful_n_quakes(n):
    sorted_df = QUAKE_DF.sort_values(by=["mag"], ascending=False)
    while (sorted_df["mag"][n] == sorted_df["mag"][n+1]):
        n+=1
    return sorted_df[:n]

most_powerful_n_quakes(2)
55/66:
# Question 2c answer cell

def most_powerful_n_quakes(n):
    n -= 1
    sorted_df = QUAKE_DF.sort_values(by=["mag"], ascending=False)
    while (sorted_df["mag"][n] == sorted_df["mag"][n+1]):
        n+=1
    return sorted_df[:n]

most_powerful_n_quakes(2)
55/67:
# Question 2c answer cell

def most_powerful_n_quakes(n):
    n -= 1
    sorted_df = QUAKE_DF.sort_values(by=["mag"], ascending=False)
    while (sorted_df["mag"][n] == sorted_df["mag"][n+1]):
        n+=1
    return sorted_df[:(n+1)]

most_powerful_n_quakes(2)
55/68:
# Question 2c answer cell

def most_powerful_n_quakes(n):
    n -= 1
    sorted_df = QUAKE_DF.sort_values(by=["mag"], ascending=False)
    print(sorted_df["mag"][n], sorted_df["mag"][n+1])
    while (sorted_df["mag"][n] == sorted_df["mag"][n+1]):
        n+=1
    return sorted_df[:(n+1)]

most_powerful_n_quakes(2)
55/69:
# Question 2c answer cell

def most_powerful_n_quakes(n):
    n -= 1
    sorted_df = QUAKE_DF.sort_values(by=["mag"], ascending=False)
    print(sorted_df[n], sorted_df["mag"][n])
    while (sorted_df["mag"][n] == sorted_df["mag"][n+1]):
        n+=1
    return sorted_df[:(n+1)]

most_powerful_n_quakes(2)
55/70:
# Question 2c answer cell

def most_powerful_n_quakes(n):
    n -= 1
    sorted_df = QUAKE_DF.sort_values(by=["mag"], ascending=False)
    print(sorted_df.iloc(n), sorted_df["mag"][n])
    while (sorted_df["mag"][n] == sorted_df["mag"][n+1]):
        n+=1
    return sorted_df[:(n+1)]

most_powerful_n_quakes(2)
55/71:
# Question 2c answer cell

def most_powerful_n_quakes(n):
    n -= 1
    sorted_df = QUAKE_DF.sort_values(by=["mag"], ascending=False)
    display(sorted_df.iloc(n))
    print(sorted_df["mag"][n])
    while (sorted_df["mag"][n] == sorted_df["mag"][n+1]):
        n+=1
    return sorted_df[:(n+1)]

most_powerful_n_quakes(2)
55/72:
# Question 2c answer cell

def most_powerful_n_quakes(n):
    n -= 1
    sorted_df = QUAKE_DF.sort_values(by=["mag"], ascending=False)
    display(sorted_df.loc(n))
    print(sorted_df["mag"][n])
    while (sorted_df["mag"][n] == sorted_df["mag"][n+1]):
        n+=1
    return sorted_df[:(n+1)]

most_powerful_n_quakes(2)
55/73:
# Question 2c answer cell

def most_powerful_n_quakes(n):
    n -= 1
    sorted_df = QUAKE_DF.sort_values(by=["mag"], ascending=False)
    display(sorted_df[n])
    print(sorted_df["mag"][n])
    while (sorted_df["mag"][n] == sorted_df["mag"][n+1]):
        n+=1
    return sorted_df[:(n+1)]

most_powerful_n_quakes(2)
55/74:
# Question 2c answer cell

def most_powerful_n_quakes(n):
    n -= 1
    sorted_df = QUAKE_DF.sort_values(by=["mag"], ascending=False)
    print(sorted_df)
    print(sorted_df["mag"][n])
    while (sorted_df["mag"][n] == sorted_df["mag"][n+1]):
        n+=1
    return sorted_df[:(n+1)]

most_powerful_n_quakes(2)
55/75:
# Question 2c answer cell

def most_powerful_n_quakes(n):
    n -= 1
    sorted_df = QUAKE_DF.sort_values(by=["mag"], ascending=False)
    print(sorted_df["mag"])
    while (sorted_df["mag"][n] == sorted_df["mag"][n+1]):
        n+=1
    return sorted_df[:(n+1)]

most_powerful_n_quakes(2)
55/76:
# Question 2c answer cell

def most_powerful_n_quakes(n):
    n -= 1
    sorted_df = QUAKE_DF.sort_values(by=["mag"], ascending=False)
    print(sorted_df["mag"])
    print(sorted_df["mag"][:5])
    print(sorted_df["mag"][4])

    while (sorted_df["mag"][n] == sorted_df["mag"][n+1]):
        n+=1
    return sorted_df[:(n+1)]

most_powerful_n_quakes(2)
55/77:
# Question 2c answer cell

def most_powerful_n_quakes(n):
    n -= 1
    sorted_df = QUAKE_DF.sort_values(by=["mag"], ascending=False)
    print(sorted_df["mag"])
    print(sorted_df["mag"][:5])
    print(sorted_df["mag"][19])

    while (sorted_df["mag"][n] == sorted_df["mag"][n+1]):
        n+=1
    return sorted_df[:(n+1)]

most_powerful_n_quakes(2)
55/78:
# Question 2c answer cell

def most_powerful_n_quakes(n):
    n -= 1
    sorted_df = QUAKE_DF.sort_values(by=["mag"], ascending=False)
    print(sorted_df["mag"])
    print(sorted_df["mag"].iloc(5))

    while (sorted_df["mag"][n] == sorted_df["mag"][n+1]):
        n+=1
    return sorted_df[:(n+1)]

most_powerful_n_quakes(2)
55/79:
# Question 2c answer cell

def most_powerful_n_quakes(n):
    n -= 1
    sorted_df = QUAKE_DF.sort_values(by=["mag"], ascending=False)
    print(sorted_df["mag"])
    print(list(sorted_df["mag"])[1])

    while (sorted_df["mag"][n] == sorted_df["mag"][n+1]):
        n+=1
    return sorted_df[:(n+1)]

most_powerful_n_quakes(2)
55/80:
# Question 2c answer cell

def most_powerful_n_quakes(n):
    n -= 1
    sorted_df = QUAKE_DF.sort_values(by=["mag"], ascending=False)
    sorted_list = list(sorted_df["mag"])

    while (sorted_list[n] == sorted_list[n+1]):
        n+=1
    return sorted_df[:(n+1)]

most_powerful_n_quakes(2)
55/81:
# Question 2c answer cell

def most_powerful_n_quakes(n):
    n -= 1
    sorted_df = QUAKE_DF.sort_values(by=["mag"], ascending=False)
    sorted_list = list(sorted_df["mag"])
    while (sorted_list[n] == sorted_list[n+1]):
        n+=1
    return sorted_df[:(n+1)]

most_powerful_n_quakes(3)
55/82:
# Question 2c answer cell

def most_powerful_n_quakes(n):
    n -= 1
    sorted_df = QUAKE_DF.sort_values(by=["mag"], ascending=False)
    sorted_list = list(sorted_df["mag"])
    while (sorted_list[n] == sorted_list[n+1]):
        n+=1
    return sorted_df[:(n+1)]

most_powerful_n_quakes(4)
55/83:
# Question 2c answer cell

def most_powerful_n_quakes(n):
    n -= 1
    sorted_df = QUAKE_DF.sort_values(by=["mag"], ascending=False)
    sorted_list = list(sorted_df["mag"])
    while (sorted_list[n] == sorted_list[n+1]):
        n+=1
    return sorted_df[:(n+1)]

most_powerful_n_quakes(5)
55/84:
# Question 2c answer cell

def most_powerful_n_quakes(n):
    n -= 1
    sorted_df = QUAKE_DF.sort_values(by=["mag"], ascending=False)
    sorted_list = list(sorted_df["mag"])
    while (sorted_list[n] == sorted_list[n+1]):
        n+=1
    return sorted_df[:(n+1)]

most_powerful_n_quakes(6)
55/85:
## 2d Answer Code Cell

def quake_distance_from_loc_dataframe(loc):
    alter_df = QUAKE_DF.copy()
    alter_df["distance_from_loc"] = haversine_distance((alter_df["latitude"],alter_df["longitude"]),loc)
    return alter_df.sort_values(by=["distance_from_loc"], ascending=False)

quaker_distance_from_loc_dataframe((-58.34,-14.8131))
55/86:
## 2d Answer Code Cell

def quake_distance_from_loc_dataframe(loc):
    alter_df = QUAKE_DF.copy()
    alter_df["distance_from_loc"] = haversine_distance((alter_df["latitude"],alter_df["longitude"]),loc)
    return alter_df.sort_values(by=["distance_from_loc"], ascending=False)

quake_distance_from_loc_dataframe((-58.34,-14.8131))
55/87:
## Function to compute distance between locations (kilometres) 
# Returns the surface distance in meters, according to the Haversine formula,
# between two locations given as (latitude, longitude) coordinate pairs.

import math
def haversine_distance( loc1 , loc2 ): 
    '''finds the distance (m) between 2 locations, where locations are defined by
    longitudes and latitudes'''
    lat1, lon1 = loc1
    lat2, lon2 = loc2
    radius = 6371  # kilometers
    dlat = math.radians(lat2 - lat1)
    dlon = math.radians(lon2 - lon1)
    a = (math.sin(dlat / 2) * math.sin(dlat / 2) +
         math.cos(math.radians(lat1)) * math.cos(math.radians(lat2)) *
         math.sin(dlon / 2) * math.sin(dlon / 2))
    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))
    d = radius * c
    return d
55/88:
## 2d Answer Code Cell

def quake_distance_from_loc_dataframe(loc):
    alter_df = QUAKE_DF.copy()
    alter_df["distance_from_loc"] = haversine_distance((alter_df["latitude"],alter_df["longitude"]),loc)
    return alter_df.sort_values(by=["distance_from_loc"], ascending=False)

quake_distance_from_loc_dataframe((-58.34,-14.8131))
55/89:
## 2d Answer Code Cell

def quake_distance_from_loc_dataframe(loc):
    alter_df = QUAKE_DF.copy()
    alter_df["distance_from_loc"] = (0.0, 0.0)
    for i,row in alter_df:
        row["distance_from_loc"] = haversine_distance((row["latitude"],row["longitude"]),loc)
    return alter_df.sort_values(by=["distance_from_loc"], ascending=False)

quake_distance_from_loc_dataframe((-58.34,-14.8131))
55/90:
## 2d Answer Code Cell

def quake_distance_from_loc_dataframe(loc):
    alter_df = QUAKE_DF.copy()
    alter_df["distance_from_loc"] = NaN
    for i,row in alter_df:
        row["distance_from_loc"] = haversine_distance((row["latitude"],row["longitude"]),loc)
    return alter_df.sort_values(by=["distance_from_loc"], ascending=False)

quake_distance_from_loc_dataframe((-58.34,-14.8131))
55/91:
## 2d Answer Code Cell

def quake_distance_from_loc_dataframe(loc):
    alter_df = QUAKE_DF.copy()
    alter_df["distance_from_loc"] = None
    for i,row in alter_df:
        row["distance_from_loc"] = haversine_distance((row["latitude"],row["longitude"]),loc)
    return alter_df.sort_values(by=["distance_from_loc"], ascending=False)

quake_distance_from_loc_dataframe((-58.34,-14.8131))
55/92:
## 2d Answer Code Cell

def quake_distance_from_loc_dataframe(loc):
    alter_df = QUAKE_DF.copy()
    alter_df["distance_from_loc"] = None
    for row in alter_df:
        row["distance_from_loc"] = haversine_distance((row["latitude"],row["longitude"]),loc)
    return alter_df.sort_values(by=["distance_from_loc"], ascending=False)

quake_distance_from_loc_dataframe((-58.34,-14.8131))
55/93:
## 2d Answer Code Cell

def quake_distance_from_loc_dataframe(loc):
    alter_df = QUAKE_DF.copy()
    alter_df["distance_from_loc"] = None
    for i,row in alter_df.iterrows():
        row["distance_from_loc"] = haversine_distance((row["latitude"],row["longitude"]),loc)
    return alter_df.sort_values(by=["distance_from_loc"], ascending=False)

quake_distance_from_loc_dataframe((-58.34,-14.8131))
55/94:
## 2d Answer Code Cell

def quake_distance_from_loc_dataframe(loc):
    alter_df = QUAKE_DF.copy()
    alter_df["distance_from_loc"] = None
    for i,row in alter_df.iterrows():
        alter_df["distance_from_loc"][i] = haversine_distance((row["latitude"],row["longitude"]),loc)
    return alter_df.sort_values(by=["distance_from_loc"], ascending=False)

quake_distance_from_loc_dataframe((-58.34,-14.8131))
55/95:
## 2d Answer Code Cell

def quake_distance_from_loc_dataframe(loc):
    alter_df = QUAKE_DF.copy()
    alter_df["distance_from_loc"] = None
    for i,row in alter_df.iterrows():
        alter_df["distance_from_loc"][i] = haversine_distance((row["latitude"],row["longitude"]),loc)
    return alter_df.sort_values(by=["distance_from_loc"], ascending=True)

quake_distance_from_loc_dataframe((-58.34,-14.8131))
55/96:
def effective_magnitude( magnitude, depth, surface_distance ):
    energy = 10**magnitude  # convert logarithmic magnitude to a linear energy value
    if depth < 1:   # Crude fix for small or negative depths (can occur where land is above sea level)
        depth = 1
    ## Calculate distance to source by Pythagorus (ignoring curvature of surface)
    dist_to_source_squared =  depth**2 + surface_distance**2
    ## Apply inverse square distance multiplier to get energy density at distance from source
    ## (Ignores damping effects)
    attenuated_energy = energy/dist_to_source_squared
    attenuated_magnitude =  math.log10(attenuated_energy) ## Convert back to a log base 10 scale
    return attenuated_magnitude

# Some test cases.
effective_magnitude(9,100,500)
effective_magnitude(6,50, 100)
55/97:
def effective_magnitude( magnitude, depth, surface_distance ):
    energy = 10**magnitude  # convert logarithmic magnitude to a linear energy value
    if depth < 1:   # Crude fix for small or negative depths (can occur where land is above sea level)
        depth = 1
    ## Calculate distance to source by Pythagorus (ignoring curvature of surface)
    dist_to_source_squared =  depth**2 + surface_distance**2
    ## Apply inverse square distance multiplier to get energy density at distance from source
    ## (Ignores damping effects)
    attenuated_energy = energy/dist_to_source_squared
    attenuated_magnitude =  math.log10(attenuated_energy) ## Convert back to a log base 10 scale
    return attenuated_magnitude

# Some test cases.
effective_magnitude(9,100,500)
# effective_magnitude(6,50, 100)
55/98:
def effective_magnitude( magnitude, depth, surface_distance ):
    energy = 10**magnitude  # convert logarithmic magnitude to a linear energy value
    if depth < 1:   # Crude fix for small or negative depths (can occur where land is above sea level)
        depth = 1
    ## Calculate distance to source by Pythagorus (ignoring curvature of surface)
    dist_to_source_squared =  depth**2 + surface_distance**2
    ## Apply inverse square distance multiplier to get energy density at distance from source
    ## (Ignores damping effects)
    attenuated_energy = energy/dist_to_source_squared
    attenuated_magnitude =  math.log10(attenuated_energy) ## Convert back to a log base 10 scale
    return attenuated_magnitude

# Some test cases.
# effective_magnitude(9,100,500)
effective_magnitude(6,50, 100)
55/99:
def effective_magnitude( magnitude, depth, surface_distance ):
    energy = 10**magnitude  # convert logarithmic magnitude to a linear energy value
    if depth < 1:   # Crude fix for small or negative depths (can occur where land is above sea level)
        depth = 1
    ## Calculate distance to source by Pythagorus (ignoring curvature of surface)
    dist_to_source_squared =  depth**2 + surface_distance**2
    ## Apply inverse square distance multiplier to get energy density at distance from source
    ## (Ignores damping effects)
    attenuated_energy = energy/dist_to_source_squared
    attenuated_magnitude =  math.log10(attenuated_energy) ## Convert back to a log base 10 scale
    return attenuated_magnitude

# Some test cases.
effective_magnitude(9,100,500)
effective_magnitude(6,50, 100)
55/100: sorted_city = WC_DF.sort_values(by=["country", "city_ascii"], ascending=[True, True])
55/101: sorted_city = WC_DF.sort_values(by=["country", "city_ascii"], ascending=[True, True])[:10]
55/102: WC_DF.sort_values(by=["country", "city_ascii"], ascending=[True, True])[:10]
55/103:
## 2e Answer Code Cell
def endangered_cities(min_population, min_effective_magnitude):
    filter1_q = QUAKE_DF[QUAKE_DF["mag"] >= min_effective_magnitude]
    filter1_q["ep_mag"] = None
    for i,row in filter1_q.iterrows():
        filter1_q["ep_mag"][i] = epicenter_magnitude(row["mag"], row["depth"])
    filter2_q = filter1_q[filter1_q["ep_mag"] >= min_effective_magnitude]
    
    
    filter1_city = WC_DF[WC_DF["population"] >= min_population]
    sorted_city = filter1_DF.sort_values(by=["country", "city_ascii"], ascending=[True, True])
    
    sorted_city["count"] = 0
    ed_c = []
    for i,row_c in sorted_city:
        loc_c = (row_c["lat"], row_c["lng"])
        endanger = False
        for j,row_q in filter2_q:
            loc_q = (row_q["latitude"],row_q["longitude"])
            dist = haversine_distance(loc_c, loc_q)
            ef_mag = effective_magnitude(row_q["mag"],row_q["depth"],dist)
            if ef_mag >= min_effective_magnitude:
                endanger = True
                break
        if endanger :
            ed_c.append((row_c["city_ascii"], row_c["country"], loc_c))
    
    return ed_c
55/104:
## 2e Answer Code Cell
def endangered_cities(min_population, min_effective_magnitude):
    filter1_q = QUAKE_DF[QUAKE_DF["mag"] >= min_effective_magnitude]
    filter1_q["ep_mag"] = None
    for i,row in filter1_q.iterrows():
        filter1_q["ep_mag"][i] = epicenter_magnitude(row["mag"], row["depth"])
    filter2_q = filter1_q[filter1_q["ep_mag"] >= min_effective_magnitude]
    
    
    filter1_city = WC_DF[WC_DF["population"] >= min_population]
    sorted_city = filter1_DF.sort_values(by=["country", "city_ascii"], ascending=[True, True])
    
    # this specification is only for testing
    sorted_city = sorted_city[sorted_city["country"] == "Japan"]
    # end of specification
    
    sorted_city["count"] = 0
    ed_c = []
    for i,row_c in sorted_city:
        loc_c = (row_c["lat"], row_c["lng"])
        endanger = False
        for j,row_q in filter2_q:
            loc_q = (row_q["latitude"],row_q["longitude"])
            dist = haversine_distance(loc_c, loc_q)
            ef_mag = effective_magnitude(row_q["mag"],row_q["depth"],dist)
            if ef_mag >= min_effective_magnitude:
                endanger = True
                break
        if endanger :
            ed_c.append((row_c["city_ascii"], row_c["country"], loc_c))
    
    return ed_c
        
endangered_cities(200000, 0.5)
55/105:
def effective_magnitude( magnitude, depth, surface_distance ):
    energy = 10**magnitude  # convert logarithmic magnitude to a linear energy value
    if depth < 1:   # Crude fix for small or negative depths (can occur where land is above sea level)
        depth = 1
    ## Calculate distance to source by Pythagorus (ignoring curvature of surface)
    dist_to_source_squared =  depth**2 + surface_distance**2
    ## Apply inverse square distance multiplier to get energy density at distance from source
    ## (Ignores damping effects)
    attenuated_energy = energy/dist_to_source_squared
    attenuated_magnitude =  math.log10(attenuated_energy) ## Convert back to a log base 10 scale
    return attenuated_magnitude

# Some test cases.
# effective_magnitude(9,100,500)
# effective_magnitude(6,50, 100)
55/106:
def epicenter_magnitude( magnitude, depth ):
    return effective_magnitude( magnitude, depth, 0)
55/107:
## 2e Answer Code Cell
def endangered_cities(min_population, min_effective_magnitude):
    filter1_q = QUAKE_DF[QUAKE_DF["mag"] >= min_effective_magnitude]
    filter1_q["ep_mag"] = None
    for i,row in filter1_q.iterrows():
        filter1_q["ep_mag"][i] = epicenter_magnitude(row["mag"], row["depth"])
    filter2_q = filter1_q[filter1_q["ep_mag"] >= min_effective_magnitude]
    
    
    filter1_city = WC_DF[WC_DF["population"] >= min_population]
    sorted_city = filter1_DF.sort_values(by=["country", "city_ascii"], ascending=[True, True])
    
    # this specification is only for testing
    sorted_city = sorted_city[sorted_city["country"] == "Japan"]
    # end of specification
    
    sorted_city["count"] = 0
    ed_c = []
    for i,row_c in sorted_city:
        loc_c = (row_c["lat"], row_c["lng"])
        endanger = False
        for j,row_q in filter2_q:
            loc_q = (row_q["latitude"],row_q["longitude"])
            dist = haversine_distance(loc_c, loc_q)
            ef_mag = effective_magnitude(row_q["mag"],row_q["depth"],dist)
            if ef_mag >= min_effective_magnitude:
                endanger = True
                break
        if endanger :
            ed_c.append((row_c["city_ascii"], row_c["country"], loc_c))
    
    return ed_c
        
endangered_cities(200000, 0.5)
55/108:
## 2e Answer Code Cell
def endangered_cities(min_population, min_effective_magnitude):
    filter1_q = QUAKE_DF[QUAKE_DF["mag"] >= min_effective_magnitude]
    filter1_q["ep_mag"] = None
    for i,row in filter1_q.iterrows():
        filter1_q["ep_mag"][i] = epicenter_magnitude(row["mag"], row["depth"])
    filter2_q = filter1_q[filter1_q["ep_mag"] >= min_effective_magnitude]
    
    
    filter1_city = WC_DF[WC_DF["population"] >= min_population]
    sorted_city = filter1_city.sort_values(by=["country", "city_ascii"], ascending=[True, True])
    
    # this specification is only for testing
    sorted_city = sorted_city[sorted_city["country"] == "Japan"]
    # end of specification
    
    sorted_city["count"] = 0
    ed_c = []
    for i,row_c in sorted_city:
        loc_c = (row_c["lat"], row_c["lng"])
        endanger = False
        for j,row_q in filter2_q:
            loc_q = (row_q["latitude"],row_q["longitude"])
            dist = haversine_distance(loc_c, loc_q)
            ef_mag = effective_magnitude(row_q["mag"],row_q["depth"],dist)
            if ef_mag >= min_effective_magnitude:
                endanger = True
                break
        if endanger :
            ed_c.append((row_c["city_ascii"], row_c["country"], loc_c))
    
    return ed_c
        
endangered_cities(200000, 0.5)
55/109:
## 2e Answer Code Cell
def endangered_cities(min_population, min_effective_magnitude):
    filter1_q = QUAKE_DF[QUAKE_DF["mag"] >= min_effective_magnitude]
    filter1_q["ep_mag"] = None
    for i,row in filter1_q.iterrows():
        filter1_q["ep_mag"][i] = epicenter_magnitude(row["mag"], row["depth"])
    filter2_q = filter1_q[filter1_q["ep_mag"] >= min_effective_magnitude]
    
    
    filter1_city = WC_DF[WC_DF["population"] >= min_population]
    sorted_city = filter1_city.sort_values(by=["country", "city_ascii"], ascending=[True, True])
    
    # this specification is only for testing
    sorted_city = sorted_city[sorted_city["country"] == "Japan"]
    # end of specification
    
    sorted_city["count"] = 0
    ed_c = []
    for i,row_c in sorted_city.iterrows():
        loc_c = (row_c["lat"], row_c["lng"])
        endanger = False
        for j,row_q in filter2_q.iterrows():
            loc_q = (row_q["latitude"],row_q["longitude"])
            dist = haversine_distance(loc_c, loc_q)
            ef_mag = effective_magnitude(row_q["mag"],row_q["depth"],dist)
            if ef_mag >= min_effective_magnitude:
                endanger = True
                break
        if endanger :
            ed_c.append((row_c["city_ascii"], row_c["country"], loc_c))
    
    return ed_c
        
endangered_cities(200000, 0.5)
55/110:
## 2e Answer Code Cell
def endangered_cities(min_population, min_effective_magnitude):
    filter1_q = QUAKE_DF[QUAKE_DF["mag"] >= min_effective_magnitude]
    filter1_q["ep_mag"] = None
    for i,row in filter1_q.iterrows():
        filter1_q["ep_mag"][i] = epicenter_magnitude(row["mag"], row["depth"])
    filter2_q = filter1_q[filter1_q["ep_mag"] >= min_effective_magnitude]
    
    
    filter1_city = WC_DF[WC_DF["population"] >= min_population]
    sorted_city = filter1_city.sort_values(by=["country", "city_ascii"], ascending=[True, True])
    
    # this specification is only for testing
    sorted_city = sorted_city[sorted_city["country"] == "Japan"]
    # end of specification
    
    sorted_city["count"] = 0
    ed_c = []
    for i,row_c in sorted_city.iterrows():
        loc_c = (row_c["lat"], row_c["lng"])
        endanger = False
        for j,row_q in filter2_q.iterrows():
            loc_q = (row_q["latitude"],row_q["longitude"])
            dist = haversine_distance(loc_c, loc_q)
            ef_mag = effective_magnitude(row_q["mag"],row_q["depth"],dist)
            if ef_mag >= min_effective_magnitude:
                endanger = True
                break
        if endanger :
            ed_c.append((row_c["city_ascii"], row_c["country"], loc_c))
    
    return ed_c
        
endangered_cities(200000, 0)
55/111:
## 2e Answer Code Cell
def endangered_cities(min_population, min_effective_magnitude):
    filter1_q = QUAKE_DF[QUAKE_DF["mag"] >= min_effective_magnitude]
    filter1_q["ep_mag"] = None
    for i,row in filter1_q.iterrows():
        filter1_q["ep_mag"][i] = epicenter_magnitude(row["mag"], row["depth"])
    filter2_q = filter1_q[filter1_q["ep_mag"] >= min_effective_magnitude]
    
    
    filter1_city = WC_DF[WC_DF["population"] >= min_population]
    sorted_city = filter1_city.sort_values(by=["country", "city_ascii"], ascending=[True, True])
    
    # this specification is only for testing
    sorted_city = sorted_city[sorted_city["country"] == "Japan"]
    # end of specification
    
    sorted_city["count"] = 0
    display(sorted_city)
    ed_c = []
    for i,row_c in sorted_city.iterrows():
        loc_c = (row_c["lat"], row_c["lng"])
        endanger = False
        for j,row_q in filter2_q.iterrows():
            loc_q = (row_q["latitude"],row_q["longitude"])
            dist = haversine_distance(loc_c, loc_q)
            ef_mag = effective_magnitude(row_q["mag"],row_q["depth"],dist)
            if ef_mag >= min_effective_magnitude:
                endanger = True
                break
        if endanger :
            ed_c.append((row_c["city_ascii"], row_c["country"], loc_c))
    
    return ed_c
        
endangered_cities(200000, 0)
55/112:
## 2e Answer Code Cell
def endangered_cities(min_population, min_effective_magnitude):
    filter1_q = QUAKE_DF[QUAKE_DF["mag"] >= min_effective_magnitude]
    filter1_q["ep_mag"] = None
    for i,row in filter1_q.iterrows():
        filter1_q["ep_mag"][i] = epicenter_magnitude(row["mag"], row["depth"])
    filter2_q = filter1_q[filter1_q["ep_mag"] >= min_effective_magnitude]
    
    
    filter1_city = WC_DF[WC_DF["population"] >= min_population]
    sorted_city = filter1_city.sort_values(by=["country", "city_ascii"], ascending=[True, True])
    
    # this specification is only for testing
    sorted_city = sorted_city[sorted_city["country"] == "Japan"]
    # end of specification
    
    sorted_city["count"] = 0
    display(sorted_city)
    ed_c = []
    for i,row_c in sorted_city.iterrows():
        loc_c = (row_c["lat"], row_c["lng"])
        endanger = False
        for j,row_q in filter2_q.iterrows():
            loc_q = (row_q["latitude"],row_q["longitude"])
            dist = haversine_distance(loc_c, loc_q)
            ef_mag = effective_magnitude(row_q["mag"],row_q["depth"],dist)
            if ef_mag >= min_effective_magnitude:
                endanger = True
                break
        if endanger :
            ed_c.append((row_c["city_ascii"], row_c["country"], loc_c))
    
    return ed_c
        
endangered_cities(1000000, 0)
55/113:
## 2e Answer Code Cell
def endangered_cities(min_population, min_effective_magnitude):
    filter1_q = QUAKE_DF[QUAKE_DF["mag"] >= min_effective_magnitude]
    filter1_q["ep_mag"] = None
    for i,row in filter1_q.iterrows():
        filter1_q["ep_mag"][i] = epicenter_magnitude(row["mag"], row["depth"])
    filter2_q = filter1_q[filter1_q["ep_mag"] >= min_effective_magnitude]
    
    
    filter1_city = WC_DF[WC_DF["population"] >= min_population]
    sorted_city = filter1_city.sort_values(by=["country", "city_ascii"], ascending=[True, True])
    
    # this specification is only for testing
    sorted_city = sorted_city[sorted_city["country"] == "Japan"]
    # end of specification
    
    sorted_city["count"] = 0
    display(sorted_city)
    ed_c = []
    for i,row_c in sorted_city.iterrows():
        loc_c = (row_c["lat"], row_c["lng"])
        endanger = False
        for j,row_q in filter2_q.iterrows():
            loc_q = (row_q["latitude"],row_q["longitude"])
            dist = haversine_distance(loc_c, loc_q)
            ef_mag = effective_magnitude(row_q["mag"],row_q["depth"],dist)
            if ef_mag >= min_effective_magnitude:
                endanger = True
                break
#         if endanger :
#             ed_c.append((row_c["city_ascii"], row_c["country"], loc_c))
        ed_c.append((row_c["city_ascii"], row_c["country"], loc_c))
    
    return ed_c
        
endangered_cities(1000000, 0)
55/114:
## 2e Answer Code Cell
def endangered_cities(min_population, min_effective_magnitude):
    filter1_q = QUAKE_DF[QUAKE_DF["mag"] >= min_effective_magnitude]
    filter1_q["ep_mag"] = None
    for i,row in filter1_q.iterrows():
        filter1_q["ep_mag"][i] = epicenter_magnitude(row["mag"], row["depth"])
    filter2_q = filter1_q[filter1_q["ep_mag"] >= min_effective_magnitude]
    
    
    filter1_city = WC_DF[WC_DF["population"] >= min_population]
    sorted_city = filter1_city.sort_values(by=["country", "city_ascii"], ascending=[True, True])
    
    # this specification is only for testing
    sorted_city = sorted_city[sorted_city["country"] == "Japan"]
    # end of specification
    
    sorted_city["count"] = 0
    display(filter2_q)
    ed_c = []
    for i,row_c in sorted_city.iterrows():
        loc_c = (row_c["lat"], row_c["lng"])
        endanger = False
        for j,row_q in filter2_q.iterrows():
            loc_q = (row_q["latitude"],row_q["longitude"])
            dist = haversine_distance(loc_c, loc_q)
            ef_mag = effective_magnitude(row_q["mag"],row_q["depth"],dist)
            if ef_mag >= min_effective_magnitude:
                endanger = True
                break
        if endanger :
            ed_c.append((row_c["city_ascii"], row_c["country"], loc_c))
    
    return ed_c
        
endangered_cities(1000000, 0)
55/115:
## 2e Answer Code Cell
def endangered_cities(min_population, min_effective_magnitude):
    filter1_q = QUAKE_DF[QUAKE_DF["mag"] >= min_effective_magnitude]
    filter1_q["ep_mag"] = None
    for i,row in filter1_q.iterrows():
        filter1_q["ep_mag"][i] = epicenter_magnitude(row["mag"], row["depth"])
    filter2_q = filter1_q[filter1_q["ep_mag"] >= min_effective_magnitude]
    
    
    filter1_city = WC_DF[WC_DF["population"] >= min_population]
    sorted_city = filter1_city.sort_values(by=["country", "city_ascii"], ascending=[True, True])
    
    # this specification is only for testing
    sorted_city = sorted_city[sorted_city["country"] == "Japan"]
    # end of specification
    
    sorted_city["count"] = 0
    display(filter2_q)
    ed_c = []
    for i,row_c in sorted_city.iterrows():
        loc_c = (row_c["lat"], row_c["lng"])
        endanger = False
        for j,row_q in filter2_q.iterrows():
            loc_q = (row_q["latitude"],row_q["longitude"])
            dist = haversine_distance(loc_c, loc_q)
            ef_mag = effective_magnitude(row_q["mag"],row_q["depth"],dist)
            if ef_mag >= min_effective_magnitude:
                endanger = True
                break
        if endanger :
            ed_c.append((row_c["city_ascii"], row_c["country"], loc_c))
    
    return ed_c
        
endangered_cities(1000000, 6)
55/116:
## 2e Answer Code Cell
def endangered_cities(min_population, min_effective_magnitude):
    filter1_q = QUAKE_DF[QUAKE_DF["mag"] >= min_effective_magnitude]
    filter1_q["ep_mag"] = None
    for i,row in filter1_q.iterrows():
        filter1_q["ep_mag"][i] = epicenter_magnitude(row["mag"], row["depth"])
    filter2_q = filter1_q[filter1_q["ep_mag"] >= min_effective_magnitude]
    
    
    filter1_city = WC_DF[WC_DF["population"] >= min_population]
    sorted_city = filter1_city.sort_values(by=["country", "city_ascii"], ascending=[True, True])
    
    # this specification is only for testing
    sorted_city = sorted_city[sorted_city["country"] == "Japan"]
    # end of specification
    
    sorted_city["count"] = 0
    display(filter2_q)
    ed_c = []
    for i,row_c in sorted_city.iterrows():
        loc_c = (row_c["lat"], row_c["lng"])
        endanger = False
        for j,row_q in filter2_q.iterrows():
            loc_q = (row_q["latitude"],row_q["longitude"])
            dist = haversine_distance(loc_c, loc_q)
            ef_mag = effective_magnitude(row_q["mag"],row_q["depth"],dist)
            if ef_mag >= min_effective_magnitude:
                endanger = True
                break
        if endanger :
            ed_c.append((row_c["city_ascii"], row_c["country"], loc_c))
    
    return ed_c
        
endangered_cities(1000000, 3)
55/117:
## 2e Answer Code Cell
def endangered_cities(min_population, min_effective_magnitude):
    filter1_q = QUAKE_DF[QUAKE_DF["mag"] >= min_effective_magnitude]
    filter1_q["ep_mag"] = None
    for i,row in filter1_q.iterrows():
        filter1_q["ep_mag"][i] = epicenter_magnitude(row["mag"], row["depth"])
    filter2_q = filter1_q[filter1_q["ep_mag"] >= min_effective_magnitude]
    
    
    filter1_city = WC_DF[WC_DF["population"] >= min_population]
    sorted_city = filter1_city.sort_values(by=["country", "city_ascii"], ascending=[True, True])
    
    # this specification is only for testing
    sorted_city = sorted_city[sorted_city["country"] == "Japan"]
    # end of specification
    
    sorted_city["count"] = 0
    display(filter2_q)
    ed_c = []
    for i,row_c in sorted_city.iterrows():
        loc_c = (row_c["lat"], row_c["lng"])
        endanger = False
        for j,row_q in filter2_q.iterrows():
            loc_q = (row_q["latitude"],row_q["longitude"])
            dist = haversine_distance(loc_c, loc_q)
            print(row_q["mag"],row_q["depth"],dist)
            ef_mag = effective_magnitude(row_q["mag"],row_q["depth"],dist)
            if ef_mag >= min_effective_magnitude:
                endanger = True
                break
        if endanger :
            ed_c.append((row_c["city_ascii"], row_c["country"], loc_c))
    
    return ed_c
        
endangered_cities(1000000, 0)
55/118:
## 2e Answer Code Cell
def endangered_cities(min_population, min_effective_magnitude):
    filter1_q = QUAKE_DF[QUAKE_DF["mag"] >= min_effective_magnitude]
    filter1_q["ep_mag"] = None
    for i,row in filter1_q.iterrows():
        filter1_q["ep_mag"][i] = epicenter_magnitude(row["mag"], row["depth"])
    filter2_q = filter1_q[filter1_q["ep_mag"] >= min_effective_magnitude]
    
    
    filter1_city = WC_DF[WC_DF["population"] >= min_population]
    sorted_city = filter1_city.sort_values(by=["country", "city_ascii"], ascending=[True, True])
    
    # this specification is only for testing
    sorted_city = sorted_city[sorted_city["country"] == "Japan"]
    # end of specification
    
    sorted_city["count"] = 0
    ed_c = []
    for i,row_c in sorted_city.iterrows():
        loc_c = (row_c["lat"], row_c["lng"])
        endanger = False
        for j,row_q in filter2_q.iterrows():
            loc_q = (row_q["latitude"],row_q["longitude"])
            dist = haversine_distance(loc_c, loc_q)
            print(row_q["mag"],row_q["depth"],dist)
            ef_mag = effective_magnitude(row_q["mag"],row_q["depth"],dist)
            if ef_mag >= min_effective_magnitude:
                endanger = True
                break
        if endanger :
            ed_c.append((row_c["city_ascii"], row_c["country"], loc_c))
    
    return ed_c
        
endangered_cities(1000000, 0)
55/119:
## 2e Answer Code Cell
def endangered_cities(min_population, min_effective_magnitude):
    filter1_q = QUAKE_DF[QUAKE_DF["mag"] >= min_effective_magnitude]
    filter1_q["ep_mag"] = None
    for i,row in filter1_q.iterrows():
        filter1_q["ep_mag"][i] = epicenter_magnitude(row["mag"], row["depth"])
    filter2_q = filter1_q[filter1_q["ep_mag"] >= min_effective_magnitude]
    
    
    filter1_city = WC_DF[WC_DF["population"] >= min_population]
    sorted_city = filter1_city.sort_values(by=["country", "city_ascii"], ascending=[True, True])
    
    # this specification is only for testing
    sorted_city = sorted_city[sorted_city["country"] == "Japan"]
    # end of specification
    
    sorted_city["count"] = 0
    ed_c = []
    for i,row_c in sorted_city.iterrows():
        loc_c = (row_c["lat"], row_c["lng"])
        endanger = False
        for j,row_q in filter2_q.iterrows():
            loc_q = (row_q["latitude"],row_q["longitude"])
            dist = haversine_distance(loc_c, loc_q)
            print(row_q["mag"],row_q["depth"],dist)
            ef_mag = effective_magnitude(row_q["mag"],row_q["depth"],dist)
            print("ef",ef_mag)
            if ef_mag >= min_effective_magnitude:
                endanger = True
                break
        if endanger :
            ed_c.append((row_c["city_ascii"], row_c["country"], loc_c))
    
    return ed_c
        
endangered_cities(1000000, 0)
55/120:
## 2e Answer Code Cell
def endangered_cities(min_population, min_effective_magnitude):
    filter1_q = QUAKE_DF[QUAKE_DF["mag"] >= min_effective_magnitude]
    filter1_q["ep_mag"] = None
    for i,row in filter1_q.iterrows():
        filter1_q["ep_mag"][i] = epicenter_magnitude(row["mag"], row["depth"])
    filter2_q = filter1_q[filter1_q["ep_mag"] >= min_effective_magnitude]
    
    
    filter1_city = WC_DF[WC_DF["population"] >= min_population]
    sorted_city = filter1_city.sort_values(by=["country", "city_ascii"], ascending=[True, True])
    
    # this specification is only for testing
    sorted_city = sorted_city[sorted_city["country"] == "Japan"]
    # end of specification
    
    sorted_city["count"] = 0
    ed_c = []
    for i,row_c in sorted_city.iterrows():
        loc_c = (row_c["lat"], row_c["lng"])
        endanger = False
        for j,row_q in filter2_q.iterrows():
            loc_q = (row_q["latitude"],row_q["longitude"])
            dist = haversine_distance(loc_c, loc_q)
            ef_mag = effective_magnitude(row_q["mag"],row_q["depth"],dist)
            if ef_mag >= min_effective_magnitude:
                endanger = True
                break
        if endanger :
            ed_c.append((row_c["city_ascii"], row_c["country"], loc_c))
    
    return ed_c
        
endangered_cities(1000000, 0)
55/121:
## 2e Answer Code Cell
def endangered_cities(min_population, min_effective_magnitude):
    filter1_q = QUAKE_DF[QUAKE_DF["mag"] >= min_effective_magnitude]
    filter1_q["ep_mag"] = None
    for i,row in filter1_q.iterrows():
        filter1_q["ep_mag"][i] = epicenter_magnitude(row["mag"], row["depth"])
    filter2_q = filter1_q[filter1_q["ep_mag"] >= min_effective_magnitude]
    
    
    filter1_city = WC_DF[WC_DF["population"] >= min_population]
    sorted_city = filter1_city.sort_values(by=["country", "city_ascii"], ascending=[True, True])
    
    # this specification is only for testing
    sorted_city = sorted_city[sorted_city["country"] == "Japan"]
    # end of specification
    
    sorted_city["count"] = 0
    ed_c = []
    for i,row_c in sorted_city.iterrows():
        loc_c = (row_c["lat"], row_c["lng"])
        endanger = False
        for j,row_q in filter2_q.iterrows():
            loc_q = (row_q["latitude"],row_q["longitude"])
            dist = haversine_distance(loc_c, loc_q)
            ef_mag = effective_magnitude(row_q["mag"],row_q["depth"],dist)
            if ef_mag >= min_effective_magnitude:
                endanger = True
                break
        if endanger :
            print("i do")
            ed_c.append((row_c["city_ascii"], row_c["country"], loc_c))
    
    return ed_c
        
endangered_cities(1000000, 0)
55/122:
## 2e Answer Code Cell
def endangered_cities(min_population, min_effective_magnitude):
    filter1_q = QUAKE_DF[QUAKE_DF["mag"] >= min_effective_magnitude]
    filter1_q["ep_mag"] = None
    for i,row in filter1_q.iterrows():
        filter1_q["ep_mag"][i] = epicenter_magnitude(row["mag"], row["depth"])
    filter2_q = filter1_q[filter1_q["ep_mag"] >= min_effective_magnitude]
    
    
    filter1_city = WC_DF[WC_DF["population"] >= min_population]
    sorted_city = filter1_city.sort_values(by=["country", "city_ascii"], ascending=[True, True])
    
    # this specification is only for testing
    sorted_city = sorted_city[sorted_city["country"] == "Japan"]
    # end of specification
    
    sorted_city["count"] = 0
    ed_c = []
    for i,row_c in sorted_city.iterrows():
        loc_c = (row_c["lat"], row_c["lng"])
        endanger = False
        for j,row_q in filter2_q.iterrows():
            loc_q = (row_q["latitude"],row_q["longitude"])
            dist = haversine_distance(loc_c, loc_q)
            ef_mag = effective_magnitude(row_q["mag"],row_q["depth"],dist)
            if ef_mag >= min_effective_magnitude:
                print("i do")
                endanger = True
                break
        if endanger :
            ed_c.append((row_c["city_ascii"], row_c["country"], loc_c))
    
    return ed_c
        
endangered_cities(1000000, 0)
55/123:
## 2e Answer Code Cell
def endangered_cities(min_population, min_effective_magnitude):
    filter1_q = QUAKE_DF[QUAKE_DF["mag"] >= min_effective_magnitude]
    filter1_q["ep_mag"] = None
    for i,row in filter1_q.iterrows():
        filter1_q["ep_mag"][i] = epicenter_magnitude(row["mag"], row["depth"])
    filter2_q = filter1_q[filter1_q["ep_mag"] >= min_effective_magnitude]
    
    
    filter1_city = WC_DF[WC_DF["population"] >= min_population]
    sorted_city = filter1_city.sort_values(by=["country", "city_ascii"], ascending=[True, True])
    
    # this specification is only for testing
    sorted_city = sorted_city[sorted_city["country"] == "Japan"]
    # end of specification
    
    sorted_city["count"] = 0
    ed_c = []
    for i,row_c in sorted_city.iterrows():
        loc_c = (row_c["lat"], row_c["lng"])
        endanger = False
        for j,row_q in filter2_q.iterrows():
            loc_q = (row_q["latitude"],row_q["longitude"])
            dist = haversine_distance(loc_c, loc_q)
            ef_mag = effective_magnitude(row_q["mag"],row_q["depth"],dist)
            print(ef_mag)
            if ef_mag >= min_effective_magnitude:
                endanger = True
                break
        if endanger :
            ed_c.append((row_c["city_ascii"], row_c["country"], loc_c))
    
    return ed_c
        
endangered_cities(1000000, 0)
55/124:
## 2e Answer Code Cell
def endangered_cities(min_population, min_effective_magnitude):
    filter1_q = QUAKE_DF[QUAKE_DF["mag"] >= min_effective_magnitude]
    filter1_q["ep_mag"] = None
    for i,row in filter1_q.iterrows():
        filter1_q["ep_mag"][i] = epicenter_magnitude(row["mag"], row["depth"])
    filter2_q = filter1_q[filter1_q["ep_mag"] >= min_effective_magnitude]
    
    
    filter1_city = WC_DF[WC_DF["population"] >= min_population]
    sorted_city = filter1_city.sort_values(by=["country", "city_ascii"], ascending=[True, True])
    
#     # this specification is only for testing
#     sorted_city = sorted_city[sorted_city["country"] == "Japan"]
#     # end of specification
    
    sorted_city["count"] = 0
    ed_c = []
    for i,row_c in sorted_city.iterrows():
        loc_c = (row_c["lat"], row_c["lng"])
        endanger = False
        for j,row_q in filter2_q.iterrows():
            loc_q = (row_q["latitude"],row_q["longitude"])
            dist = haversine_distance(loc_c, loc_q)
            ef_mag = effective_magnitude(row_q["mag"],row_q["depth"],dist)
            if ef_mag >= min_effective_magnitude:
                endanger = True
                break
        if endanger :
            ed_c.append((row_c["city_ascii"], row_c["country"], loc_c))
    
    return ed_c
        
endangered_cities(1000000, 0)
55/125:
## 2e Answer Code Cell
def endangered_cities(min_population, min_effective_magnitude):
    filter1_q = QUAKE_DF[QUAKE_DF["mag"] >= min_effective_magnitude]
    filter1_q["ep_mag"] = None
    for i,row in filter1_q.iterrows():
        filter1_q["ep_mag"][i] = epicenter_magnitude(row["mag"], row["depth"])
    filter2_q = filter1_q[filter1_q["ep_mag"] >= min_effective_magnitude]
    
    
    filter1_city = WC_DF[WC_DF["population"] >= min_population]
    sorted_city = filter1_city.sort_values(by=["country", "city_ascii"], ascending=[True, True])
    
#     # this specification is only for testing
#     sorted_city = sorted_city[sorted_city["country"] == "Japan"]
#     # end of specification
    
    sorted_city["count"] = 0
    ed_c = []
    for i,row_c in sorted_city.iterrows():
        loc_c = (row_c["lat"], row_c["lng"])
        endanger = False
        for j,row_q in filter2_q.iterrows():
            loc_q = (row_q["latitude"],row_q["longitude"])
            dist = haversine_distance(loc_c, loc_q)
            ef_mag = effective_magnitude(row_q["mag"],row_q["depth"],dist)
            if ef_mag >= min_effective_magnitude:
                endanger = True
                break
        if endanger :
            ed_c.append((row_c["city_ascii"], row_c["country"], loc_c))
    
    return ed_c
        
endangered_cities(100000, 0)
55/126:
from ipyleaflet import Map, basemaps, basemap_to_tiles, Circle, Polyline
from ipywidgets import Layout

LEEDS_LOC  = ( 53.8008,  -1.5491  ) # Here we define the longitude and latitude of Leeds
WORLD_MAP = Map(basemap=basemaps.OpenTopoMap, center=LEEDS_LOC, zoom=1.5,
                layout=Layout(height="500px")) # Here we create a map object centred on Leeds

WORLD_MAP
55/127:
from ipyleaflet import Map, basemaps, basemap_to_tiles, Circle, Polyline
from ipywidgets import Layout

LEEDS_LOC  = ( 53.8008,  -1.5491  ) # Here we define the longitude and latitude of Leeds
WORLD_MAP = Map(basemap=basemaps.OpenTopoMap, center=LEEDS_LOC, zoom=1.5,
                layout=Layout(height="500px")) # Here we create a map object centred on Leeds

WORLD_MAP
56/1:
from ipyleaflet import Map, basemaps, basemap_to_tiles, Circle, Polyline
from ipywidgets import Layout

LEEDS_LOC  = ( 53.8008,  -1.5491  ) # Here we define the longitude and latitude of Leeds
WORLD_MAP = Map(basemap=basemaps.OpenTopoMap, center=LEEDS_LOC, zoom=1.5,
                layout=Layout(height="500px")) # Here we create a map object centred on Leeds

WORLD_MAP
58/1:
## WC_DF Initialisation

import pandas  ## This is the module for creating and manipulating DataFrames

WC_DF = pandas.read_csv("worldcities.csv")
58/2:
top_5_cities = WC_DF["city"][:5]   ## selects the first 5 items of the "city" column.
top_5_cities
58/3:
top_5_cities.index = list("abcde")
top_5_cities
58/4: top_5_cities.index
58/5: WC_DF["city"][:5].values
58/6: WC_DF.columns
58/7: list(WC_DF.columns)
58/8: WC_DF.iloc[1]
58/9:
for i, row in WC_DF.iterrows():
    print(i, row['city_ascii'], row['lat'], row['lng'])
    if i >3: break
58/10: WC_DF.sort_values(by=["country"], ascending=True)[:10] # Sorts countries by alphabet
58/11:
filtered_DF = WC_DF[ WC_DF['capital'] == 'admin'] # This keeps only administrative capitals
filtered_DF.head()
58/12:
# Question 1a answer cell

def non_ascii_cities():
    return set([row['city'] for i, row in WC_DF.iterrows() if row['city']!= row['city_ascii']])
58/13:
# Question 1b answer cell

def num_cities_occurring_n_times(n):
    counts = dict()
    for city in WC_DF["city"]:
        if city in counts:
            counts[city] += 1
        else:
            counts[city] = 1
    return [city for city in counts if counts[city] == n]
58/14:
def country_num_cities_dict():
    counts = dict()
    for country in WC_DF["country"]:
        if country in counts:
            counts[country] += 1
        else:
            counts[country] = 1
    return counts
58/15:
# Question 1d answer cell

def largest_cities_dataframe(n):
    return WC_DF.sort_values(by=["population"], ascending=False)[:n]

largest_cities_dataframe(10)
58/16:
# Complete question 1d in this cell

def big_cities_in_country(country, population): # country is a string argument
    sorted_df = WC_DF.sort_values(by=["population"], ascending=True)
    sorted_country = sorted_df[sorted_df['country'] == country]
    return sorted_country[sorted_country['population'] >= population]
58/17:
## Question 1e Answer Code Cell

def country_total_cities_population(country):
    country_df = WC_DF[WC_DF['country'] == country]
    population = 0
    for p in country_df["population"]:
        if not p!=p:
            population += p
    return population
58/18:
# Q2a answer code cell

import pandas    ## This is the module for creating and manupulating DataFrames

# Here we have assigned the url of the quake datasource to the global variable 
# 'QUAKE_SOURCE' for your convenience.
QUAKE_SOURCE = ( "http://earthquake.usgs.gov/" +
                 "earthquakes/feed/v1.0/summary/all_day.csv" )


QUAKE_DF = pandas.read_csv(QUAKE_SOURCE)  ## Modify this line to import the data using Pandas
58/19:
## If QUAKE_DF is a DataFrame, show the first 5 rows
try:
    if type(QUAKE_DF) == pandas.DataFrame:
        display(QUAKE_DF.head())
    else:
        print("QUAKE_DF is not a DataFrame")
except:
    print("QUAKE_DF has not been assigned a value")
58/20:
def show_deep_quakes( depth ):
    # make deep_quakes DataFrame by selecting rows from QUAKE_DF
    deep_quakes = QUAKE_DF[ QUAKE_DF["depth"] >= depth ]  ## This is how you select rows by a condition
                                                          ## on one of the column values.
        
    print("Number of quakes of depth {} or deeper:".format(depth), 
           len(deep_quakes.index))     ## This finds the number of rows of the deep_quakes DataFrame
    
    display(deep_quakes.sort_values("depth", ascending=False))  ## Sort by descending depth value
58/21:
try:
    show_deep_quakes(100)
except:
    print("Probably QUAKE_DF not correctly set")
58/22:
try:
    QUAKE_DF["depth"].max()
except:
    print("Probably QUAKE_DF not correctly set")
58/23:
try:
    QUAKE_DF["depth"].min()
except:
    print("Probably QUAKE_DF not correctly set")
58/24:
# Complete question 2b answer cell

def powerful_quakes(mag):
    deep_quakes = QUAKE_DF[ QUAKE_DF["mag"] >= mag ]
    return deep_quakes
58/25:
# Question 2c answer cell

def most_powerful_n_quakes(n):
    n -= 1
    sorted_df = QUAKE_DF.sort_values(by=["mag"], ascending=False)
    sorted_list = list(sorted_df["mag"])
    while (sorted_list[n] == sorted_list[n+1]):
        n+=1
    return sorted_df[:(n+1)]
58/26:
## Function to compute distance between locations (kilometres) 
# Returns the surface distance in meters, according to the Haversine formula,
# between two locations given as (latitude, longitude) coordinate pairs.

import math
def haversine_distance( loc1 , loc2 ): 
    '''finds the distance (m) between 2 locations, where locations are defined by
    longitudes and latitudes'''
    lat1, lon1 = loc1
    lat2, lon2 = loc2
    radius = 6371  # kilometers
    dlat = math.radians(lat2 - lat1)
    dlon = math.radians(lon2 - lon1)
    a = (math.sin(dlat / 2) * math.sin(dlat / 2) +
         math.cos(math.radians(lat1)) * math.cos(math.radians(lat2)) *
         math.sin(dlon / 2) * math.sin(dlon / 2))
    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))
    d = radius * c
    return d
58/27:
## 2d Answer Code Cell

def quake_distance_from_loc_dataframe(loc):
    alter_df = QUAKE_DF.copy()
    alter_df["distance_from_loc"] = None
    for i,row in alter_df.iterrows():
        alter_df["distance_from_loc"][i] = haversine_distance((row["latitude"],row["longitude"]),loc)
    return alter_df.sort_values(by=["distance_from_loc"], ascending=True)
58/28:
def effective_magnitude( magnitude, depth, surface_distance ):
    energy = 10**magnitude  # convert logarithmic magnitude to a linear energy value
    if depth < 1:   # Crude fix for small or negative depths (can occur where land is above sea level)
        depth = 1
    ## Calculate distance to source by Pythagorus (ignoring curvature of surface)
    dist_to_source_squared =  depth**2 + surface_distance**2
    ## Apply inverse square distance multiplier to get energy density at distance from source
    ## (Ignores damping effects)
    attenuated_energy = energy/dist_to_source_squared
    attenuated_magnitude =  math.log10(attenuated_energy) ## Convert back to a log base 10 scale
    return attenuated_magnitude

# Some test cases.
# effective_magnitude(9,100,500)
# effective_magnitude(6,50, 100)
58/29:
def epicenter_magnitude( magnitude, depth ):
    return effective_magnitude( magnitude, depth, 0)
58/30:
## 2e Answer Code Cell
def endangered_cities(min_population, min_effective_magnitude):
    filter1_q = QUAKE_DF[QUAKE_DF["mag"] >= min_effective_magnitude]
    filter1_q["ep_mag"] = None
    for i,row in filter1_q.iterrows():
        filter1_q["ep_mag"][i] = epicenter_magnitude(row["mag"], row["depth"])
    filter2_q = filter1_q[filter1_q["ep_mag"] >= min_effective_magnitude]
    
    
    filter1_city = WC_DF[WC_DF["population"] >= min_population]
    sorted_city = filter1_city.sort_values(by=["country", "city_ascii"], ascending=[True, True])
    
#     # this specification is only for testing
#     sorted_city = sorted_city[sorted_city["country"] == "Japan"]
#     # end of specification
    
    sorted_city["count"] = 0
    ed_c = []
    for i,row_c in sorted_city.iterrows():
        loc_c = (row_c["lat"], row_c["lng"])
        endanger = False
        for j,row_q in filter2_q.iterrows():
            loc_q = (row_q["latitude"],row_q["longitude"])
            dist = haversine_distance(loc_c, loc_q)
            ef_mag = effective_magnitude(row_q["mag"],row_q["depth"],dist)
            if ef_mag >= min_effective_magnitude:
                endanger = True
                break
        if endanger :
            ed_c.append((row_c["city_ascii"], row_c["country"], loc_c))
    
    return ed_c
        
endangered_cities(100000, 0)
58/31:
from ipyleaflet import Map, basemaps, basemap_to_tiles, Circle, Polyline
from ipywidgets import Layout

LEEDS_LOC  = ( 53.8008,  -1.5491  ) # Here we define the longitude and latitude of Leeds
WORLD_MAP = Map(basemap=basemaps.OpenTopoMap, center=LEEDS_LOC, zoom=1.5,
                layout=Layout(height="500px")) # Here we create a map object centred on Leeds

WORLD_MAP
58/32:
## 2e Answer Code Cell
def endangered_cities(min_population, min_effective_magnitude):
    filter1_q = QUAKE_DF[QUAKE_DF["mag"] >= min_effective_magnitude]
    filter1_q["ep_mag"] = None
    for i,row in filter1_q.iterrows():
        filter1_q["ep_mag"][i] = epicenter_magnitude(row["mag"], row["depth"])
    filter2_q = filter1_q[filter1_q["ep_mag"] >= min_effective_magnitude]
    
    
    filter1_city = WC_DF[WC_DF["population"] >= min_population]
    sorted_city = filter1_city.sort_values(by=["country", "city_ascii"], ascending=[True, True])
    
#     # this specification is only for testing
#     sorted_city = sorted_city[sorted_city["country"] == "Japan"]
#     # end of specification
    
    sorted_city["count"] = 0
    ed_c = []
    for i,row_c in sorted_city.iterrows():
        loc_c = (row_c["lat"], row_c["lng"])
        endanger = False
        for j,row_q in filter2_q.iterrows():
            loc_q = (row_q["latitude"],row_q["longitude"])
            dist = haversine_distance(loc_c, loc_q)
            ef_mag = effective_magnitude(row_q["mag"],row_q["depth"],dist)
            if ef_mag >= min_effective_magnitude:
                endanger = True
                break
        if endanger :
            ed_c.append((row_c["city_ascii"], row_c["country"], loc_c))
    
    return ed_c
        
endangered_cities(100000, 0)
58/33:
# Question 2c answer cell

def most_powerful_n_quakes(n):
    n -= 1
    sorted_df = QUAKE_DF.sort_values(by=["mag"], ascending=False)
    sorted_list = list(sorted_df["mag"])
    while (sorted_list[n] == sorted_list[n+1]):
        n+=1
    return sorted_df[:(n+1)]

most_powerful_n_quakes(10)
58/34:
# Question 2c answer cell

def most_powerful_n_quakes(n):
    n -= 1
    sorted_df = QUAKE_DF.sort_values(by=["mag"], ascending=False)
    sorted_list = list(sorted_df["mag"])
    while (sorted_list[n] == sorted_list[n+1]):
        n+=1
    return sorted_df[:(n+1)]

most_powerful_n_quakes(10)
58/35:
## WC_DF Initialisation

import pandas  ## This is the module for creating and manipulating DataFrames

WC_DF = pandas.read_csv("worldcities.csv")
58/36:
top_5_cities = WC_DF["city"][:5]   ## selects the first 5 items of the "city" column.
top_5_cities
58/37:
top_5_cities.index = list("abcde")
top_5_cities
58/38:
top_5_cities = WC_DF["city"][:5]   ## selects the first 5 items of the "city" column.
top_5_cities
58/39:
## 2e Answer Code Cell
def endangered_cities(min_population, min_effective_magnitude):
    filter1_q = QUAKE_DF[QUAKE_DF["mag"] >= min_effective_magnitude]
    filter1_q["ep_mag"] = None
    for i,row in filter1_q.iterrows():
        filter1_q["ep_mag"][i] = epicenter_magnitude(row["mag"], row["depth"])
    filter2_q = filter1_q[filter1_q["ep_mag"] >= min_effective_magnitude]
    
    
    filter1_city = WC_DF[WC_DF["population"] >= min_population]
    sorted_city = filter1_city.sort_values(by=["country", "city_ascii"], ascending=[True, True])
    
#     # this specification is only for testing
#     sorted_city = sorted_city[sorted_city["country"] == "Japan"]
#     # end of specification
    
    sorted_city["count"] = 0
    ed_c = []
    for i,row_c in sorted_city.iterrows():
        loc_c = (row_c["lat"], row_c["lng"])
        endanger = False
        for j,row_q in filter2_q.iterrows():
            loc_q = (row_q["latitude"],row_q["longitude"])
            dist = haversine_distance(loc_c, loc_q)
            ef_mag = effective_magnitude(row_q["mag"],row_q["depth"],dist)
            if ef_mag >= min_effective_magnitude:
                endanger = True
                break
        if endanger :
            ed_c.append((row_c["city_ascii"], row_c["country"], loc_c))
    
    return ed_c
        
endangered_cities(2000, 0.5)
58/40:
## 2e Answer Code Cell
def endangered_cities(min_population, min_effective_magnitude):
    filter1_q = QUAKE_DF[QUAKE_DF["mag"] >= min_effective_magnitude]
    filter1_q["ep_mag"] = None
    for i,row in filter1_q.iterrows():
        filter1_q["ep_mag"][i] = epicenter_magnitude(row["mag"], row["depth"])
    filter2_q = filter1_q[filter1_q["ep_mag"] >= min_effective_magnitude]
    
    
    filter1_city = WC_DF[WC_DF["population"] >= min_population]
    sorted_city = filter1_city.sort_values(by=["country", "city_ascii"], ascending=[True, True])
    
#     # this specification is only for testing
#     sorted_city = sorted_city[sorted_city["country"] == "Japan"]
#     # end of specification
    
    sorted_city["count"] = 0
    ed_c = []
    for i,row_c in sorted_city.iterrows():
        loc_c = (row_c["lat"], row_c["lng"])
        endanger = False
        for j,row_q in filter2_q.iterrows():
            loc_q = (row_q["latitude"],row_q["longitude"])
            dist = haversine_distance(loc_c, loc_q)
            ef_mag = effective_magnitude(row_q["mag"],row_q["depth"],dist)
            if ef_mag >= min_effective_magnitude:
                endanger = True
                break
        if endanger :
            ed_c.append((row_c["city_ascii"], row_c["country"], loc_c))
    
    return ed_c
        
endangered_cities(2000, 0.5)
59/1:
## WC_DF Initialisation

import pandas  ## This is the module for creating and manipulating DataFrames

WC_DF = pandas.read_csv("worldcities.csv")
59/2:
top_5_cities = WC_DF["city"][:5]   ## selects the first 5 items of the "city" column.
top_5_cities
59/3:
top_5_cities.index = list("abcde")
top_5_cities
59/4: top_5_cities.index
59/5: WC_DF["city"][:5].values
59/6: WC_DF.columns
59/7: list(WC_DF.columns)
59/8: WC_DF.iloc[1]
59/9:
for i, row in WC_DF.iterrows():
    print(i, row['city_ascii'], row['lat'], row['lng'])
    if i >3: break
59/10: WC_DF.sort_values(by=["country"], ascending=True)[:10] # Sorts countries by alphabet
59/11:
filtered_DF = WC_DF[ WC_DF['capital'] == 'admin'] # This keeps only administrative capitals
filtered_DF.head()
59/12:
# Question 1a answer cell

def non_ascii_cities():
    return set([row['city'] for i, row in WC_DF.iterrows() if row['city']!= row['city_ascii']])
59/13:
# Question 1b answer cell

def num_cities_occurring_n_times(n):
    counts = dict()
    for city in WC_DF["city"]:
        if city in counts:
            counts[city] += 1
        else:
            counts[city] = 1
    return [city for city in counts if counts[city] == n]
59/14:
def country_num_cities_dict():
    counts = dict()
    for country in WC_DF["country"]:
        if country in counts:
            counts[country] += 1
        else:
            counts[country] = 1
    return counts
59/15:
# Question 1d answer cell

def largest_cities_dataframe(n):
    return WC_DF.sort_values(by=["population"], ascending=False)[:n]

largest_cities_dataframe(10)
59/16:
# Complete question 1d in this cell

def big_cities_in_country(country, population): # country is a string argument
    sorted_df = WC_DF.sort_values(by=["population"], ascending=True)
    sorted_country = sorted_df[sorted_df['country'] == country]
    return sorted_country[sorted_country['population'] >= population]
59/17:
## Question 1e Answer Code Cell

def country_total_cities_population(country):
    country_df = WC_DF[WC_DF['country'] == country]
    population = 0
    for p in country_df["population"]:
        if not p!=p:
            population += p
    return population
59/18:
# Q2a answer code cell

import pandas    ## This is the module for creating and manupulating DataFrames

# Here we have assigned the url of the quake datasource to the global variable 
# 'QUAKE_SOURCE' for your convenience.
QUAKE_SOURCE = ( "http://earthquake.usgs.gov/" +
                 "earthquakes/feed/v1.0/summary/all_day.csv" )


QUAKE_DF = pandas.read_csv(QUAKE_SOURCE)  ## Modify this line to import the data using Pandas
59/19:
## If QUAKE_DF is a DataFrame, show the first 5 rows
try:
    if type(QUAKE_DF) == pandas.DataFrame:
        display(QUAKE_DF.head())
    else:
        print("QUAKE_DF is not a DataFrame")
except:
    print("QUAKE_DF has not been assigned a value")
59/20:
def show_deep_quakes( depth ):
    # make deep_quakes DataFrame by selecting rows from QUAKE_DF
    deep_quakes = QUAKE_DF[ QUAKE_DF["depth"] >= depth ]  ## This is how you select rows by a condition
                                                          ## on one of the column values.
        
    print("Number of quakes of depth {} or deeper:".format(depth), 
           len(deep_quakes.index))     ## This finds the number of rows of the deep_quakes DataFrame
    
    display(deep_quakes.sort_values("depth", ascending=False))  ## Sort by descending depth value
59/21:
try:
    show_deep_quakes(100)
except:
    print("Probably QUAKE_DF not correctly set")
59/22:
try:
    QUAKE_DF["depth"].max()
except:
    print("Probably QUAKE_DF not correctly set")
59/23:
try:
    QUAKE_DF["depth"].min()
except:
    print("Probably QUAKE_DF not correctly set")
59/24:
# Complete question 2b answer cell

def powerful_quakes(mag):
    deep_quakes = QUAKE_DF[ QUAKE_DF["mag"] >= mag ]
    return deep_quakes
59/25:
# Question 2c answer cell

def most_powerful_n_quakes(n):
    n -= 1
    sorted_df = QUAKE_DF.sort_values(by=["mag"], ascending=False)
    sorted_list = list(sorted_df["mag"])
    while (sorted_list[n] == sorted_list[n+1]):
        n+=1
    return sorted_df[:(n+1)]

most_powerful_n_quakes(10)
59/26:
## Function to compute distance between locations (kilometres) 
# Returns the surface distance in meters, according to the Haversine formula,
# between two locations given as (latitude, longitude) coordinate pairs.

import math
def haversine_distance( loc1 , loc2 ): 
    '''finds the distance (m) between 2 locations, where locations are defined by
    longitudes and latitudes'''
    lat1, lon1 = loc1
    lat2, lon2 = loc2
    radius = 6371  # kilometers
    dlat = math.radians(lat2 - lat1)
    dlon = math.radians(lon2 - lon1)
    a = (math.sin(dlat / 2) * math.sin(dlat / 2) +
         math.cos(math.radians(lat1)) * math.cos(math.radians(lat2)) *
         math.sin(dlon / 2) * math.sin(dlon / 2))
    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))
    d = radius * c
    return d
59/27:
## 2d Answer Code Cell

def quake_distance_from_loc_dataframe(loc):
    alter_df = QUAKE_DF.copy()
    alter_df["distance_from_loc"] = None
    for i,row in alter_df.iterrows():
        alter_df["distance_from_loc"][i] = haversine_distance((row["latitude"],row["longitude"]),loc)
    return alter_df.sort_values(by=["distance_from_loc"], ascending=True)
59/28:
def effective_magnitude( magnitude, depth, surface_distance ):
    energy = 10**magnitude  # convert logarithmic magnitude to a linear energy value
    if depth < 1:   # Crude fix for small or negative depths (can occur where land is above sea level)
        depth = 1
    ## Calculate distance to source by Pythagorus (ignoring curvature of surface)
    dist_to_source_squared =  depth**2 + surface_distance**2
    ## Apply inverse square distance multiplier to get energy density at distance from source
    ## (Ignores damping effects)
    attenuated_energy = energy/dist_to_source_squared
    attenuated_magnitude =  math.log10(attenuated_energy) ## Convert back to a log base 10 scale
    return attenuated_magnitude

# Some test cases.
# effective_magnitude(9,100,500)
# effective_magnitude(6,50, 100)
59/29:
def epicenter_magnitude( magnitude, depth ):
    return effective_magnitude( magnitude, depth, 0)
59/30:
## 2e Answer Code Cell
def endangered_cities(min_population, min_effective_magnitude):
    filter1_q = QUAKE_DF[QUAKE_DF["mag"] >= min_effective_magnitude]
    filter1_q["ep_mag"] = None
    for i,row in filter1_q.iterrows():
        filter1_q["ep_mag"][i] = epicenter_magnitude(row["mag"], row["depth"])
    filter2_q = filter1_q[filter1_q["ep_mag"] >= min_effective_magnitude]
    
    
    filter1_city = WC_DF[WC_DF["population"] >= min_population]
    sorted_city = filter1_city.sort_values(by=["country", "city_ascii"], ascending=[True, True])
    
#     # this specification is only for testing
#     sorted_city = sorted_city[sorted_city["country"] == "Japan"]
#     # end of specification
    
    sorted_city["count"] = 0
    ed_c = []
    for i,row_c in sorted_city.iterrows():
        loc_c = (row_c["lat"], row_c["lng"])
        endanger = False
        for j,row_q in filter2_q.iterrows():
            loc_q = (row_q["latitude"],row_q["longitude"])
            dist = haversine_distance(loc_c, loc_q)
            ef_mag = effective_magnitude(row_q["mag"],row_q["depth"],dist)
            if ef_mag >= min_effective_magnitude:
                endanger = True
                break
        if endanger :
            ed_c.append((row_c["city_ascii"], row_c["country"], loc_c))
    
    return ed_c
        

endangered_cities(2000, 0.5)
61/1:
## WC_DF Initialisation

import pandas  ## This is the module for creating and manipulating DataFrames

WC_DF = pandas.read_csv("worldcities.csv")
61/2:
top_5_cities = WC_DF["city"][:5]   ## selects the first 5 items of the "city" column.
top_5_cities
61/3:
top_5_cities.index = list("abcde")
top_5_cities
61/4: top_5_cities.index
61/5: WC_DF["city"][:5].values
61/6: WC_DF.columns
61/7: list(WC_DF.columns)
61/8: WC_DF.iloc[1]
61/9:
for i, row in WC_DF.iterrows():
    print(i, row['city_ascii'], row['lat'], row['lng'])
    if i >3: break
61/10: WC_DF.sort_values(by=["country"], ascending=True)[:10] # Sorts countries by alphabet
61/11:
filtered_DF = WC_DF[ WC_DF['capital'] == 'admin'] # This keeps only administrative capitals
filtered_DF.head()
61/12:
# Question 1a answer cell

def non_ascii_cities():
    return set([row['city'] for i, row in WC_DF.iterrows() if row['city']!= row['city_ascii']])
61/13:
# Question 1b answer cell

def num_cities_occurring_n_times(n):
    counts = dict()
    for city in WC_DF["city"]:
        if city in counts:
            counts[city] += 1
        else:
            counts[city] = 1
    return [city for city in counts if counts[city] == n]
61/14:
def country_num_cities_dict():
    counts = dict()
    for country in WC_DF["country"]:
        if country in counts:
            counts[country] += 1
        else:
            counts[country] = 1
    return counts
61/15:
# Question 1d answer cell

def largest_cities_dataframe(n):
    return WC_DF.sort_values(by=["population"], ascending=False)[:n]

largest_cities_dataframe(10)
61/16:
# Complete question 1d in this cell

def big_cities_in_country(country, population): # country is a string argument
    sorted_df = WC_DF.sort_values(by=["population"], ascending=True)
    sorted_country = sorted_df[sorted_df['country'] == country]
    return sorted_country[sorted_country['population'] >= population]
61/17:
## Question 1e Answer Code Cell

def country_total_cities_population(country):
    country_df = WC_DF[WC_DF['country'] == country]
    population = 0
    for p in country_df["population"]:
        if not p!=p:
            population += p
    return population
61/18:
# Q2a answer code cell

import pandas    ## This is the module for creating and manupulating DataFrames

# Here we have assigned the url of the quake datasource to the global variable 
# 'QUAKE_SOURCE' for your convenience.
QUAKE_SOURCE = ( "http://earthquake.usgs.gov/" +
                 "earthquakes/feed/v1.0/summary/all_day.csv" )


QUAKE_DF = pandas.read_csv(QUAKE_SOURCE)  ## Modify this line to import the data using Pandas
61/19:
## If QUAKE_DF is a DataFrame, show the first 5 rows
try:
    if type(QUAKE_DF) == pandas.DataFrame:
        display(QUAKE_DF.head())
    else:
        print("QUAKE_DF is not a DataFrame")
except:
    print("QUAKE_DF has not been assigned a value")
61/20:
def show_deep_quakes( depth ):
    # make deep_quakes DataFrame by selecting rows from QUAKE_DF
    deep_quakes = QUAKE_DF[ QUAKE_DF["depth"] >= depth ]  ## This is how you select rows by a condition
                                                          ## on one of the column values.
        
    print("Number of quakes of depth {} or deeper:".format(depth), 
           len(deep_quakes.index))     ## This finds the number of rows of the deep_quakes DataFrame
    
    display(deep_quakes.sort_values("depth", ascending=False))  ## Sort by descending depth value
61/21:
try:
    show_deep_quakes(100)
except:
    print("Probably QUAKE_DF not correctly set")
61/22:
try:
    QUAKE_DF["depth"].max()
except:
    print("Probably QUAKE_DF not correctly set")
61/23:
try:
    QUAKE_DF["depth"].min()
except:
    print("Probably QUAKE_DF not correctly set")
61/24:
# Complete question 2b answer cell

def powerful_quakes(mag):
    deep_quakes = QUAKE_DF[ QUAKE_DF["mag"] >= mag ]
    return deep_quakes
61/25:
# Question 2c answer cell

def most_powerful_n_quakes(n):
    n -= 1
    sorted_df = QUAKE_DF.sort_values(by=["mag"], ascending=False)
    sorted_list = list(sorted_df["mag"])
    while (sorted_list[n] == sorted_list[n+1]):
        n+=1
    return sorted_df[:(n+1)]

most_powerful_n_quakes(10)
61/26:
## Function to compute distance between locations (kilometres) 
# Returns the surface distance in meters, according to the Haversine formula,
# between two locations given as (latitude, longitude) coordinate pairs.

import math
def haversine_distance( loc1 , loc2 ): 
    '''finds the distance (m) between 2 locations, where locations are defined by
    longitudes and latitudes'''
    lat1, lon1 = loc1
    lat2, lon2 = loc2
    radius = 6371  # kilometers
    dlat = math.radians(lat2 - lat1)
    dlon = math.radians(lon2 - lon1)
    a = (math.sin(dlat / 2) * math.sin(dlat / 2) +
         math.cos(math.radians(lat1)) * math.cos(math.radians(lat2)) *
         math.sin(dlon / 2) * math.sin(dlon / 2))
    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))
    d = radius * c
    return d
61/27:
## 2d Answer Code Cell

def quake_distance_from_loc_dataframe(loc):
    alter_df = QUAKE_DF.copy()
    alter_df["distance_from_loc"] = None
    for i,row in alter_df.iterrows():
        alter_df["distance_from_loc"][i] = haversine_distance((row["latitude"],row["longitude"]),loc)
    return alter_df.sort_values(by=["distance_from_loc"], ascending=True)
61/28:
def effective_magnitude( magnitude, depth, surface_distance ):
    energy = 10**magnitude  # convert logarithmic magnitude to a linear energy value
    if depth < 1:   # Crude fix for small or negative depths (can occur where land is above sea level)
        depth = 1
    ## Calculate distance to source by Pythagorus (ignoring curvature of surface)
    dist_to_source_squared =  depth**2 + surface_distance**2
    ## Apply inverse square distance multiplier to get energy density at distance from source
    ## (Ignores damping effects)
    attenuated_energy = energy/dist_to_source_squared
    attenuated_magnitude =  math.log10(attenuated_energy) ## Convert back to a log base 10 scale
    return attenuated_magnitude

# Some test cases.
# effective_magnitude(9,100,500)
# effective_magnitude(6,50, 100)
61/29:
def epicenter_magnitude( magnitude, depth ):
    return effective_magnitude( magnitude, depth, 0)
61/30:
## 2e Answer Code Cell
def endangered_cities(min_population, min_effective_magnitude):
    filter1_q = QUAKE_DF[QUAKE_DF["mag"] >= min_effective_magnitude]
    filter1_q["ep_mag"] = None
    for i,row in filter1_q.iterrows():
        filter1_q["ep_mag"][i] = epicenter_magnitude(row["mag"], row["depth"])
    filter2_q = filter1_q[filter1_q["ep_mag"] >= min_effective_magnitude]
    
    
    filter1_city = WC_DF[WC_DF["population"] >= min_population]
    sorted_city = filter1_city.sort_values(by=["country", "city_ascii"], ascending=[True, True])
    
#     # this specification is only for testing
#     sorted_city = sorted_city[sorted_city["country"] == "Japan"]
#     # end of specification
    
    sorted_city["count"] = 0
    ed_c = []
    for i,row_c in sorted_city.iterrows():
        loc_c = (row_c["lat"], row_c["lng"])
        endanger = False
        for j,row_q in filter2_q.iterrows():
            loc_q = (row_q["latitude"],row_q["longitude"])
            dist = haversine_distance(loc_c, loc_q)
            ef_mag = effective_magnitude(row_q["mag"],row_q["depth"],dist)
            if ef_mag >= min_effective_magnitude:
                endanger = True
                break
        if endanger :
            ed_c.append((row_c["city_ascii"], row_c["country"], loc_c))
    
    return ed_c
        

endangered_cities(2000, 0.5)
62/1:
## WC_DF Initialisation

import pandas  ## This is the module for creating and manipulating DataFrames

WC_DF = pandas.read_csv("worldcities.csv")
62/2:
top_5_cities = WC_DF["city"][:5]   ## selects the first 5 items of the "city" column.
top_5_cities
62/3:
top_5_cities.index = list("abcde")
top_5_cities
62/4: top_5_cities.index
62/5: WC_DF["city"][:5].values
62/6: WC_DF.columns
62/7: list(WC_DF.columns)
62/8: WC_DF.iloc[1]
62/9:
for i, row in WC_DF.iterrows():
    print(i, row['city_ascii'], row['lat'], row['lng'])
    if i >3: break
62/10: WC_DF.sort_values(by=["country"], ascending=True)[:10] # Sorts countries by alphabet
62/11:
filtered_DF = WC_DF[ WC_DF['capital'] == 'admin'] # This keeps only administrative capitals
filtered_DF.head()
62/12:
# Question 1a answer cell

def non_ascii_cities():
    return set([row['city'] for i, row in WC_DF.iterrows() if row['city']!= row['city_ascii']])
62/13:
# Question 1b answer cell

def num_cities_occurring_n_times(n):
    counts = dict()
    for city in WC_DF["city"]:
        if city in counts:
            counts[city] += 1
        else:
            counts[city] = 1
    return [city for city in counts if counts[city] == n]
62/14:
def country_num_cities_dict():
    counts = dict()
    for country in WC_DF["country"]:
        if country in counts:
            counts[country] += 1
        else:
            counts[country] = 1
    return counts
62/15:
# Question 1d answer cell

def largest_cities_dataframe(n):
    return WC_DF.sort_values(by=["population"], ascending=False)[:n]

largest_cities_dataframe(10)
62/16:
# Complete question 1d in this cell

def big_cities_in_country(country, population): # country is a string argument
    sorted_df = WC_DF.sort_values(by=["population"], ascending=True)
    sorted_country = sorted_df[sorted_df['country'] == country]
    return sorted_country[sorted_country['population'] >= population]
62/17:
## Question 1e Answer Code Cell

def country_total_cities_population(country):
    country_df = WC_DF[WC_DF['country'] == country]
    population = 0
    for p in country_df["population"]:
        if not p!=p:
            population += p
    return population
62/18:
# Q2a answer code cell

import pandas    ## This is the module for creating and manupulating DataFrames

# Here we have assigned the url of the quake datasource to the global variable 
# 'QUAKE_SOURCE' for your convenience.
QUAKE_SOURCE = ( "http://earthquake.usgs.gov/" +
                 "earthquakes/feed/v1.0/summary/all_day.csv" )


QUAKE_DF = pandas.read_csv(QUAKE_SOURCE)  ## Modify this line to import the data using Pandas
62/19:
## If QUAKE_DF is a DataFrame, show the first 5 rows
try:
    if type(QUAKE_DF) == pandas.DataFrame:
        display(QUAKE_DF.head())
    else:
        print("QUAKE_DF is not a DataFrame")
except:
    print("QUAKE_DF has not been assigned a value")
62/20:
def show_deep_quakes( depth ):
    # make deep_quakes DataFrame by selecting rows from QUAKE_DF
    deep_quakes = QUAKE_DF[ QUAKE_DF["depth"] >= depth ]  ## This is how you select rows by a condition
                                                          ## on one of the column values.
        
    print("Number of quakes of depth {} or deeper:".format(depth), 
           len(deep_quakes.index))     ## This finds the number of rows of the deep_quakes DataFrame
    
    display(deep_quakes.sort_values("depth", ascending=False))  ## Sort by descending depth value
62/21:
try:
    show_deep_quakes(100)
except:
    print("Probably QUAKE_DF not correctly set")
62/22:
try:
    QUAKE_DF["depth"].max()
except:
    print("Probably QUAKE_DF not correctly set")
62/23:
try:
    QUAKE_DF["depth"].min()
except:
    print("Probably QUAKE_DF not correctly set")
62/24:
# Complete question 2b answer cell

def powerful_quakes(mag):
    deep_quakes = QUAKE_DF[ QUAKE_DF["mag"] >= mag ]
    return deep_quakes
62/25:
# Question 2c answer cell

def most_powerful_n_quakes(n):
    n -= 1
    sorted_df = QUAKE_DF.sort_values(by=["mag"], ascending=False)
    sorted_list = list(sorted_df["mag"])
    while (sorted_list[n] == sorted_list[n+1]):
        n+=1
    return sorted_df[:(n+1)]

most_powerful_n_quakes(10)
62/26:
## Function to compute distance between locations (kilometres) 
# Returns the surface distance in meters, according to the Haversine formula,
# between two locations given as (latitude, longitude) coordinate pairs.

import math
def haversine_distance( loc1 , loc2 ): 
    '''finds the distance (m) between 2 locations, where locations are defined by
    longitudes and latitudes'''
    lat1, lon1 = loc1
    lat2, lon2 = loc2
    radius = 6371  # kilometers
    dlat = math.radians(lat2 - lat1)
    dlon = math.radians(lon2 - lon1)
    a = (math.sin(dlat / 2) * math.sin(dlat / 2) +
         math.cos(math.radians(lat1)) * math.cos(math.radians(lat2)) *
         math.sin(dlon / 2) * math.sin(dlon / 2))
    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))
    d = radius * c
    return d
62/27:
## 2d Answer Code Cell

def quake_distance_from_loc_dataframe(loc):
    alter_df = QUAKE_DF.copy()
    alter_df["distance_from_loc"] = None
    for i,row in alter_df.iterrows():
        alter_df["distance_from_loc"][i] = haversine_distance((row["latitude"],row["longitude"]),loc)
    return alter_df.sort_values(by=["distance_from_loc"], ascending=True)
62/28:
def effective_magnitude( magnitude, depth, surface_distance ):
    energy = 10**magnitude  # convert logarithmic magnitude to a linear energy value
    if depth < 1:   # Crude fix for small or negative depths (can occur where land is above sea level)
        depth = 1
    ## Calculate distance to source by Pythagorus (ignoring curvature of surface)
    dist_to_source_squared =  depth**2 + surface_distance**2
    ## Apply inverse square distance multiplier to get energy density at distance from source
    ## (Ignores damping effects)
    attenuated_energy = energy/dist_to_source_squared
    attenuated_magnitude =  math.log10(attenuated_energy) ## Convert back to a log base 10 scale
    return attenuated_magnitude

# Some test cases.
# effective_magnitude(9,100,500)
# effective_magnitude(6,50, 100)
62/29:
def epicenter_magnitude( magnitude, depth ):
    return effective_magnitude( magnitude, depth, 0)
62/30:
## 2e Answer Code Cell
def endangered_cities(min_population, min_effective_magnitude):
    filter1_q = QUAKE_DF[QUAKE_DF["mag"] >= min_effective_magnitude]
    filter1_q["ep_mag"] = None
    for i,row in filter1_q.iterrows():
        filter1_q["ep_mag"][i] = epicenter_magnitude(row["mag"], row["depth"])
    filter2_q = filter1_q[filter1_q["ep_mag"] >= min_effective_magnitude]
    
    
    filter1_city = WC_DF[WC_DF["population"] >= min_population]
    sorted_city = filter1_city.sort_values(by=["country", "city_ascii"], ascending=[True, True])
    
#     # this specification is only for testing
#     sorted_city = sorted_city[sorted_city["country"] == "Japan"]
#     # end of specification
    
    sorted_city["count"] = 0
    ed_c = []
    for i,row_c in sorted_city.iterrows():
        loc_c = (row_c["lat"], row_c["lng"])
        endanger = False
        for j,row_q in filter2_q.iterrows():
            loc_q = (row_q["latitude"],row_q["longitude"])
            dist = haversine_distance(loc_c, loc_q)
            ef_mag = effective_magnitude(row_q["mag"],row_q["depth"],dist)
            if ef_mag >= min_effective_magnitude:
                endanger = True
                break
        if endanger :
            ed_c.append((row_c["city_ascii"], row_c["country"], loc_c))
    
    return ed_c
        
print("nah")
# endangered_cities(2000, 0.5)
62/31:
from ipyleaflet import Map, basemaps, basemap_to_tiles, Circle, Polyline
from ipywidgets import Layout

LEEDS_LOC  = ( 53.8008,  -1.5491  ) # Here we define the longitude and latitude of Leeds
WORLD_MAP = Map(basemap=basemaps.OpenTopoMap, center=LEEDS_LOC, zoom=1.5,
                layout=Layout(height="500px")) # Here we create a map object centred on Leeds

WORLD_MAP
62/32:
## 2e Answer Code Cell
def endangered_cities(min_population, min_effective_magnitude):
    filter1_q = QUAKE_DF[QUAKE_DF["mag"] >= min_effective_magnitude]
    filter1_q["ep_mag"] = None
    for i,row in filter1_q.iterrows():
        filter1_q["ep_mag"][i] = epicenter_magnitude(row["mag"], row["depth"])
    filter2_q = filter1_q[filter1_q["ep_mag"] >= min_effective_magnitude]
    
    
    filter1_city = WC_DF[WC_DF["population"] >= min_population]
    sorted_city = filter1_city.sort_values(by=["country", "city_ascii"], ascending=[True, True])
    
#     # this specification is only for testing
#     sorted_city = sorted_city[sorted_city["country"] == "Japan"]
#     # end of specification
    
    sorted_city["count"] = 0
    ed_c = []
    for i,row_c in sorted_city.iterrows():
        loc_c = (row_c["lat"], row_c["lng"])
        endanger = False
        for j,row_q in filter2_q.iterrows():
            loc_q = (row_q["latitude"],row_q["longitude"])
            dist = haversine_distance(loc_c, loc_q)
            ef_mag = effective_magnitude(row_q["mag"],row_q["depth"],dist)
            if ef_mag >= min_effective_magnitude:
                endanger = True
                break
        if endanger :
            ed_c.append((row_c["city_ascii"], row_c["country"], loc_c))
    
    return ed_c
        
# print("nah")
endangered_cities(20000, 0.5)
62/33:
## 2e Answer Code Cell
def endangered_cities(min_population, min_effective_magnitude):
    filter1_q = QUAKE_DF[QUAKE_DF["mag"] >= min_effective_magnitude]
    filter1_q["ep_mag"] = None
    for i,row in filter1_q.iterrows():
        filter1_q["ep_mag"][i] = epicenter_magnitude(row["mag"], row["depth"])
    filter2_q = filter1_q[filter1_q["ep_mag"] >= min_effective_magnitude]
    
    
    filter1_city = WC_DF[WC_DF["population"] >= min_population]
    sorted_city = filter1_city.sort_values(by=["country", "city_ascii"], ascending=[True, True])
    
#     # this specification is only for testing
#     sorted_city = sorted_city[sorted_city["country"] == "Japan"]
#     # end of specification
    
    sorted_city["count"] = 0
    ed_c = []
    for i,row_c in sorted_city.iterrows():
        loc_c = (row_c["lat"], row_c["lng"])
        endanger = False
        for j,row_q in filter2_q.iterrows():
            loc_q = (row_q["latitude"],row_q["longitude"])
            dist = haversine_distance(loc_c, loc_q)
            ef_mag = effective_magnitude(row_q["mag"],row_q["depth"],dist)
            if ef_mag >= min_effective_magnitude:
                endanger = True
                break
        if endanger :
            ed_c.append((row_c["city_ascii"], row_c["country"], loc_c))
    
    return ed_c
        
# print("nah")
endangered_cities(2000, 0.5)
63/1:
# Question 1b answer cell

def num_cities_occurring_n_times(n):
    counts = dict()
    for city in WC_DF["city"]:
        if city in counts:
            counts[city] += 1
        else:
            counts[city] = 1
    return [city for city in counts if counts[city] == n]
63/2:
def country_num_cities_dict():
    counts = dict()
    for country in WC_DF["country"]:
        if country in counts:
            counts[country] += 1
        else:
            counts[country] = 1
    return counts
63/3:
# Question 1b answer cell

def num_cities_occurring_n_times(n):
    counts = dict()
    for city in WC_DF["city"]:
        if city in counts:
            counts[city] += 1
        else:
            counts[city] = 1
    return [city for city in counts if counts[city] == n]

num_cities_occurring_n_times(3)
63/4:
## WC_DF Initialisation

import pandas  ## This is the module for creating and manipulating DataFrames

WC_DF = pandas.read_csv("worldcities.csv")
63/5:
top_5_cities = WC_DF["city"][:5]   ## selects the first 5 items of the "city" column.
top_5_cities
63/6:
top_5_cities.index = list("abcde")
top_5_cities
63/7: top_5_cities.index
63/8: WC_DF["city"][:5].values
63/9: WC_DF.columns
63/10: list(WC_DF.columns)
63/11: WC_DF.iloc[1]
63/12:
for i, row in WC_DF.iterrows():
    print(i, row['city_ascii'], row['lat'], row['lng'])
    if i >3: break
63/13: WC_DF.sort_values(by=["country"], ascending=True)[:10] # Sorts countries by alphabet
63/14:
filtered_DF = WC_DF[ WC_DF['capital'] == 'admin'] # This keeps only administrative capitals
filtered_DF.head()
63/15:
# Question 1a answer cell

def non_ascii_cities():
    return set([row['city'] for i, row in WC_DF.iterrows() if row['city']!= row['city_ascii']])
63/16:
# Question 1b answer cell

def num_cities_occurring_n_times(n):
    counts = dict()
    for city in WC_DF["city"]:
        if city in counts:
            counts[city] += 1
        else:
            counts[city] = 1
    return [city for city in counts if counts[city] == n]

num_cities_occurring_n_times(3)
63/17:
# Question 1b answer cell

def num_cities_occurring_n_times(n):
    counts = dict()
    for city in WC_DF["city"]:
        if city in counts:
            counts[city] += 1
        else:
            counts[city] = 1
    return len(counts)

num_cities_occurring_n_times(3)
63/18:
# Complete question 1d in this cell

for i in WC_DF[:10]:
    print(i)

def big_cities_in_country(country, population): # country is a string argument
    sorted_df = WC_DF.sort_values(by=["population"], ascending=True)
    sorted_country = sorted_df[sorted_df['country'] == country]
    filter_population = sorted_country[sorted_country['population'] >= population]
    return [()]
63/19:
# Complete question 1d in this cell

print(WC_DF["country"][10])

def big_cities_in_country(country, population): # country is a string argument
    sorted_df = WC_DF.sort_values(by=["population"], ascending=True)
    sorted_country = sorted_df[sorted_df['country'] == country]
    filter_population = sorted_country[sorted_country['population'] >= population]
    return [()]
63/20:
# Complete question 1d in this cell

print((WC_DF["country"][10],WC_DF["population"][10])

def big_cities_in_country(country, population): # country is a string argument
    sorted_df = WC_DF.sort_values(by=["population"], ascending=True)
    sorted_country = sorted_df[sorted_df['country'] == country]
    filter_population = sorted_country[sorted_country['population'] >= population]
    return [()]
63/21:
# Complete question 1d in this cell

print((WC_DF["country"][10],WC_DF["population"][10]))

def big_cities_in_country(country, population): # country is a string argument
    sorted_df = WC_DF.sort_values(by=["population"], ascending=True)
    sorted_country = sorted_df[sorted_df['country'] == country]
    filter_population = sorted_country[sorted_country['population'] >= population]
    return [()]
63/22:
# Complete question 1d in this cell


def big_cities_in_country(country, population): # country is a string argument
    sorted_df = WC_DF.sort_values(by=["population"], ascending=True)
    sorted_country = sorted_df[sorted_df['country'] == country]
    filter_population = sorted_country[sorted_country['population'] >= population]
    return [(WC_DF["country"][i],WC_DF["population"][i]) for i in range(len(filter_population))]
63/23:
# Complete question 1d in this cell


def big_cities_in_country(country, population): # country is a string argument
    sorted_df = WC_DF.sort_values(by=["population"], ascending=True)
    sorted_country = sorted_df[sorted_df['country'] == country]
    filter_population = sorted_country[sorted_country['population'] >= population]
    return [(WC_DF["country"][i],WC_DF["population"][i]) for i in range(len(filter_population))]

big_cities_in_country("Japan",1000000)
63/24:
# Complete question 1d in this cell


def big_cities_in_country(country, population): # country is a string argument
    sorted_df = WC_DF.sort_values(by=["population"], ascending=True)
    sorted_country = sorted_df[sorted_df['country'] == country]
    filter_population = sorted_country[sorted_country['population'] >= population]
    return [(WC_DF["city"][i],WC_DF["population"][i]) for i in range(len(filter_population))]

big_cities_in_country("Japan",1000000)
63/25:
# Complete question 1d in this cell


def big_cities_in_country(country, population): # country is a string argument
    sorted_df = WC_DF.sort_values(by=["population"], ascending=True)
    sorted_country = sorted_df[sorted_df['country'] == country]
    f_population = sorted_country[sorted_country['population'] >= population]
    return [(f_population["city"][i],f_population["population"][i]) for i in range(len(filter_population))]

big_cities_in_country("Japan",1000000)
63/26:
# Complete question 1d in this cell


def big_cities_in_country(country, population): # country is a string argument
    sorted_df = WC_DF.sort_values(by=["population"], ascending=True)
    sorted_country = sorted_df[sorted_df['country'] == country]
    f_population = sorted_country[sorted_country['population'] >= population]
    return [(f_population["city"][i],f_population["population"][i]) for i in range(len(f_population))]

big_cities_in_country("Japan",1000000)
63/27:
# Complete question 1d in this cell


def big_cities_in_country(country, population): # country is a string argument
    sorted_df = WC_DF.sort_values(by=["population"], ascending=True)
    sorted_country = sorted_df[sorted_df['country'] == country]
    f_population = sorted_country[sorted_country['population'] >= population]
    return f_population

big_cities_in_country("Japan",1000000)
63/28:
# Complete question 1d in this cell


def big_cities_in_country(country, population): # country is a string argument
    sorted_df = WC_DF.sort_values(by=["population"], ascending=True)
    sorted_country = sorted_df[sorted_df['country'] == country]
    f_population = sorted_country[sorted_country['population'] >= population]
    return range(len(f_population))

big_cities_in_country("Japan",1000000)
63/29:
# Complete question 1d in this cell


def big_cities_in_country(country, population): # country is a string argument
    sorted_df = WC_DF.sort_values(by=["population"], ascending=True)
    sorted_country = sorted_df[sorted_df['country'] == country]
    f_population = sorted_country[sorted_country['population'] >= population]
    return range([f_population["city"][i] for i in range(len(f_population))])

big_cities_in_country("Japan",1000000)
63/30: WC_DF.sort_values(by=["country"], ascending=True)[10] # Sorts countries by alphabet
63/31: WC_DF.sort_values(by=["country"], ascending=True)[:10] # Sorts countries by alphabet
63/32: WC_DF.sort_values(by=["country"], ascending=True).iloc(5) # Sorts countries by alphabet
63/33: WC_DF.sort_values(by=["country"], ascending=True)[:10] # Sorts countries by alphabet
63/34:
# Complete question 1d in this cell


def big_cities_in_country(country, population): # country is a string argument
    sorted_df = WC_DF.sort_values(by=["population"], ascending=True)
    sorted_country = sorted_df[sorted_df['country'] == country]
    f_population = sorted_country[sorted_country['population'] >= population]
    return [(row["city"],row["population"]) for i,row in f_population.iterrows())]

big_cities_in_country("Japan",1000000)
63/35:
# Complete question 1d in this cell


def big_cities_in_country(country, population): # country is a string argument
    sorted_df = WC_DF.sort_values(by=["population"], ascending=True)
    sorted_country = sorted_df[sorted_df['country'] == country]
    f_population = sorted_country[sorted_country['population'] >= population]
    return [(row["city"],row["population"]) for i,row in f_population.iterrows()]

big_cities_in_country("Japan",1000000)
63/36:
## Question 1e Answer Code Cell

def country_total_cities_population(country):
    country_df = WC_DF[WC_DF['country'] == country]
    population = 0
    for p in country_df["population"]:
        if not p!=p:
            population += p
    return population

country_total_cities("Japan")
63/37:
## Question 1e Answer Code Cell

def country_total_cities_population(country):
    country_df = WC_DF[WC_DF['country'] == country]
    population = 0
    for p in country_df["population"]:
        if not p!=p:
            population += p
    return population

country_total_cities_population("Japan")
63/38:
## Question 1e Answer Code Cell

def country_total_cities_population(country):
    country_df = WC_DF[WC_DF['country'] == country]
    population = 0
    
    count = 0
    count_Not_None = 0
    for p in country_df["population"]:
        count += 1
        if not p!=p:
            count_Not_None += 1
            population += p
    print(count,count_Not_None)
    return population

country_total_cities_population("Japan")
63/39:
## Question 1e Answer Code Cell

def country_total_cities_population(country):
    country_df = WC_DF[WC_DF['country'] == country]
    population = 0
    
    count = 0
    count_Not_None = 0
    for p in country_df["population"]:
        count += 1
        if not p!=p:
            count_Not_None += 1
            population += p
    print(count,count_Not_None)
    return population

country_total_cities_population("Ethiopia")
63/40:
## Question 1e Answer Code Cell

def country_total_cities_population(country):
    country_df = WC_DF[WC_DF['country'] == country]
    population = 0
    
    count = 0
    count_Not_None = 0
    for p in country_df["population"]:
        count += 1
        if not p!=p:
            count_Not_None += 1
            population += p
        else:
            print(p)
    print(count,count_Not_None)
    return population

country_total_cities_population("Ethiopia")
63/41:
## Question 1e Answer Code Cell

def country_total_cities_population(country):
    country_df = WC_DF[WC_DF['country'] == country]
    population = 0
    
    print(country)
    
    count = 0
    count_Not_None = 0
    for p in country_df["population"]:
        count += 1
        if not p!=p:
            print(p)
            count_Not_None += 1
            population += p
    print(count,count_Not_None)
    return population

country_total_cities_population("Finland")
63/42:
## Question 1e Answer Code Cell

def country_total_cities_population(country):
    country_df = WC_DF[WC_DF['country'] == country]
    population = 0
    
    print(country)
    
    count = 0
    count_Not_None = 0
    for p in country_df["population"]:
        count += 1
        if not p!=p:
            print(p)
            count_Not_None += 1
            population += p
    print(count,count_Not_None)
    return int(population)

country_total_cities_population("Finland")
63/43:
# Question 1b answer cell

def num_cities_occurring_n_times(n):
    counts = dict()
    for city in WC_DF["city"]:
        if city in counts:
            counts[city] += 1
        else:
            counts[city] = 1
    return len(counts)
print(WC_DF["city"][4],WC_DF["city_ascii"][4])
63/44:
# Question 1b answer cell

def num_cities_occurring_n_times(n):
    counts = dict()
    for city in WC_DF["city"]:
        if city in counts:
            counts[city] += 1
        else:
            counts[city] = 1
    return len(counts)

counts = dict()
print(WC_DF["city"][4],WC_DF["city_ascii"][4])

if WC_DF["city"][4] in counts:
    counts[WC_DF["city"][4]] += 1
else:
    counts[WC_DF["city"][4]] = 1
    
if WC_DF["city_ascii"][4] in counts:
    counts[WC_DF["city_ascii"][4]] += 1
else:
    counts[WC_DF["city_ascii"][4]] = 1

print(counts)
63/45:
# Question 1b answer cell

def num_cities_occurring_n_times(n):
    counts = dict()
    for city in WC_DF["city"]:
        if city in counts:
            counts[city] += 1
        else:
            counts[city] = 1
    return len([i for i in counts if counts[i] == n]])
63/46:
# Question 1b answer cell

def num_cities_occurring_n_times(n):
    counts = dict()
    for city in WC_DF["city"]:
        if city in counts:
            counts[city] += 1
        else:
            counts[city] = 1
    return len([i for i in counts if counts[i] == n])
63/47:
# Question 1b answer cell

def num_cities_occurring_n_times(n):
    counts = dict()
    for city in WC_DF["city"]:
        if city in counts:
            counts[city] += 1
        else:
            counts[city] = 1
    return len([i for i in counts if counts[i] == n])

num_cities_occurring_n_times(3)
64/1:
def effective_magnitude( magnitude, depth, surface_distance ):
    energy = 10**magnitude  # convert logarithmic magnitude to a linear energy value
    if depth < 1:   # Crude fix for small or negative depths (can occur where land is above sea level)
        depth = 1
    ## Calculate distance to source by Pythagorus (ignoring curvature of surface)
    dist_to_source_squared =  depth**2 + surface_distance**2
    ## Apply inverse square distance multiplier to get energy density at distance from source
    ## (Ignores damping effects)
    attenuated_energy = energy/dist_to_source_squared
    attenuated_magnitude =  math.log10(attenuated_energy) ## Convert back to a log base 10 scale
    return attenuated_magnitude

# Some test cases.
# effective_magnitude(9,100,500)
# effective_magnitude(6,50, 100)
64/2:
def epicenter_magnitude( magnitude, depth ):
    return effective_magnitude( magnitude, depth, 0)
64/3:
## 2e Answer Code Cell
def endangered_cities(min_population, min_effective_magnitude):
    filter1_q = QUAKE_DF[QUAKE_DF["mag"] >= min_effective_magnitude]
    filter1_q["ep_mag"] = None
    for i,row in filter1_q.iterrows():
        filter1_q["ep_mag"][i] = epicenter_magnitude(row["mag"], row["depth"])
    filter2_q = filter1_q[filter1_q["ep_mag"] >= min_effective_magnitude]
    
    
    filter1_city = WC_DF[WC_DF["population"] >= min_population]
    sorted_city = filter1_city.sort_values(by=["country", "city_ascii"], ascending=[True, True])
    
    sorted_city["count"] = 0
    ed_c = []
    for i,row_c in sorted_city.iterrows():
        loc_c = (row_c["lat"], row_c["lng"])
        endanger = False
        for j,row_q in filter2_q.iterrows():
            loc_q = (row_q["latitude"],row_q["longitude"])
            dist = haversine_distance(loc_c, loc_q)
            ef_mag = effective_magnitude(row_q["mag"],row_q["depth"],dist)
            if ef_mag >= min_effective_magnitude:
                endanger = True
                break
        if endanger :
            ed_c.append((row_c["city_ascii"], row_c["country"], loc_c))
    
    return ed_c
        
# print("nah")
endangered_cities(2000, 0.5)
65/1:
## WC_DF Initialisation

import pandas  ## This is the module for creating and manipulating DataFrames

WC_DF = pandas.read_csv("worldcities.csv")
65/2:
top_5_cities = WC_DF["city"][:5]   ## selects the first 5 items of the "city" column.
top_5_cities
65/3:
top_5_cities.index = list("abcde")
top_5_cities
65/4: top_5_cities.index
65/5: WC_DF["city"][:5].values
65/6: WC_DF.columns
65/7: list(WC_DF.columns)
65/8: WC_DF.iloc[1]
65/9:
for i, row in WC_DF.iterrows():
    print(i, row['city_ascii'], row['lat'], row['lng'])
    if i >3: break
65/10: WC_DF.sort_values(by=["country"], ascending=True)[:10] # Sorts countries by alphabet
65/11:
filtered_DF = WC_DF[ WC_DF['capital'] == 'admin'] # This keeps only administrative capitals
filtered_DF.head()
65/12:
# Question 1a answer cell

def non_ascii_cities():
    return set([row['city'] for i, row in WC_DF.iterrows() if row['city']!= row['city_ascii']])
65/13:
# Question 1b answer cell

def num_cities_occurring_n_times(n):
    counts = dict()
    for city in WC_DF["city"]:
        if city in counts:
            counts[city] += 1
        else:
            counts[city] = 1
    return len([i for i in counts if counts[i] == n])
65/14:
def country_num_cities_dict():
    counts = dict()
    for country in WC_DF["country"]:
        if country in counts:
            counts[country] += 1
        else:
            counts[country] = 1
    return counts
65/15:
# Question 1d answer cell

def largest_cities_dataframe(n):
    return WC_DF.sort_values(by=["population"], ascending=False)[:n]
65/16:
# Complete question 1d in this cell


def big_cities_in_country(country, population): # country is a string argument
    sorted_df = WC_DF.sort_values(by=["population"], ascending=True)
    sorted_country = sorted_df[sorted_df['country'] == country]
    f_population = sorted_country[sorted_country['population'] >= population]
    return [(row["city_ascii"],row["population"]) for i,row in f_population.iterrows()]
65/17:
## Question 1e Answer Code Cell

def country_total_cities_population(country):
    country_df = WC_DF[WC_DF['country'] == country]
    population = 0
    for p in country_df["population"]:
        if not p!=p:
            population += p
    return int(population)
65/18:
# Q2a answer code cell

import pandas    ## This is the module for creating and manupulating DataFrames

# Here we have assigned the url of the quake datasource to the global variable 
# 'QUAKE_SOURCE' for your convenience.
QUAKE_SOURCE = ( "http://earthquake.usgs.gov/" +
                 "earthquakes/feed/v1.0/summary/all_day.csv" )


QUAKE_DF = pandas.read_csv(QUAKE_SOURCE)  ## Modify this line to import the data using Pandas
65/19:
## If QUAKE_DF is a DataFrame, show the first 5 rows
try:
    if type(QUAKE_DF) == pandas.DataFrame:
        display(QUAKE_DF.head())
    else:
        print("QUAKE_DF is not a DataFrame")
except:
    print("QUAKE_DF has not been assigned a value")
65/20:
def show_deep_quakes( depth ):
    # make deep_quakes DataFrame by selecting rows from QUAKE_DF
    deep_quakes = QUAKE_DF[ QUAKE_DF["depth"] >= depth ]  ## This is how you select rows by a condition
                                                          ## on one of the column values.
        
    print("Number of quakes of depth {} or deeper:".format(depth), 
           len(deep_quakes.index))     ## This finds the number of rows of the deep_quakes DataFrame
    
    display(deep_quakes.sort_values("depth", ascending=False))  ## Sort by descending depth value
65/21:
try:
    show_deep_quakes(100)
except:
    print("Probably QUAKE_DF not correctly set")
65/22:
try:
    QUAKE_DF["depth"].max()
except:
    print("Probably QUAKE_DF not correctly set")
65/23:
try:
    QUAKE_DF["depth"].min()
except:
    print("Probably QUAKE_DF not correctly set")
65/24:
# Complete question 2b answer cell

def powerful_quakes(mag):
    deep_quakes = QUAKE_DF[ QUAKE_DF["mag"] >= mag ]
    return deep_quakes
65/25:
# Question 2c answer cell

def most_powerful_n_quakes(n):
    n -= 1
    sorted_df = QUAKE_DF.sort_values(by=["mag"], ascending=False)
    sorted_list = list(sorted_df["mag"])
    while (sorted_list[n] == sorted_list[n+1]):
        n+=1
    return sorted_df[:(n+1)]

most_powerful_n_quakes(10)
65/26:
## Function to compute distance between locations (kilometres) 
# Returns the surface distance in meters, according to the Haversine formula,
# between two locations given as (latitude, longitude) coordinate pairs.

import math
def haversine_distance( loc1 , loc2 ): 
    '''finds the distance (m) between 2 locations, where locations are defined by
    longitudes and latitudes'''
    lat1, lon1 = loc1
    lat2, lon2 = loc2
    radius = 6371  # kilometers
    dlat = math.radians(lat2 - lat1)
    dlon = math.radians(lon2 - lon1)
    a = (math.sin(dlat / 2) * math.sin(dlat / 2) +
         math.cos(math.radians(lat1)) * math.cos(math.radians(lat2)) *
         math.sin(dlon / 2) * math.sin(dlon / 2))
    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))
    d = radius * c
    return d
65/27:
## 2d Answer Code Cell

def quake_distance_from_loc_dataframe(loc):
    alter_df = QUAKE_DF.copy()
    alter_df["distance_from_loc"] = None
    for i,row in alter_df.iterrows():
        alter_df["distance_from_loc"][i] = haversine_distance((row["latitude"],row["longitude"]),loc)
    return alter_df.sort_values(by=["distance_from_loc"], ascending=True)
65/28:
def effective_magnitude( magnitude, depth, surface_distance ):
    energy = 10**magnitude  # convert logarithmic magnitude to a linear energy value
    if depth < 1:   # Crude fix for small or negative depths (can occur where land is above sea level)
        depth = 1
    ## Calculate distance to source by Pythagorus (ignoring curvature of surface)
    dist_to_source_squared =  depth**2 + surface_distance**2
    ## Apply inverse square distance multiplier to get energy density at distance from source
    ## (Ignores damping effects)
    attenuated_energy = energy/dist_to_source_squared
    attenuated_magnitude =  math.log10(attenuated_energy) ## Convert back to a log base 10 scale
    return attenuated_magnitude

# Some test cases.
# effective_magnitude(9,100,500)
# effective_magnitude(6,50, 100)
65/29:
def epicenter_magnitude( magnitude, depth ):
    return effective_magnitude( magnitude, depth, 0)
65/30:
## 2e Answer Code Cell
def endangered_cities(min_population, min_effective_magnitude):
    filter1_q = QUAKE_DF[QUAKE_DF["mag"] >= min_effective_magnitude]
    filter1_q["ep_mag"] = None
    for i,row in filter1_q.iterrows():
        filter1_q["ep_mag"][i] = epicenter_magnitude(row["mag"], row["depth"])
    filter2_q = filter1_q[filter1_q["ep_mag"] >= min_effective_magnitude]
    
    
    filter1_city = WC_DF[WC_DF["population"] >= min_population]
    sorted_city = filter1_city.sort_values(by=["country", "city_ascii"], ascending=[True, True])
    
    sorted_city["count"] = 0
    ed_c = []
    for i,row_c in sorted_city.iterrows():
        loc_c = (row_c["lat"], row_c["lng"])
        endanger = False
        for j,row_q in filter2_q.iterrows():
            loc_q = (row_q["latitude"],row_q["longitude"])
            dist = haversine_distance(loc_c, loc_q)
            ef_mag = effective_magnitude(row_q["mag"],row_q["depth"],dist)
            if ef_mag >= min_effective_magnitude:
                endanger = True
                break
        if endanger :
            ed_c.append((row_c["city_ascii"], row_c["country"], loc_c))
    
    return ed_c
        
# print("nah")
endangered_cities(2000, 0.5)
65/31:
from ipyleaflet import Map, basemaps, basemap_to_tiles, Circle, Polyline
from ipywidgets import Layout

LEEDS_LOC  = ( 53.8008,  -1.5491  ) # Here we define the longitude and latitude of Leeds
WORLD_MAP = Map(basemap=basemaps.OpenTopoMap, center=LEEDS_LOC, zoom=1.5,
                layout=Layout(height="500px")) # Here we create a map object centred on Leeds

WORLD_MAP
65/32:
## 2e Answer Code Cell
def endangered_cities(min_population, min_effective_magnitude):
    filter1_q = QUAKE_DF[QUAKE_DF["mag"] >= min_effective_magnitude]
    filter1_q["ep_mag"] = None
    for i,row in filter1_q.iterrows():
        filter1_q["ep_mag"][i] = epicenter_magnitude(row["mag"], row["depth"])
    filter2_q = filter1_q[filter1_q["ep_mag"] >= min_effective_magnitude]
    
    
    filter1_city = WC_DF[WC_DF["population"] >= min_population]
    sorted_city = filter1_city.sort_values(by=["country", "city_ascii"], ascending=[True, True])
    
    sorted_city["count"] = 0
    ed_c = []
    for i,row_c in sorted_city.iterrows():
        loc_c = (row_c["lat"], row_c["lng"])
        endanger = False
        for j,row_q in filter2_q.iterrows():
            loc_q = (row_q["latitude"],row_q["longitude"])
            dist = haversine_distance(loc_c, loc_q)
            ef_mag = effective_magnitude(row_q["mag"],row_q["depth"],dist)
            if ef_mag >= min_effective_magnitude:
                endanger = True
                break
        if endanger :
            ed_c.append((row_c["city_ascii"], row_c["country"], loc_c))
    
    return ed_c
        
# print("nah")
endangered_cities(20000, 0.5)
65/33:
## 2e Answer Code Cell
def endangered_cities(min_population, min_effective_magnitude):
    filter1_q = QUAKE_DF[QUAKE_DF["mag"] >= min_effective_magnitude]
    filter1_q["ep_mag"] = None
    for i,row in filter1_q.iterrows():
        filter1_q["ep_mag"][i] = epicenter_magnitude(row["mag"], row["depth"])
    filter2_q = filter1_q[filter1_q["ep_mag"] >= min_effective_magnitude]
    
    
    filter1_city = WC_DF[WC_DF["population"] >= min_population]
    sorted_city = filter1_city.sort_values(by=["country", "city_ascii"], ascending=[True, True])
    
    sorted_city["count"] = 0
    ed_c = []
    for i,row_c in sorted_city.iterrows():
        loc_c = (row_c["lat"], row_c["lng"])
        endanger = False
        for j,row_q in filter2_q.iterrows():
            loc_q = (row_q["latitude"],row_q["longitude"])
            dist = haversine_distance(loc_c, loc_q)
            ef_mag = effective_magnitude(row_q["mag"],row_q["depth"],dist)
            if ef_mag >= min_effective_magnitude:
                endanger = True
                break
        if endanger :
            ed_c.append((row_c["city_ascii"], row_c["country"], loc_c))
    
    return ed_c
        
# print("nah")
endangered_cities(2000, 0.5)
68/1:
print('PyDev console: using IPython 7.28.0\n')

import sys; print('Python %s on %s' % (sys.version, sys.platform))
sys.path.extend(['/Users/winson/Desktop/glaciers'])
77/1:
## Code Cell
import pandas as pd
df = pd.read_csv("zomato.csv")
df.head()
78/1:
print('PyDev console: using IPython 7.28.0\n')

import sys; print('Python %s on %s' % (sys.version, sys.platform))
sys.path.extend(['/Users/winson/Desktop/DS_CW2'])
79/1:
## Code Cell
import pandas as pd
df = pd.read_csv("zomato.csv")
df.head()
df.columns
79/2:
print("Percentage null or na values in df")
((data.isnull() | data.isna()).sum() * 100 / data.index.size).round(2)
79/3:
print("Percentage null or na values in df")
((df.isnull() | df.isna()).sum() * 100 / df.index.size).round(2)
79/4: df.isnull().sum()
79/5:

df.head()
79/6: df[df["location"] != df["listed_in(city)"]]
79/7: df["location"].unique()
79/8: df["location"].unique(), df["listed_in(city"].unique()
79/9: df["location"].unique(), df["listed_in(city)"].unique()
79/10:
import pandas as pd
from geopy.geocoders import Nominatim
80/1:
import pandas as pd
from geopy.geocoders import Nominatim
80/2:
import pandas as pd
# from geopy.geocoders import Nominatim
which python
80/3:
import pandas as pd
from geopy.geocoders import Nominatim
80/4:
import pandas as pd
from geopy.geocoders import Nominatim
import sys
print(sys.path)
80/5:
import pandas as pd
# from geopy.geocoders import Nominatim
import sys
print(sys.path)
80/6:
import pandas as pd
# from geopy.geocoders import Nominatim
import os
os.environ['PYTHONPATH'].split(os.pathsep)
80/7:
import pandas as pd
# from geopy.geocoders import Nominatim
import geopy
81/1:
import pandas as pd
from geopy.geocoders import Nominatim
81/2:
## Code Cell
df = pd.read_csv("zomato.csv")
df.columns
81/3:
import pandas as pd
from geopy.geocoders import Nominatim


geolocator=Nominatim(user_agent="app")
81/4:
locations = []
index = 0
for location in df["address"]:
    if index>= 10:
        break
    location = geolocator.geocode(location)    
    locations.appen((location.latitude, location.longitude) )
    
print(locations)
81/5:
locations = []
index = 0
for location in df["address"]:
    if index>= 10:
        break
    location = geolocator.geocode(location)    
    locations.append((location.latitude, location.longitude) )
    
print(locations)
81/6:
locations = []
index = 0
for location in df["address"]:
    if index>= 10:
        break
    print(locations)
    location_g = geolocator.geocode(location)    
    locations.append((location_g.latitude, location_g.longitude) )
    
print(locations)
81/7:
locations = []
index = 0
for location in df["address"]:
    if index>= 10:
        break
    print(location)
    location_g = geolocator.geocode(location)    
    locations.append((location_g.latitude, location_g.longitude) )
    
print(locations)
81/8:
locations = []
index = 0

locator = Nominatim(user_agent=”myGeocoder”)
location = locator.geocode(“Champ de Mars, Paris, France”)

for location in df["address"]:
    if index>= 10:
        break
    print(location)
    location_g = geolocator.geocode(location)    
    locations.append((location_g.latitude, location_g.longitude) )
    
print(locations)
81/9:
locations = []
index = 0

locator = Nominatim(user_agent=myGeocoder")
location = locator.geocode(“Champ de Mars, Paris, France")

for location in df["address"]:
    if index>= 10:
        break
    print(location)
    location_g = geolocator.geocode(location)    
    locations.append((location_g.latitude, location_g.longitude) )
    
print(locations)
81/10:
locations = []
index = 0

locator = Nominatim(user_agent="myGeocoder")
location = locator.geocode("Champ de Mars, Paris, France")

for location in df["address"]:
    if index>= 10:
        break
    print(location)
    location_g = geolocator.geocode(location)    
    locations.append((location_g.latitude, location_g.longitude) )
    
print(locations)
81/11:
locations = []
index = 0

locator = Nominatim(user_agent="myGeocoder")
location = locator.geocode("Champ de Mars, Paris, France")
print(location)

for location in df["address"]:
    if index>= 10:
        break
    print(location)
    location_g = geolocator.geocode(location)    
    locations.append((location_g.latitude, location_g.longitude) )
    
print(locations)
81/12:
locations = []
index = 0

locator = Nominatim(user_agent="myGeocoder")
location = locator.geocode("Champ de Mars, Paris, France")
print(location.latitude, location.longitude)

for location in df["address"]:
    if index>= 10:
        break
    print(location)
    location_g = geolocator.geocode(location)    
    locations.append((location_g.latitude, location_g.longitude) )
    
print(locations)
81/13:
locations = []
index = 0

locator = Nominatim(user_agent="myGeocoder")

for location in df["address"]:
    if index>= 10:
        break
    print(location)
    location_g = locator.geocode(location)    
    locations.append((location_g.latitude, location_g.longitude) )
    
print(locations)
81/14:
locations = []
index = 0

locator = Nominatim(user_agent="myGeocoder")
location = locator.geocode("Champ de Mars, Paris, France")
print(location.latitude, location.longitude)

for location in df["address"]:
    if location is not None:
        if index>= 10:
            break
        else:
            index += 1
        print(location)
        location_g = geolocator.geocode(location)    
        locations.append((location_g.latitude, location_g.longitude) )
    
print(locations)
81/15:
locations = []
index = 0

locator = Nominatim(user_agent="myGeocoder")
location = locator.geocode("Champ de Mars, Paris, France")
print(location.latitude, location.longitude)

for location in df["location"]:
    if location is not None:
        if index>= 10:
            break
        else:
            index += 1
        print(location)
        location_g = geolocator.geocode(location)    
        locations.append((location_g.latitude, location_g.longitude) )
    
print(locations)
81/16:
locations = []
index = 0

locator = Nominatim(user_agent="myGeocoder")
location = locator.geocode(df["address"][0])
print(df["address"][0])
print(location.latitude, location.longitude)

for location in df["location"]:
    if location is not None:
        if index>= 10:
            break
        else:
            index += 1
        print(location)
        location_g = geolocator.geocode(location)    
        locations.append((location_g.latitude, location_g.longitude) )
    
print(locations)
81/17:
locations = []
index = 0

locator = Nominatim(user_agent="myGeocoder")
location = locator.geocode("21st Main Road, 2nd Stage, Banashankari, Bangalore")
print(df["address"][0])
print(location.latitude, location.longitude)

for location in df["location"]:
    if location is not None:
        if index>= 10:
            break
        else:
            index += 1
        print(location)
        location_g = geolocator.geocode(location)    
        locations.append((location_g.latitude, location_g.longitude) )
    
print(locations)
83/1:
import pandas as pd
from geopy.geocoders import Nominatim


geolocator=Nominatim(user_agent="app")
83/2:
## Code Cell
df = pd.read_csv("zomato.csv")
df.columns
83/3:

df["address"].unique()
83/4:

len(df["address"].unique())
83/5:

len(df["address"].unique())
df["a"] = 1
df
83/6:

len(df["address"].unique())
df
83/7:

len(df["rate"].unique())
83/8:

df["rate"].unique()
83/9:

df["rate"].split().unique()
83/10:

df["rate"].apply(split).unique()
83/11:

df["rate"] = 1
83/12: df
83/13: len(df["approx_cost(for two people)"].unique())
83/14: df["approx_cost(for two people)"].unique()
83/15: df["approx_cost(for two people)"].apply(int).unique()
83/16: df["approx_cost(for two people)"].unique()
83/17:
def strip_1(s):
    s.strip(",")
df["approx_cost(for two people)"].apply(strip_1).apply(int).unique()
83/18:
def strip_1(s):
    print(s)
    s.strip(",")
df["approx_cost(for two people)"].apply(strip_1).apply(int).unique()
83/19:

df["approx_cost(for two people)"].dtype(float).unique()
83/20:

df["approx_cost(for two people)"].type(float).unique()
83/21:

df["approx_cost(for two people)"].astype(float).unique()
83/22:
def strip_1(s):
    print(s)
    return s.strip(",")
df["approx_cost(for two people)"].apply(strip_1).apply(int).unique()
83/23:
def strip_1(s):
    if not s:
        return 0
    return s.strip(",")
df["approx_cost(for two people)"].apply(strip_1).apply(float).unique()
83/24:
def strip_1(s):
    print(s)
    if not s:
        return 0
    return s.strip(",")
df["approx_cost(for two people)"].apply(strip_1).apply(float).unique()
83/25:
def strip_1(s):
    print(type(s))
    if not s:
        return 0
    return s.strip(",")
df["approx_cost(for two people)"].apply(strip_1).apply(float).unique()
83/26:
def strip_1(s):
    print(type(s))
    if s.isna():
        return 0
    return s.strip(",")
df["approx_cost(for two people)"].apply(strip_1).apply(float).unique()
83/27:
def strip_1(s):
    print(type(s))
    if pd.isna(s):
        return 0
    return s.strip(",")
df["approx_cost(for two people)"].apply(strip_1).apply(float).unique()
83/28:
def strip_1(s):
    if pd.isna(s):
        return 0
    return s.strip(",")
df["approx_cost(for two people)"].apply(strip_1).apply(float).unique()
83/29:
def strip_1(s):
    if pd.isna(s):
        return 0
    return s.strip(",")
# df["approx_cost(for two people)"].apply(strip_1).apply(float).unique()
strip_1("1,200")
83/30:
def strip_1(s):
    if pd.isna(s):
        return 0
    return s.replace(",", "")
# df["approx_cost(for two people)"].apply(strip_1).apply(float).unique()
strip_1("1,200")
83/31:
def strip_1(s):
    if pd.isna(s):
        return 0
    return s.replace(",", "")
df["approx_cost(for two people)"].apply(strip_1).apply(float).unique()
# strip_1("1,200")
83/32:
## Code Cell
df = pd.read_csv("zomato.csv")
df.columns
83/33:

df.rate
83/34:

df[rate].replace("NEW", np.nan)
83/35:

df["rate"].replace("NEW", np.nan)
83/36:
import pandas as pd
from geopy.geocoders import Nominatim
import numpy as np


geolocator=Nominatim(user_agent="app")
83/37:

df["rate"].replace("NEW", np.nan)
83/38:

df["rate"].replace("NEW", np.nan)
zomato.isnull().sum()
83/39:

df["rate"].replace("NEW", np.nan)
df.isnull().sum()
83/40:

df["rate"].replace("NEW", np.nan)
df.isnull().sum()

df.head()
83/41:
def drop_address_data(df):
    new_df = df.drop(columns=["address"])
    return new_df


def drop_menu_related_data(df):
    new_df = df.drop(columns=['dish_liked', 'cuisines', 'menu_item'])
    return new_df


def drop_irrelevant_data(df):
    new_df = df.drop(columns=['url', 'phone', 'reviews_list'])
    return new_df


def drop_nan(df):
    df["rate"].replace("NEW", np.nan)
    df.dropna(how='any',inplace=True)
    return df
83/42:
def transform_rate(df):
    rates = []
    for _, row in df.iterrows():
        rate_in_float = float(row["rate"].split("/")[0])
        rates.append(rate_in_float)
    df["rate"] = rates
    return df


def transform_price(df):
    prices = []
    for _, row in df.iterrows():
        price_s = row["price"]
        price = int(price_s.replace(",", ""))
        prices.append(price)
    df["price"] = prices
    return df




def transform_address(df):
    latitudes = []
    longitudes = []
    for _, row in df.iterrows():
        latitude, longitude = get_geo_info(row)
        latitudes.append(latitude)
        longitudes.append(longitude)
    df["lat"] = latitudes
    df["lon"] = longitudes
    return df


def get_geo_info(row):
    address = row["address"]
    location = geolocator.geocode(address)
    next_index = address.find(",")

    while not location and next_index != -1:
        address = address[(next_index+1):]
        location = geolocator.geocode(address)
        next_index = address.find(",")

    if not location:
        location = geolocator.geocode(row["location"])
    return location.latitude, location.longitude


def rename_features(df):
    df.rename(column={'approx_cost(for two people)': 'price', 'listed_in(city)': 'city_area',
                      'listed_in(type)': 'meal_type'})
    return df
83/43:
## Code Cell
df = pd.read_csv("zomato.csv")
df.columns
83/44:
df.drop(columns=["address"])
df.drop(columns=['dish_liked', 'cuisines', 'menu_item'])
df.drop(columns=['url', 'phone', 'reviews_list'])

df["rate"].replace("NEW", np.nan)
df.dropna(how='any',inplace=True)
83/45:
df.drop(columns=["address"])
df.drop(columns=['dish_liked', 'cuisines', 'menu_item'])
df.drop(columns=['url', 'phone', 'reviews_list'])

df["rate"].replace("NEW", np.nan)
df.dropna(how='any',inplace=True)

df
83/46:
#transform_rate(df):
rates = []
for _, row in df.iterrows():
    rate_in_float = float(row["rate"].split("/")[0])
    rates.append(rate_in_float)
df["rate"] = rates


#transform_price(df):
prices = []
for _, row in df.iterrows():
    price_s = row["price"]
    price = int(price_s.replace(",", ""))
    prices.append(price)
df["price"] = prices



def get_geo_info(row):
    address = row["address"]
    location = geolocator.geocode(address)
    next_index = address.find(",")

    while not location and next_index != -1:
        address = address[(next_index+1):]
        location = geolocator.geocode(address)
        next_index = address.find(",")

    if not location:
        location = geolocator.geocode(row["location"])
    return location.latitude, location.longitude


#transform_address(df):
latitudes = []
longitudes = []
for _, row in df.iterrows():
    latitude, longitude = get_geo_info(row)
    latitudes.append(latitude)
    longitudes.append(longitude)
df["lat"] = latitudes
df["lon"] = longitudes




#rename_features(df):
df.rename(column={'approx_cost(for two people)': 'price', 'listed_in(city)': 'city_area','listed_in(type)': 'meal_type'})
83/47:
df.drop(columns=["address"])
df.drop(columns=['dish_liked', 'cuisines', 'menu_item'])
df.drop(columns=['url', 'phone', 'reviews_list'])

df = df["rate"].replace("NEW", np.nan)
df.dropna(how='any',inplace=True)

df
83/48:
df.drop(columns=["address"])
df.drop(columns=['dish_liked', 'cuisines', 'menu_item'])
df.drop(columns=['url', 'phone', 'reviews_list'])

df = df["rate"].replace("NEW", np.nan)
df.dropna(how='any',inplace=True)

df.unique()
83/49:
## Code Cell
df = pd.read_csv("zomato.csv")
df.columns
83/50:
df.drop(columns=["address"])
df.drop(columns=['dish_liked', 'cuisines', 'menu_item'])
df.drop(columns=['url', 'phone', 'reviews_list'])

df = df["rate"].replace("NEW", np.nan)
df.dropna(how='any',inplace=True)

df.unique()
83/51:
df.drop(columns=["address"])
df.drop(columns=['dish_liked', 'cuisines', 'menu_item'])
df.drop(columns=['url', 'phone', 'reviews_list'])

df["rate"] = df["rate"].replace("NEW", np.nan)
df["rate"] = df["rate"].replace("-", np.nan)
df.dropna(how='any',inplace=True)

len(df.unique())
83/52:
df.drop(columns=["address"])
df.drop(columns=['dish_liked', 'cuisines', 'menu_item'])
df.drop(columns=['url', 'phone', 'reviews_list'])

df["rate"] = df["rate"].replace("NEW", np.nan)
df["rate"] = df["rate"].replace("-", np.nan)
df.dropna(how='any',inplace=True)

df.unique().isna()
83/53:
## Code Cell
df = pd.read_csv("zomato.csv")
df.columns
83/54:
df.drop(columns=["address"])
df.drop(columns=['dish_liked', 'cuisines', 'menu_item'])
df.drop(columns=['url', 'phone', 'reviews_list'])

df["rate"] = df["rate"].replace("NEW", np.nan)
df["rate"] = df["rate"].replace("-", np.nan)
df.dropna(how='any',inplace=True)

df.unique().isna()
83/55:
## Code Cell
df = pd.read_csv("zomato.csv")
df.columns
83/56:
df.drop(columns=["address"])
df.drop(columns=['dish_liked', 'cuisines', 'menu_item'])
df.drop(columns=['url', 'phone', 'reviews_list'])

df["rate"] = df["rate"].replace("NEW", np.nan)
df["rate"] = df["rate"].replace("-", np.nan)
df.dropna(how='any',inplace=True)

df.isna()
83/57:
df.drop(columns=["address"])
df.drop(columns=['dish_liked', 'cuisines', 'menu_item'])
df.drop(columns=['url', 'phone', 'reviews_list'])

df["rate"] = df["rate"].replace("NEW", np.nan)
df["rate"] = df["rate"].replace("-", np.nan)
df.dropna(how='any',inplace=True)

df.isna().sum()
83/58:
df.drop(columns=["address"])
df.drop(columns=['dish_liked', 'cuisines', 'menu_item'])
df.drop(columns=['url', 'phone', 'reviews_list'])

df["rate"] = df["rate"].replace("NEW", np.nan)
df["rate"] = df["rate"].replace("-", np.nan)
df.dropna(how='any',inplace=True)

df.info()
83/59: df.duplicated().sum()
83/60: df.address.duplicated().sum()
83/61: df.shape
83/62: df.address.unique()
83/63: df.info()
83/64:
#transform_rate(df):
rates = []
for _, row in df.iterrows():
    rate_in_float = float(row["rate"].split("/")[0])
    rates.append(rate_in_float)
df["rate"] = rates


#transform_price(df):
prices = []
for _, row in df.iterrows():
    price_s = row["price"]
    price = int(price_s.replace(",", ""))
    prices.append(price)
df["price"] = prices



def get_geo_info(row):
    address = row["address"]
    location = geolocator.geocode(address)
    next_index = address.find(",")

    while not location and next_index != -1:
        address = address[(next_index+1):]
        location = geolocator.geocode(address)
        next_index = address.find(",")

    if not location:
        location = geolocator.geocode(row["location"])
    return location.latitude, location.longitude


#transform_address(df):
latitudes = []
longitudes = []
for _, row in df.iterrows():
    latitude, longitude = get_geo_info(row)
    latitudes.append(latitude)
    longitudes.append(longitude)
df["lat"] = latitudes
df["lon"] = longitudes




#rename_features(df):
df.rename(column={'approx_cost(for two people)': 'price', 'listed_in(city)': 'city_area','listed_in(type)': 'meal_type'})
83/65:
#rename_features(df):
df.rename(column={'approx_cost(for two people)': 'price', 'listed_in(city)': 'city_area',
                  'listed_in(type)': 'meal_type'})
83/66:
#rename_features(df):
df.rename(columns={'approx_cost(for two people)': 'price', 'listed_in(city)': 'city_area',
                  'listed_in(type)': 'meal_type'})
83/67:
df.drop(columns=["address"], inplace=True)
df.drop(columns=['dish_liked', 'cuisines', 'menu_item'], inplace=True)
df.drop(columns=['url', 'phone', 'reviews_list'], inplace=True)

df["rate"] = df["rate"].replace("NEW", np.nan)
df["rate"] = df["rate"].replace("-", np.nan)
df.dropna(how='any',inplace=True)

df.info()
83/68:
#rename_features(df):
df.rename(columns={'approx_cost(for two people)': 'price', 'listed_in(city)': 'city_area',
                  'listed_in(type)': 'meal_type'})
83/69:
#transform_rate(df):
rates = []
for _, row in df.iterrows():
    rate_in_float = float(row["rate"].split("/")[0])
    rates.append(rate_in_float)
df["rate"] = rates


#transform_price(df):
prices = []
for _, row in df.iterrows():
    price_s = row["price"]
    price = int(price_s.replace(",", ""))
    prices.append(price)
df["price"] = prices



def get_geo_info(row):
    address = row["address"]
    location = geolocator.geocode(address)
    next_index = address.find(",")

    while not location and next_index != -1:
        address = address[(next_index+1):]
        location = geolocator.geocode(address)
        next_index = address.find(",")

    if not location:
        location = geolocator.geocode(row["location"])
    return location.latitude, location.longitude


#transform_address(df):
latitudes = []
longitudes = []
for _, row in df.iterrows():
    latitude, longitude = get_geo_info(row)
    latitudes.append(latitude)
    longitudes.append(longitude)
df["lat"] = latitudes
df["lon"] = longitudes
83/70:
#transform_rate(df):
rates = []
for _, row in df.iterrows():
    rate_in_float = float(row["rate"].split("/")[0])
    rates.append(rate_in_float)
df["rate"] = rates


#transform_price(df):
prices = []
for _, row in df.iterrows():
    price_s = row["price"]
    if type() == str:
        price_s = price_s.replace(",", "")
    price = int()
    prices.append(price)
df["price"] = prices



def get_geo_info(row):
    address = row["address"]
    location = geolocator.geocode(address)
    next_index = address.find(",")

    while not location and next_index != -1:
        address = address[(next_index+1):]
        location = geolocator.geocode(address)
        next_index = address.find(",")

    if not location:
        location = geolocator.geocode(row["location"])
    return location.latitude, location.longitude


#transform_address(df):
latitudes = []
longitudes = []
for _, row in df.iterrows():
    latitude, longitude = get_geo_info(row)
    latitudes.append(latitude)
    longitudes.append(longitude)
df["lat"] = latitudes
df["lon"] = longitudes
83/71:
## Code Cell
df = pd.read_csv("zomato.csv")
df.columns
83/72:
df.drop(columns=["address"], inplace=True)
df.drop(columns=['dish_liked', 'cuisines', 'menu_item'], inplace=True)
df.drop(columns=['url', 'phone', 'reviews_list'], inplace=True)

df["rate"] = df["rate"].replace("NEW", np.nan)
df["rate"] = df["rate"].replace("-", np.nan)
df.dropna(how='any',inplace=True)

df.info()
83/73:
#rename_features(df):
df.rename(columns={'approx_cost(for two people)': 'price', 'listed_in(city)': 'city_area',
                  'listed_in(type)': 'meal_type'})
83/74:
#transform_rate(df):
rates = []
for _, row in df.iterrows():
    rate_in_float = float(row["rate"].split("/")[0])
    rates.append(rate_in_float)
df["rate"] = rates


#transform_price(df):
prices = []
for _, row in df.iterrows():
    price_s = row["price"]
    if type() == str:
        price_s = price_s.replace(",", "")
    price = int()
    prices.append(price)
df["price"] = prices



def get_geo_info(row):
    address = row["address"]
    location = geolocator.geocode(address)
    next_index = address.find(",")

    while not location and next_index != -1:
        address = address[(next_index+1):]
        location = geolocator.geocode(address)
        next_index = address.find(",")

    if not location:
        location = geolocator.geocode(row["location"])
    return location.latitude, location.longitude


#transform_address(df):
latitudes = []
longitudes = []
for _, row in df.iterrows():
    latitude, longitude = get_geo_info(row)
    latitudes.append(latitude)
    longitudes.append(longitude)
df["lat"] = latitudes
df["lon"] = longitudes
83/75:
#rename_features(df):
df.rename(columns={'approx_cost(for two people)': 'price', 'listed_in(city)': 'city_area',
                  'listed_in(type)': 'meal_type'}, inplace=True)
83/76:
#rename_features(df):
df.rename(columns={'approx_cost(for two people)': 'price', 'listed_in(city)': 'city_area',
                  'listed_in(type)': 'meal_type'}, inplace=True)
df.columns
83/77:
#transform_rate(df):
rates = []
for _, row in df.iterrows():
    rate_in_float = float(row["rate"].split("/")[0])
    rates.append(rate_in_float)
df["rate"] = rates


#transform_price(df):
prices = []
for _, row in df.iterrows():
    price_s = row["price"]
    if type() == str:
        price_s = price_s.replace(",", "")
    price = int()
    prices.append(price)
df["price"] = prices
83/78:
## Code Cell
df = pd.read_csv("zomato.csv")
df.columns
83/79:
## Code Cell
original_df = pd.read_csv("zomato.csv")
original_df.columns
83/80:
df = original_df.copy()

df.drop(columns=["address"], inplace=True)
df.drop(columns=['dish_liked', 'cuisines', 'menu_item'], inplace=True)
df.drop(columns=['url', 'phone', 'reviews_list'], inplace=True)

df["rate"] = df["rate"].replace("NEW", np.nan)
df["rate"] = df["rate"].replace("-", np.nan)
df.dropna(how='any',inplace=True)

df.info()
83/81:
#rename_features(df):
df.rename(columns={'approx_cost(for two people)': 'price', 'listed_in(city)': 'city_area',
                  'listed_in(type)': 'meal_type'}, inplace=True)
df.columns
83/82:
#transform_rate(df):
rates = []
for _, row in df.iterrows():
    rate_in_float = float(row["rate"].split("/")[0])
    rates.append(rate_in_float)
df["rate"] = rates


#transform_price(df):
prices = []
for _, row in df.iterrows():
    price_s = row["price"]
    if type() == str:
        price_s = price_s.replace(",", "")
    price = int()
    prices.append(price)
df["price"] = prices
83/83:
#transform_rate(df):
rates = []
for _, row in df.iterrows():
    rate_in_float = float(row["rate"].split("/")[0])
    rates.append(rate_in_float)
df["rate"] = rates


#transform_price(df):
prices = []
for _, row in df.iterrows():
    price_s = row["price"]
    if type() == str:
        price_s = price_s.replace(",", "")
    price = int(price)
    prices.append(price)
df["price"] = prices
83/84:
df = original_df.copy()

df.drop(columns=["address"], inplace=True)
df.drop(columns=['dish_liked', 'cuisines', 'menu_item'], inplace=True)
df.drop(columns=['url', 'phone', 'reviews_list'], inplace=True)

df["rate"] = df["rate"].replace("NEW", np.nan)
df["rate"] = df["rate"].replace("-", np.nan)
df.dropna(how='any',inplace=True)

df.info()
83/85:
#rename_features(df):
df.rename(columns={'approx_cost(for two people)': 'price', 'listed_in(city)': 'city_area',
                  'listed_in(type)': 'meal_type'}, inplace=True)
df.columns
83/86:
#transform_rate(df):
rates = []
for _, row in df.iterrows():
    rate_in_float = float(row["rate"].split("/")[0])
    rates.append(rate_in_float)
df["rate"] = rates


#transform_price(df):
prices = []
for _, row in df.iterrows():
    price_s = row["price"]
    if type() == str:
        price_s = price_s.replace(",", "")
    price = int(price)
    prices.append(price)
df["price"] = prices
83/87:
df = original_df.copy()

df.drop(columns=["address"], inplace=True)
df.drop(columns=['dish_liked', 'cuisines', 'menu_item'], inplace=True)
df.drop(columns=['url', 'phone', 'reviews_list'], inplace=True)

df["rate"] = df["rate"].replace("NEW", np.nan)
df["rate"] = df["rate"].replace("-", np.nan)
df.dropna(how='any',inplace=True)

df.info()
83/88:
#rename_features(df):
df.rename(columns={'approx_cost(for two people)': 'price', 'listed_in(city)': 'city_area',
                  'listed_in(type)': 'meal_type'}, inplace=True)
df.columns
83/89:
#transform_rate(df):
rates = []
for _, row in df.iterrows():
    rate_in_float = float(row["rate"].split("/")[0])
    rates.append(rate_in_float)
df["rate"] = rates


#transform_price(df):
prices = []
for _, row in df.iterrows():
    price_s = row["price"]
    price_s = price_s.replace(",", "")
    price = int(price_s)
    prices.append(price)
df["price"] = prices
83/90: df.price.unique()
83/91: df.rate.unique()
83/92:
def get_geo_info(row):
    address = row["address"]
    location = geolocator.geocode(address)
    next_index = address.find(",")

    while not location and next_index != -1:
        address = address[(next_index+1):]
        location = geolocator.geocode(address)
        next_index = address.find(",")

    if not location:
        location = geolocator.geocode(row["location"])
    return location.latitude, location.longitude


#transform_address(df):
latitudes = []
longitudes = []
for _, row in df.iterrows():
    latitude, longitude = get_geo_info(row)
    latitudes.append(latitude)
    longitudes.append(longitude)
df["lat"] = latitudes
df["lon"] = longitudes
83/93:
df = original_df.copy()

df.drop(columns=['dish_liked', 'cuisines', 'menu_item'], inplace=True)
df.drop(columns=['name', 'url', 'phone', 'reviews_list'], inplace=True)

df["rate"] = df["rate"].replace("NEW", np.nan)
df["rate"] = df["rate"].replace("-", np.nan)
df.dropna(how='any',inplace=True)

df.info()
83/94:
#rename_features(df):
df.rename(columns={'approx_cost(for two people)': 'price', 'listed_in(city)': 'city_area',
                  'listed_in(type)': 'meal_type'}, inplace=True)
df.columns
83/95:
#transform_rate(df):
rates = []
for _, row in df.iterrows():
    rate_in_float = float(row["rate"].split("/")[0])
    rates.append(rate_in_float)
df["rate"] = rates


#transform_price(df):
prices = []
for _, row in df.iterrows():
    price_s = row["price"]
    price_s = price_s.replace(",", "")
    price = int(price_s)
    prices.append(price)
df["price"] = prices
83/96: df.duplicated().sum()
83/97:
df = original_df.copy()

df.drop(columns=['dish_liked', 'cuisines', 'menu_item'], inplace=True)
df.drop(columns=['name', 'url', 'phone', 'reviews_list'], inplace=True)

df["rate"] = df["rate"].replace("NEW", np.nan)
df["rate"] = df["rate"].replace("-", np.nan)
df.dropna(how='any',inplace=True)

df.drop_duplicates(inplace=True)
df.info()
83/98:
#transform_rate(df):
rates = []
for _, row in df.iterrows():
    rate_in_float = float(row["rate"].split("/")[0])
    rates.append(rate_in_float)
df["rate"] = rates


#transform_price(df):
prices = []
for _, row in df.iterrows():
    price_s = row["price"]
    price_s = price_s.replace(",", "")
    price = int(price_s)
    prices.append(price)
df["price"] = prices
83/99:
#transform_rate(df):
rates = []
for _, row in df.iterrows():
    rate_in_float = float(row["rate"].split("/")[0])
    rates.append(rate_in_float)
df["rate"] = rates


#transform_price(df):
prices = []
for _, row in df.iterrows():
    print(row)
    price_s = row["price"]
    price_s = price_s.replace(",", "")
    price = int(price_s)
    prices.append(price)
df["price"] = prices
83/100:
df = original_df.copy()

df.drop(columns=['dish_liked', 'cuisines', 'menu_item'], inplace=True)
df.drop(columns=['name', 'url', 'phone', 'reviews_list'], inplace=True)

df["rate"] = df["rate"].replace("NEW", np.nan)
df["rate"] = df["rate"].replace("-", np.nan)
df.dropna(how='any',inplace=True)

df.drop_duplicates(inplace=True)
df.info()
83/101:
df = original_df.copy()

df.drop(columns=['dish_liked', 'cuisines', 'menu_item'], inplace=True)
df.drop(columns=['name', 'url', 'phone', 'reviews_list'], inplace=True)

df["rate"] = df["rate"].replace("NEW", np.nan)
df["rate"] = df["rate"].replace("-", np.nan)
df.dropna(how='any',inplace=True)

#rename_features(df):
df.rename(columns={'approx_cost(for two people)': 'price', 'listed_in(city)': 'city_area',
                  'listed_in(type)': 'meal_type'}, inplace=True)

df.drop_duplicates(inplace=True)
df.info()
83/102:
#transform_rate(df):
rates = []
for _, row in df.iterrows():
    rate_in_float = float(row["rate"].split("/")[0])
    rates.append(rate_in_float)
df["rate"] = rates


#transform_price(df):
prices = []
for _, row in df.iterrows():
    print(row)
    price_s = row["price"]
    price_s = price_s.replace(",", "")
    price = int(price_s)
    prices.append(price)
df["price"] = prices
83/103:
#transform_rate(df):
rates = []
for _, row in df.iterrows():
    rate_in_float = float(row["rate"].split("/")[0])
    rates.append(rate_in_float)
df["rate"] = rates


#transform_price(df):
prices = []
for _, row in df.iterrows():
    price_s = row["price"]
    price_s = price_s.replace(",", "")
    price = int(price_s)
    prices.append(price)
df["price"] = prices
83/104:
df = original_df.copy()

df.drop(columns=['dish_liked', 'cuisines', 'menu_item'], inplace=True)
df.drop(columns=['name', 'url', 'phone', 'reviews_list'], inplace=True)

df["rate"] = df["rate"].replace("NEW", np.nan)
df["rate"] = df["rate"].replace("-", np.nan)
df.dropna(how='any',inplace=True)

#rename_features(df):
df.rename(columns={'approx_cost(for two people)': 'price', 'listed_in(city)': 'city_area',
                  'listed_in(type)': 'meal_type'}, inplace=True)

df.drop_duplicates(inplace=True)
df.info()
83/105:
#transform_rate(df):
rates = []
for _, row in df.iterrows():
    rate_in_float = float(row["rate"].split("/")[0])
    rates.append(rate_in_float)
df["rate"] = rates


#transform_price(df):
prices = []
for _, row in df.iterrows():
    price_s = row["price"]
    price_s = price_s.replace(",", "")
    price = int(price_s)
    prices.append(price)
df["price"] = prices
83/106:
copy_df = df
df = copy_df.copy()

def get_geo_info(row):
    address = row["address"]
    location = geolocator.geocode(address)
    next_index = address.find(",")

    while not location and next_index != -1:
        address = address[(next_index+1):]
        location = geolocator.geocode(address)
        next_index = address.find(",")

    if not location:
        location = geolocator.geocode(row["location"])
    return location.latitude, location.longitude


#transform_address(df):
latitudes = []
longitudes = []
for _, row in df.iterrows():
    row
    latitude, longitude = get_geo_info(row)
    latitudes.append(latitude)
    longitudes.append(longitude)
df["lat"] = latitudes
df["lon"] = longitudes
83/107:
copy_df = df
df = copy_df.copy()
print("on it")

def get_geo_info(row):
    address = row["address"]
    location = geolocator.geocode(address)
    next_index = address.find(",")

    while not location and next_index != -1:
        address = address[(next_index+1):]
        location = geolocator.geocode(address)
        next_index = address.find(",")

    if not location:
        location = geolocator.geocode(row["location"])
    return location.latitude, location.longitude


#transform_address(df):
latitudes = []
longitudes = []
for _, row in df.iterrows():
    row
    latitude, longitude = get_geo_info(row)
    latitudes.append(latitude)
    longitudes.append(longitude)
df["lat"] = latitudes
df["lon"] = longitudes

df
83/108:
copy_df = df
df = copy_df.copy()
print("on it")

def get_geo_info(row):
    address = row["address"]
    location = geolocator.geocode(address)
    next_index = address.find(",")

    while not location and next_index != -1:
        address = address[(next_index+1):]
        location = geolocator.geocode(address)
        next_index = address.find(",")

    if not location:
        location = geolocator.geocode(row["location"])
    return location.latitude, location.longitude


#transform_address(df):
latitudes = []
longitudes = []
for _, row in df.iterrows():
    row
    latitude, longitude = get_geo_info(row)
    latitudes.append(latitude)
    longitudes.append(longitude)
df["lat"] = latitudes
df["lon"] = longitudes
print("finished")

df
83/109:
copy_df = df
df = copy_df.copy()
print("on it")

def get_geo_info(row):
    address = row["address"]
    location = geolocator.geocode(address)
    next_index = address.find(",")

    while not location and next_index != -1:
        address = address[(next_index+1):]
        location = geolocator.geocode(address)
        next_index = address.find(",")

    if not location:
        location = geolocator.geocode(row["location"])
    return location.latitude, location.longitude


#transform_address(df):
latitudes = []
longitudes = []
index =0
for _, row in df.iterrows():
    index += 1
    if index % 100 == 0:
        print(index)
    latitude, longitude = get_geo_info(row)
    latitudes.append(latitude)
    longitudes.append(longitude)
df["lat"] = latitudes
df["lon"] = longitudes
print("finished")

df
83/110:
copy_df = df
df = copy_df.copy()
print("on it")

def get_geo_info(row):
    address = row["address"]
    location = geolocator.geocode(address)
    next_index = address.find(",")

    while not location and next_index != -1:
        address = address[(next_index+1):]
        location = geolocator.geocode(address)
        next_index = address.find(",")

    if not location:
        location = geolocator.geocode(row["location"])
    return location.latitude, location.longitude


#transform_address(df):
latitudes = []
longitudes = []
index =0
for _, row in df.iterrows():
    index += 1
    print(index)
    latitude, longitude = get_geo_info(row)
    latitudes.append(latitude)
    longitudes.append(longitude)
df["lat"] = latitudes
df["lon"] = longitudes
print("finished")

df
83/111:
copy_df = df
df = copy_df.copy()
print("on it")

def get_geo_info_address(address):
    address = row["address"]
    location = geolocator.geocode(address)
    next_index = address.find(",")

    while not location and next_index != -1:
        address = address[(next_index+1):]
        location = geolocator.geocode(address)
        next_index = address.find(",")

    if not location:
        location = geolocator.geocode(row["location"])
    return location.latitude, location.longitude 


def get_geo_info_location(location):
    geo_info = geolocator.geocode(location)
    return geo_info.latitude, geo_info.longitude


#transform_address(df):
all_location = df["location"].unique()

print("finished")

df
83/112:
copy_df = df
df = copy_df.copy()
print("on it")

def get_geo_info_address(address):
    address = row["address"]
    location = geolocator.geocode(address)
    next_index = address.find(",")

    while not location and next_index != -1:
        address = address[(next_index+1):]
        location = geolocator.geocode(address)
        next_index = address.find(",")

    if not location:
        location = geolocator.geocode(row["location"])
    return location.latitude, location.longitude 


def get_geo_info_location(location):
    geo_info = geolocator.geocode(location)
    return geo_info.latitude, geo_info.longitude


#transform_address(df):
all_location = df["location"].unique()
83/113:
copy_df = df
df = copy_df.copy()
print("on it")

def get_geo_info_address(address):
    address = row["address"]
    location = geolocator.geocode(address)
    next_index = address.find(",")

    while not location and next_index != -1:
        address = address[(next_index+1):]
        location = geolocator.geocode(address)
        next_index = address.find(",")

    if not location:
        location = geolocator.geocode(row["location"])
    return location.latitude, location.longitude 


def get_geo_info_location(location):
    geo_info = geolocator.geocode(location)
    return geo_info.latitude, geo_info.longitude


#transform_address(df):
all_location = df["location"].unique()

all_location
83/114:
def get_geo_info_address(address):
    address = row["address"]
    location = geolocator.geocode(address)
    next_index = address.find(",")

    while not location and next_index != -1:
        address = address[(next_index+1):]
        location = geolocator.geocode(address)
        next_index = address.find(",")

    if not location:
        location = geolocator.geocode(row["location"])
    return location.latitude, location.longitude 


def get_geo_info_location(location):
    geo_info = geolocator.geocode(location)
    return geo_info.latitude, geo_info.longitude


#transform_address(df):
all_location = df["location"].unique()

lat_lon_location = dict{}
for location in all_location:
    lat_lon_location[location] = get_geo_info_location(location)
    
lat_lon_location
83/115:
def get_geo_info_address(address):
    address = row["address"]
    location = geolocator.geocode(address)
    next_index = address.find(",")

    while not location and next_index != -1:
        address = address[(next_index+1):]
        location = geolocator.geocode(address)
        next_index = address.find(",")

    if not location:
        location = geolocator.geocode(row["location"])
    return location.latitude, location.longitude 


def get_geo_info_location(location):
    geo_info = geolocator.geocode(location)
    return geo_info.latitude, geo_info.longitude


#transform_address(df):
all_location = df["location"].unique()

lat_lon_location = dict()
for location in all_location:
    lat_lon_location[location] = get_geo_info_location(location)
    
lat_lon_location
83/116:
def get_geo_info_address(address):
    address = row["address"]
    location = geolocator.geocode(address)
    next_index = address.find(",")

    while not location and next_index != -1:
        address = address[(next_index+1):]
        location = geolocator.geocode(address)
        next_index = address.find(",")

    if not location:
        location = geolocator.geocode(row["location"])
    return location.latitude, location.longitude 


def get_geo_info_location(location):
    geo_info = geolocator.geocode(location)
    return geo_info.latitude, geo_info.longitude


#transform_address(df):
all_location = df["location"].unique()
all_location
# lat_lon_location = dict()
# for location in all_location:
#     lat_lon_location[location] = get_geo_info_location(location)
    
# lat_lon_location
83/117:
def get_geo_info_address(address):
    address = row["address"]
    location = geolocator.geocode(address)
    next_index = address.find(",")

    while not location and next_index != -1:
        address = address[(next_index+1):]
        location = geolocator.geocode(address)
        next_index = address.find(",")

    if not location:
        location = geolocator.geocode(row["location"])
    return location.latitude, location.longitude 


def get_geo_info_location(location):
    geo_info = geolocator.geocode(location)
    if geo_info
        return geo_info.latitude, geo_info.longitude
    return np.nan, np.nan


#transform_address(df):
all_location = df["location"].unique()
lat_lon_location = dict()
for location in all_location:
    lat_lon_location[location] = get_geo_info_location(location)
    
lat_lon_location
83/118:
def get_geo_info_address(address):
    address = row["address"]
    location = geolocator.geocode(address)
    next_index = address.find(",")

    while not location and next_index != -1:
        address = address[(next_index+1):]
        location = geolocator.geocode(address)
        next_index = address.find(",")

    if not location:
        location = geolocator.geocode(row["location"])
    return location.latitude, location.longitude 


def get_geo_info_location(location):
    geo_info = geolocator.geocode(location)
    if geo_info：
        return geo_info.latitude, geo_info.longitude
    return np.nan, np.nan


#transform_address(df):
all_location = df["location"].unique()
lat_lon_location = dict()
for location in all_location:
    lat_lon_location[location] = get_geo_info_location(location)
    
lat_lon_location
83/119:
def get_geo_info_address(address):
    address = row["address"]
    location = geolocator.geocode(address)
    next_index = address.find(",")

    while not location and next_index != -1:
        address = address[(next_index+1):]
        location = geolocator.geocode(address)
        next_index = address.find(",")

    if not location:
        location = geolocator.geocode(row["location"])
    return location.latitude, location.longitude 


def get_geo_info_location(location):
    geo_info = geolocator.geocode(location)
    if geo_info:
        return geo_info.latitude, geo_info.longitude
    return np.nan, np.nan


#transform_address(df):
all_location = df["location"].unique()
lat_lon_location = dict()
for location in all_location:
    lat_lon_location[location] = get_geo_info_location(location)
    
lat_lon_location
83/120:
def get_geo_info_address(address):
    address = row["address"]
    location = geolocator.geocode(address)
    next_index = address.find(",")

    while not location and next_index != -1:
        address = address[(next_index+1):]
        location = geolocator.geocode(address)
        next_index = address.find(",")

    if not location:
        location = geolocator.geocode(row["location"])
    return location.latitude, location.longitude 


def get_geo_info_location(location):
    geo_info = geolocator.geocode(location)
    if geo_info.isna() or get_info is None:
        return geo_info.latitude, geo_info.longitude
    return np.nan, np.nan


#transform_address(df):
all_location = df["location"].unique()
lat_lon_location = dict()
for location in all_location:
    lat_lon_location[location] = get_geo_info_location(location)
    
lat_lon_location
83/121:
def get_geo_info_address(address):
    address = row["address"]
    location = geolocator.geocode(address)
    next_index = address.find(",")

    while not location and next_index != -1:
        address = address[(next_index+1):]
        location = geolocator.geocode(address)
        next_index = address.find(",")

    if not location:
        location = geolocator.geocode(row["location"])
    return location.latitude, location.longitude 


def get_geo_info_location(location):
    geo_info = geolocator.geocode(location)
    if get_info is None:
        return geo_info.latitude, geo_info.longitude
    return np.nan, np.nan


#transform_address(df):
all_location = df["location"].unique()
lat_lon_location = dict()
for location in all_location:
    lat_lon_location[location] = get_geo_info_location(location)
    
lat_lon_location
83/122:
def get_geo_info_address(address):
    address = row["address"]
    location = geolocator.geocode(address)
    next_index = address.find(",")

    while not location and next_index != -1:
        address = address[(next_index+1):]
        location = geolocator.geocode(address)
        next_index = address.find(",")

    if not location:
        location = geolocator.geocode(row["location"])
    return location.latitude, location.longitude 


def get_geo_info_location(location):
    geo_info = geolocator.geocode(location)
    if geo_info is None:
        return geo_info.latitude, geo_info.longitude
    return np.nan, np.nan


#transform_address(df):
all_location = df["location"].unique()
lat_lon_location = dict()
for location in all_location:
    lat_lon_location[location] = get_geo_info_location(location)
    
lat_lon_location
83/123:
def get_geo_info_address(address):
    address = row["address"]
    location = geolocator.geocode(address)
    next_index = address.find(",")

    while not location and next_index != -1:
        address = address[(next_index+1):]
        location = geolocator.geocode(address)
        next_index = address.find(",")

    if not location:
        location = geolocator.geocode(row["location"])
    return location.latitude, location.longitude 


def get_geo_info_location(location):
    geo_info = geolocator.geocode(location)
    if geo_info is None:
        return np.nan, np.nan
    return geo_info.latitude, geo_info.longitude


#transform_address(df):
all_location = df["location"].unique()
lat_lon_location = dict()
for location in all_location:
    lat_lon_location[location] = get_geo_info_location(location)
    
lat_lon_location
83/124: df['location'].value_counts()
83/125: df['location'].value_counts().reset_index()
83/126: df['location'].value_counts()
83/127: df['location'].value_counts()["BTM"]
83/128: dict(df['location'].value_counts())
83/129:
import pandas as pd
from geopy.geocoders import Nominatim
import numpy as np
import folium
from folium.plugins import HeatMap


geolocator=Nominatim(user_agent="app")
83/130:
import pandas as pd
from geopy.geocoders import Nominatim
import numpy as np
import folium
from folium.plugins import HeatMap


geolocator=Nominatim(user_agent="app")
83/131:
location_count = dict(df['location'].value_counts())
map_info = []
for location in location_count:
    map_info.append(lat_lon_location[location] + [location_count[location]])
base_map = folium.Map(location=[12.9716, 77.5946], zoom_start=15)
HeatMap(map_info ,zoom=20,radius=15).add_to(basemap)
basemap
83/132:
location_count = dict(df['location'].value_counts())
map_info = []
for location in location_count:
    map_info.append(list(lat_lon_location[location]) + [location_count[location]])
base_map = folium.Map(location=[12.9716, 77.5946], zoom_start=15)
HeatMap(map_info ,zoom=20,radius=15).add_to(basemap)
basemap
83/133:
location_count = dict(df['location'].value_counts())
map_info = []
for location in location_count:
    if not lat_lon_location[location][0].isna():
        map_info.append(list(lat_lon_location[location]) + [location_count[location]])
base_map = folium.Map(location=[12.9716, 77.5946], zoom_start=15)
HeatMap(map_info ,zoom=20,radius=15).add_to(basemap)
basemap
83/134:
location_count = dict(df['location'].value_counts())
map_info = []
for location in location_count:
    if not np.isnan(lat_lon_location[location][0]):
        map_info.append(list(lat_lon_location[location]) + [location_count[location]])
base_map = folium.Map(location=[12.9716, 77.5946], zoom_start=15)
HeatMap(map_info ,zoom=20,radius=15).add_to(basemap)
basemap
83/135:
location_count = dict(df['location'].value_counts())
map_info = []
for location in location_count:
    if not np.isnan(lat_lon_location[location][0]):
        map_info.append(list(lat_lon_location[location]) + [location_count[location]])
base_map = folium.Map(location=[12.9716, 77.5946], zoom_start=15)
HeatMap(map_info ,zoom=20,radius=15).add_to(base_map)
base_map
83/136:
location_count = dict(df['location'].value_counts())
map_info = []
for location in location_count:
    if not np.isnan(lat_lon_location[location][0]):
        map_info.append(list(lat_lon_location[location]) + [location_count[location]])
base_map = folium.Map(location=[12.9716, 77.5946], zoom_start=15)
HeatMap(map_info ,zoom=20,radius=15).add_to(base_map)
83/137: base_map
83/138:
location_count = dict(df['location'].value_counts())
map_info = []
for location in location_count:
    if not np.isnan(lat_lon_location[location][0]):
        map_info.append(list(lat_lon_location[location]) + [location_count[location]])
base_map = folium.Map(location=[12.9716, 77.5946], zoom_start=15)
base_map
# HeatMap(map_info ,zoom=20,radius=15).add_to(base_map)
83/139:
location_count = dict(df['location'].value_counts())
map_info = []
for location in location_count:
    if not np.isnan(lat_lon_location[location][0]):
        map_info.append(list(lat_lon_location[location]) + [float(location_count[location])])
base_map = folium.Map(location=[12.9716, 77.5946], zoom_start=15)
HeatMap(map_info ,zoom=20,radius=15).add_to(base_map)
base_map
83/140:
location_count = dict(df['location'].value_counts())
map_info = []
for location in location_count:
    if not np.isnan(lat_lon_location[location][0]):
        map_info.append(list(lat_lon_location[location]) + [float(location_count[location])])
base_map = folium.Map(location=[12.9716, 77.5946], zoom_start=15)
HeatMap(map_info ,zoom=20,radius=15).add_to(base_map)
base_map
83/141:
location_count = dict(df['location'].value_counts())
map_info = []
for location in location_count:
    if not np.isnan(lat_lon_location[location][0]):
        map_info.append(list(lat_lon_location[location]) + [float(location_count[location])])
base_map = folium.Map(location=[12.9716, 77.5946], zoom_start=12)
HeatMap(map_info ,zoom=20,radius=15).add_to(base_map)
base_map
83/142:
location_count = dict(df['location'].value_counts())
map_info = []
for location in location_count:
    if not np.isnan(lat_lon_location[location][0]):
        map_info.append(list(lat_lon_location[location]) + [float(location_count[location])])
base_map = folium.Map(location=[12.9716, 77.5946], zoom_start=10)
HeatMap(map_info ,zoom=20,radius=15).add_to(base_map)
base_map
83/143:
location_count = dict(df['location'].value_counts())
map_info = []
for location in location_count:
    if not np.isnan(lat_lon_location[location][0]):
        map_info.append(list(lat_lon_location[location]) + [float(location_count[location])])
base_map = folium.Map(location=[12.9716, 77.5946], zoom_start=11)
HeatMap(map_info ,zoom=20,radius=15).add_to(base_map)
base_map
83/144:
location_count = dict(df['location'].value_counts())
map_info = []
for location in location_count:
    if not np.isnan(lat_lon_location[location][0]):
        map_info.append(list(lat_lon_location[location]) + [float(location_count[location])])
base_map = folium.Map(location=[12.9716, 77.5946], zoom_start=11)
HeatMap(map_info ,zoom=10,radius=15).add_to(base_map)
base_map
83/145:
location_count = dict(df['location'].value_counts())
map_info = []
for location in location_count:
    if not np.isnan(lat_lon_location[location][0]):
        map_info.append(list(lat_lon_location[location]) + [float(location_count[location])])
base_map = folium.Map(location=[12.9716, 77.5946], zoom_start=11)
HeatMap(map_info ,zoom=50,radius=15).add_to(base_map)
base_map
83/146:
location_count = dict(df['location'].value_counts())
map_info = []
for location in location_count:
    if not np.isnan(lat_lon_location[location][0]):
        map_info.append(list(lat_lon_location[location]) + [float(location_count[location])])
base_map = folium.Map(location=[12.9716, 77.5946], zoom_start=11)
HeatMap(map_info ,zoom=10,radius=15).add_to(base_map)
base_map
83/147:
location_count = dict(df['location'].value_counts())
map_info = []
for location in location_count:
    if not np.isnan(lat_lon_location[location][0]):
        map_info.append(list(lat_lon_location[location]) + [float(location_count[location])])
base_map = folium.Map(location=[12.9716, 77.5946], zoom_start=11)
HeatMap(map_info ,zoom=1,radius=15).add_to(base_map)
base_map
83/148: get_geo_info_location("Church Street")
83/149: get_geo_info_location("Church Street, India")
83/150: get_geo_info_location("Church Street, Bangalore")
83/151:
def get_geo_info_address(address):
    address = row["address"]
    location = geolocator.geocode(address)
    next_index = address.find(",")

    while not location and next_index != -1:
        address = address[(next_index+1):]
        location = geolocator.geocode(address)
        next_index = address.find(",")

    if not location:
        location = geolocator.geocode(row["location"] + ", Bangalore")
    return location.latitude, location.longitude 


def get_geo_info_location(location):
    geo_info = geolocator.geocode(location)
    if geo_info is None:
        return np.nan, np.nan
    return geo_info.latitude, geo_info.longitude


#transform_address(df):
all_location = df["location"].unique()
lat_lon_location = dict()
for location in all_location:
    lat_lon_location[location] = get_geo_info_location(location)
    
lat_lon_location
83/152:
def get_geo_info_address(address):
    address = row["address"]
    location = geolocator.geocode(address)
    next_index = address.find(",")

    while not location and next_index != -1:
        address = address[(next_index+1):]
        location = geolocator.geocode(address)
        next_index = address.find(",")

    if not location:
        location = geolocator.geocode(row["location"])
    return location.latitude, location.longitude 


def get_geo_info_location(location):
    geo_info = geolocator.geocode(location)
    if geo_info is None:
        return np.nan, np.nan
    return geo_info.latitude, geo_info.longitude


#transform_address(df):
all_location = df["location"].unique()
lat_lon_location = dict()
for location in all_location:
    lat_lon_location[location] = get_geo_info_location(location + ", Bangalore")
    
lat_lon_location
83/153:
def get_geo_info_address(address):
    address = row["address"]
    location = geolocator.geocode(address)
    next_index = address.find(",")

    while not location and next_index != -1:
        address = address[(next_index+1):]
        location = geolocator.geocode(address)
        next_index = address.find(",")

    if not location:
        location = geolocator.geocode(row["location"])
    return location.latitude, location.longitude 


def get_geo_info_location(location):
    geo_info = geolocator.geocode(location)
    if geo_info is None:
        return np.nan, np.nan
    return geo_info.latitude, geo_info.longitude


#transform_address(df):
all_location = df["location"].unique()
lat_lon_location = dict()
for location in all_location:
    lat_lon_location[location] = get_geo_info_location(location + ", Bangalore")
    
lat_lon_location
83/154:
location_count = dict(df['location'].value_counts())
map_info = []
for location in location_count:
    if not np.isnan(lat_lon_location[location][0]):
        map_info.append(list(lat_lon_location[location]) + [float(location_count[location])])
base_map = folium.Map(location=[12.9716, 77.5946], zoom_start=11)
HeatMap(map_info ,zoom=20,radius=15).add_to(base_map)
base_map
83/155:
location_count = dict(df['location'].value_counts())
map_info = []
for location in location_count:
    if not np.isnan(lat_lon_location[location][0]):
        map_info.append(list(lat_lon_location[location]) + [float(location_count[location])])
base_map = folium.Map(location=[12.9716, 77.5946], zoom_start=11)
HeatMap(map_info ,zoom=20,radius=15).add_to(base_map)
base_map
83/156: df.groupby(['location'])['price'].mean()
83/157: df.groupby(['location'])['price'].medium()
83/158: df.groupby(['location'])['price'].median()
83/159: df.groupby(['location'])['price'].mean().values()
83/160: df.groupby(['location'])['price'].mean().values
83/161:
avg_price = dict(df.groupby(['location'])['price'].mean())

map_info = []
for location in location_count:
    if not np.isnan(lat_lon_location[location][0]):
        map_info.append(list(lat_lon_location[location]) + [float(avg_price[location])])
base_map = folium.Map(location=[12.9716, 77.5946], zoom_start=11)
HeatMap(map_info ,zoom=20,radius=15).add_to(base_map)
base_map
83/162:
avg_price = dict(df.groupby(['location'])['price'].medium())

map_info = []
for location in location_count:
    if not np.isnan(lat_lon_location[location][0]):
        map_info.append(list(lat_lon_location[location]) + [float(avg_price[location])])
base_map = folium.Map(location=[12.9716, 77.5946], zoom_start=11)
HeatMap(map_info ,zoom=20,radius=15).add_to(base_map)
base_map
83/163:
avg_price = dict(df.groupby(['location'])['price'].median())

map_info = []
for location in location_count:
    if not np.isnan(lat_lon_location[location][0]):
        map_info.append(list(lat_lon_location[location]) + [float(avg_price[location])])
base_map = folium.Map(location=[12.9716, 77.5946], zoom_start=11)
HeatMap(map_info ,zoom=20,radius=15).add_to(base_map)
base_map
83/164:
avg_price = dict(df.groupby(['location'])['price'].mean())

map_info = []
for location in location_count:
    if not np.isnan(lat_lon_location[location][0]):
        map_info.append(list(lat_lon_location[location]) + [float(avg_price[location])])
base_map = folium.Map(location=[12.9716, 77.5946], zoom_start=11)
HeatMap(map_info ,zoom=20,radius=15).add_to(base_map)
base_map
83/165:
avg_price = dict(df.groupby(['location'])['price'].median())

map_info = []
for location in location_count:
    if not np.isnan(lat_lon_location[location][0]):
        map_info.append(list(lat_lon_location[location]) + [float(avg_price[location])])
base_map = folium.Map(location=[12.9716, 77.5946], zoom_start=11)
HeatMap(map_info ,zoom=20,radius=15).add_to(base_map)
base_map
83/166:
avg_price = dict(df.groupby(['location'])['price'].mean())

map_info = []
for location in location_count:
    if not np.isnan(lat_lon_location[location][0]):
        map_info.append(list(lat_lon_location[location]) + [float(avg_price[location])])
base_map = folium.Map(location=[12.9716, 77.5946], zoom_start=11)
HeatMap(map_info ,zoom=20,radius=15).add_to(base_map)
base_map
83/167:
avg_price = dict(df.groupby(['location'])['price'].max())

map_info = []
for location in location_count:
    if not np.isnan(lat_lon_location[location][0]):
        map_info.append(list(lat_lon_location[location]) + [float(avg_price[location])])
base_map = folium.Map(location=[12.9716, 77.5946], zoom_start=11)
HeatMap(map_info ,zoom=20,radius=15).add_to(base_map)
base_map
83/168:
avg_price = dict(df.groupby(['location'])['price'].min())

map_info = []
for location in location_count:
    if not np.isnan(lat_lon_location[location][0]):
        map_info.append(list(lat_lon_location[location]) + [float(avg_price[location])])
base_map = folium.Map(location=[12.9716, 77.5946], zoom_start=11)
HeatMap(map_info ,zoom=20,radius=15).add_to(base_map)
base_map
83/169:
avg_price = dict(df.groupby(['location'])['price'].maxx())

map_info = []
for location in location_count:
    if not np.isnan(lat_lon_location[location][0]):
        map_info.append(list(lat_lon_location[location]) + [float(avg_price[location])])
base_map = folium.Map(location=[12.9716, 77.5946], zoom_start=11)
HeatMap(map_info ,zoom=20,radius=15).add_to(base_map)
base_map
83/170:
avg_price = dict(df.groupby(['location'])['price'].max())

map_info = []
for location in location_count:
    if not np.isnan(lat_lon_location[location][0]):
        map_info.append(list(lat_lon_location[location]) + [float(avg_price[location])])
base_map = folium.Map(location=[12.9716, 77.5946], zoom_start=11)
HeatMap(map_info ,zoom=20,radius=15).add_to(base_map)
base_map
83/171:
avg_price = dict(df.groupby(['location'])['price'].mean())

map_info = []
for location in location_count:
    if not np.isnan(lat_lon_location[location][0]):
        map_info.append(list(lat_lon_location[location]) + [float(avg_price[location])])
base_map = folium.Map(location=[12.9716, 77.5946], zoom_start=11)
HeatMap(map_info ,zoom=20,radius=15).add_to(base_map)
base_map
83/172:
avg_price = dict(df.groupby(['location'])['price'].median())

map_info = []
for location in location_count:
    if not np.isnan(lat_lon_location[location][0]):
        map_info.append(list(lat_lon_location[location]) + [float(avg_price[location])])
base_map = folium.Map(location=[12.9716, 77.5946], zoom_start=11)
# HeatMap(map_info ,zoom=20,radius=15).add_to(base_map)
FastMarkerCluster(map_info).add_to(basemap)
base_map
83/173:
import pandas as pd
from geopy.geocoders import Nominatim
import numpy as np
import folium
from folium.plugins import HeatMap, FastMarkerCluster


geolocator=Nominatim(user_agent="app")
83/174:
avg_price = dict(df.groupby(['location'])['price'].median())

map_info = []
for location in location_count:
    if not np.isnan(lat_lon_location[location][0]):
        map_info.append(list(lat_lon_location[location]) + [float(avg_price[location])])
base_map = folium.Map(location=[12.9716, 77.5946], zoom_start=11)
# HeatMap(map_info ,zoom=20,radius=15).add_to(base_map)
FastMarkerCluster(map_info).add_to(basemap)
base_map
83/175:
avg_price = dict(df.groupby(['location'])['price'].median())

map_info = []
for location in location_count:
    if not np.isnan(lat_lon_location[location][0]):
        map_info.append(list(lat_lon_location[location]) + [float(avg_price[location])])
base_map = folium.Map(location=[12.9716, 77.5946], zoom_start=11)
# HeatMap(map_info ,zoom=20,radius=15).add_to(base_map)
FastMarkerCluster(map_info).add_to(base_map)
base_map
83/176:
avg_price = dict(df.groupby(['location'])['price'].median())

map_info = []
for location in location_count:
    if not np.isnan(lat_lon_location[location][0]):
        map_info.append(list(lat_lon_location[location]) + [float(avg_price[location])])
base_map = folium.Map(location=[12.9716, 77.5946], zoom_start=11)
HeatMap(map_info ,zoom=20,radius=15).add_to(base_map)
FastMarkerCluster(map_info).add_to(base_map)
base_map
83/177:
avg_price = dict(df.groupby(['location'])['price'].mean())

map_info = []
for location in location_count:
    if not np.isnan(lat_lon_location[location][0]):
        map_info.append(list(lat_lon_location[location]) + [float(avg_price[location])])
base_map = folium.Map(location=[12.9716, 77.5946], zoom_start=11)
HeatMap(map_info ,zoom=20,radius=15).add_to(base_map)
FastMarkerCluster(map_info).add_to(base_map)
base_map
83/178:
avg_price = dict(df.groupby(['location'])['price'].median())

map_info = []
for location in location_count:
    if not np.isnan(lat_lon_location[location][0]):
        map_info.append(list(lat_lon_location[location]) + [float(avg_price[location])])
base_map = folium.Map(location=[12.9716, 77.5946], zoom_start=11)
HeatMap(map_info ,zoom=20,radius=15).add_to(base_map)
FastMarkerCluster(map_info).add_to(base_map)
base_map
83/179:
import pandas as pd
from geopy.geocoders import Nominatim
import numpy as np
import folium
from folium.plugins import HeatMap, FastMarkerCluster
import matplotlib.pyplot as plt
import seaborn as sns


geolocator=Nominatim(user_agent="app")
83/180:
fig, ax = plt.subplots(figsize=[16,4])
sns.distplot(map_info,ax=ax)
83/181:
fig, ax = plt.subplots(figsize=[16,4])
sns.distplot(df["price"],ax=ax)
83/182:
fig, ax = plt.subplots(figsize=[16,4])
sns.distplot(df["price"].apply(// 100),ax=ax)
83/183:
def change_into_range(x)
    return int(x // 100) * 100

fig, ax = plt.subplots(figsize=[16,4])
sns.distplot(df["price"].apply(// 100),ax=ax)
83/184:
def change_into_range(x)
    return int(x // 100) * 100

fig, ax = plt.subplots(figsize=[16,4])
sns.distplot(df["price"].apply(change_into_range),ax=ax)
83/185:
def change_into_range(x):
    return int(x // 100) * 100

fig, ax = plt.subplots(figsize=[16,4])
sns.distplot(df["price"].apply(change_into_range),ax=ax)
83/186:
def change_into_range(x):
    return int(x // 100) * 100

fig, ax = plt.subplots(figsize=[16,4])
sns.distplot(df["price"].apply(change_into_range),ax=ax)
83/187:
def change_into_range(x):
    return int(x / 100) * 100

fig, ax = plt.subplots(figsize=[16,4])
sns.distplot(df["price"].apply(change_into_range),ax=ax)
83/188:
def change_into_range(x):
    return int(x // 500) * 500

fig, ax = plt.subplots(figsize=[16,4])
sns.distplot(df["price"].apply(change_into_range),ax=ax)
83/189:
fig, ax = plt.subplots(figsize=[16,4])
sns.distplot(df["price"].apply(change_into_range),ax=ax)
83/190:
fig, ax = plt.subplots(figsize=[16,4])
sns.distplot(df["price"],ax=ax)
83/191:
plt.plot(df["price"], df["rate"])
plt.show()
83/192:
plt.plot(df["price"], df["rate"], "ob")
plt.show()
83/193:
fig, ax = plt.subplots(figsize=[16,4])
sns.distplot(df["rate"],ax=ax)
83/194:
fig, ax = plt.subplots(figsize=[10,4])
sns.distplot(df["rate"],ax=ax)
83/195:
fig, ax = plt.subplots(figsize=[10,4])
sns.distplot(df["price"],ax=ax)
83/196:
location_count = dict(df['location'].value_counts())
map_info = []
for location in location_count:
    if not np.isnan(lat_lon_location[location][0]):
        map_info.append(list(lat_lon_location[location]) + [float(location_count[location])])
base_map = folium.Map(location=[12.9716, 77.5946], zoom_start=11)
HeatMap(map_info ,zoom=20,radius=15).add_to(base_map)
base_map
84/1:
import pandas as pd
from geopy.geocoders import Nominatim
import numpy as np
import folium
from folium.plugins import HeatMap, FastMarkerCluster
import matplotlib.pyplot as plt
import seaborn as sns


geolocator=Nominatim(user_agent="app")
84/2:
## Code Cell
original_df = pd.read_csv("zomato.csv")
original_df.columns
84/3:
df = original_df.copy()

df.drop(columns=['dish_liked', 'cuisines', 'menu_item'], inplace=True)
df.drop(columns=['name', 'url', 'phone', 'reviews_list'], inplace=True)

df["rate"] = df["rate"].replace("NEW", np.nan)
df["rate"] = df["rate"].replace("-", np.nan)
df.dropna(how='any',inplace=True)

#rename_features(df):
df.rename(columns={'approx_cost(for two people)': 'price', 'listed_in(city)': 'city_area',
                  'listed_in(type)': 'meal_type'}, inplace=True)

df.drop_duplicates(inplace=True)
df.info()
84/4:
#transform_rate(df):
rates = []
for _, row in df.iterrows():
    rate_in_float = float(row["rate"].split("/")[0])
    rates.append(rate_in_float)
df["rate"] = rates


#transform_price(df):
prices = []
for _, row in df.iterrows():
    price_s = row["price"]
    price_s = price_s.replace(",", "")
    price = int(price_s)
    prices.append(price)
df["price"] = prices
84/5:
def get_geo_info_address(address):
    address = row["address"]
    location = geolocator.geocode(address)
    next_index = address.find(",")

    while not location and next_index != -1:
        address = address[(next_index+1):]
        location = geolocator.geocode(address)
        next_index = address.find(",")

    if not location:
        location = geolocator.geocode(row["location"])
    return location.latitude, location.longitude 


def get_geo_info_location(location):
    geo_info = geolocator.geocode(location)
    if geo_info is None:
        return np.nan, np.nan
    return geo_info.latitude, geo_info.longitude


#transform_address(df):
all_location = df["location"].unique()
lat_lon_location = dict()
for location in all_location:
    lat_lon_location[location] = get_geo_info_location(location + ", Bangalore")
    
lat_lon_location
84/6:
location_count = dict(df['location'].value_counts())
map_info = []
for location in location_count:
    if not np.isnan(lat_lon_location[location][0]):
        map_info.append(list(lat_lon_location[location]) + [float(location_count[location])])
base_map = folium.Map(location=[12.9716, 77.5946], zoom_start=11)
HeatMap(map_info ,zoom=20,radius=15).add_to(base_map)
base_map
84/7:
avg_price = dict(df.groupby(['location'])['price'].mean())

map_info = []
for location in location_count:
    if not np.isnan(lat_lon_location[location][0]):
        map_info.append(list(lat_lon_location[location]) + [float(avg_price[location])])
base_map = folium.Map(location=[12.9716, 77.5946], zoom_start=11)
HeatMap(map_info ,zoom=20,radius=15).add_to(base_map)
FastMarkerCluster(map_info).add_to(base_map)
base_map
84/8:
avg_price = dict(df.groupby(['location'])['price'].median())

map_info = []
for location in location_count:
    if not np.isnan(lat_lon_location[location][0]):
        map_info.append(list(lat_lon_location[location]) + [float(avg_price[location])])
base_map = folium.Map(location=[12.9716, 77.5946], zoom_start=11)
HeatMap(map_info ,zoom=20,radius=15).add_to(base_map)
FastMarkerCluster(map_info).add_to(base_map)
base_map
84/9:
fig, ax = plt.subplots(figsize=[10,4])
sns.distplot(df["price"],ax=ax)
84/10:
fig, ax = plt.subplots(figsize=[10,4])
sns.distplot(df["rate"],ax=ax)
84/11: df
84/12:
x = df.iloc[:,[2,3,5,6,7,8,9,10]]
x
84/13:
x = df.iloc[:,[1,2,4,5,6,7,8,9]]
x
84/14:
x = df.iloc[:,[1,2,4,5,6,7,8,9]]
y = df["rate"]

x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=.1,random_state=353)

x_train.head()
84/15:
import pandas as pd
from geopy.geocoders import Nominatim
import numpy as np
import folium
from folium.plugins import HeatMap, FastMarkerCluster
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split


geolocator=Nominatim(user_agent="app")
84/16:
x = df.iloc[:,[1,2,4,5,6,7,8,9]]
y = df["rate"]

x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=.1,random_state=353)

x_train.head()
84/17:
x = df.iloc[:,[1,2,4,5,6,7,8,9]]
y = df["rate"]

x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=.1,random_state=353)

x_train.head()
y_train.head()
84/18:
from sklearn.tree import DecisionTreeRegressor
DTree=DecisionTreeRegressor(min_samples_leaf=.0001)
DTree.fit(x_train,y_train)
y_predict=DTree.predict(x_test)
from sklearn.metrics import r2_score
r2_score(y_test,y_predict)
84/19:
def Encode(zomato):
    for column in zomato.columns[~zomato.columns.isin(['rate', 'cost', 'votes'])]:
        zomato[column] = zomato[column].factorize()[0]
    return zomato

df_float = Encode(df.copy())
84/20:
def Encode(zomato):
    for column in zomato.columns[~zomato.columns.isin(['rate', 'cost', 'votes'])]:
        zomato[column] = zomato[column].factorize()[0]
    return zomato

df_float = Encode(df.copy())
df_float
84/21:
def Encode(zomato):
    for column in zomato.columns[~zomato.columns.isin(['rate', 'cost', 'votes'])]:
        zomato[column] = zomato[column].factorize()[0]
    return zomato

df_float = Encode(df.copy())
df_float["location"].unique()
84/22:
def Encode(zomato):
    for column in zomato.columns[~zomato.columns.isin(['rate', 'cost', 'votes'])]:
        zomato[column] = zomato[column].factorize()[0]
    return zomato

df_float = Encode(df.copy())
df_float
84/23:
def Encode(zomato):
    for column in zomato.columns[~zomato.columns.isin(['rate', 'price', 'votes'])]:
        zomato[column] = zomato[column].factorize()[0]
    return zomato

df_float = Encode(df.copy())
df_float
84/24:
def Encode(zomato):
    for column in zomato.columns[~zomato.columns.isin(['rate', 'price', 'votes'])]:
        zomato[column] = zomato[column].factorize()[0]
    return zomato

df_float = Encode(df.copy())
df_float
df
84/25:
df = original_df.copy()

df.drop(columns=['dish_liked', 'cuisines', 'menu_item'], inplace=True)
df.drop(columns=['name', 'url', 'phone', 'reviews_list', "address"], inplace=True)

df["rate"] = df["rate"].replace("NEW", np.nan)
df["rate"] = df["rate"].replace("-", np.nan)
df.dropna(how='any',inplace=True)

#rename_features(df):
df.rename(columns={'approx_cost(for two people)': 'price', 'listed_in(city)': 'city_area',
                  'listed_in(type)': 'meal_type'}, inplace=True)

df.drop_duplicates(inplace=True)
df.info()
84/26:
#transform_rate(df):
rates = []
for _, row in df.iterrows():
    rate_in_float = float(row["rate"].split("/")[0])
    rates.append(rate_in_float)
df["rate"] = rates


#transform_price(df):
prices = []
for _, row in df.iterrows():
    price_s = row["price"]
    price_s = price_s.replace(",", "")
    price = int(price_s)
    prices.append(price)
df["price"] = prices
84/27:
location_count = dict(df['location'].value_counts())
map_info = []
for location in location_count:
    if not np.isnan(lat_lon_location[location][0]):
        map_info.append(list(lat_lon_location[location]) + [float(location_count[location])])
base_map = folium.Map(location=[12.9716, 77.5946], zoom_start=11)
HeatMap(map_info ,zoom=20,radius=15).add_to(base_map)
base_map
84/28:
fig, ax = plt.subplots(figsize=[10,4])
sns.distplot(df["price"],ax=ax)
84/29:
def Encode(zomato):
    for column in zomato.columns[~zomato.columns.isin(['rate', 'price', 'votes'])]:
        zomato[column] = zomato[column].factorize()[0]
    return zomato

df_float = Encode(df.copy())
df_float
df
84/30:
def Encode(zomato):
    for column in zomato.columns[~zomato.columns.isin(['rate', 'price', 'votes'])]:
        zomato[column] = zomato[column].factorize()[0]
    return zomato

df_float = Encode(df.copy())
df_float
84/31:
x = df_float.iloc[:,[0,1,4,5,6,7,8]]
y = zomato_en['rate']
#Getting Test and Training Set
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=.1,random_state=353)
x_train.head()
84/32:
x = df_float.iloc[:,[0,1,4,5,6,7,8]]
y = df_float['rate']
#Getting Test and Training Set
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=.1,random_state=353)
x_train.head()
84/33:
from sklearn.ensemble import RandomForestRegressor
RForest=RandomForestRegressor(n_estimators=500,random_state=329,min_samples_leaf=.0001)
RForest.fit(x_train,y_train)
y_predict=RForest.predict(x_test)

r2_score(y_test,y_predict)
84/34:
import pandas as pd
from geopy.geocoders import Nominatim
import numpy as np
import folium
from folium.plugins import HeatMap, FastMarkerCluster
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score


geolocator=Nominatim(user_agent="app")
84/35:
from sklearn.ensemble import RandomForestRegressor
RForest=RandomForestRegressor(n_estimators=500,random_state=329,min_samples_leaf=.0001)
RForest.fit(x_train,y_train)
y_predict=RForest.predict(x_test)

r2_score(y_test,y_predict)
84/36:
from sklearn.tree import DecisionTreeRegressor
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=.1,random_state=105)
DTree=DecisionTreeRegressor(min_samples_leaf=.0001)
DTree.fit(x_train,y_train)
y_predict=DTree.predict(x_test)

r2_score(y_test,y_predict)
84/37:
from sklearn.ensemble import RandomForestRegressor
RForest=RandomForestRegressor(n_estimators=500,random_state=329,min_samples_leaf=.0001)
RForest.fit(x_train,y_train)
y_predict=RForest.predict(x_test)

r2_score(y_test,y_predict)
84/38:
LR=LinearRegression()
LR.fit(x_train,y_train)
y_predict=LR.predict(x_test)

r2_score(y_test,y_predict)
84/39:
from sklearn.linear_model import LinearRegression
LR=LinearRegression()
LR.fit(x_train,y_train)
y_predict=LR.predict(x_test)

r2_score(y_test,y_predict)
84/40:
from sklearn.ensemble import GradientBoostingRegressor

GBR = GradientBoostingRegressor()
GBR.fit(x_train, y_train)

y_predict=GBR.predict(x_test)

r2_score(y_test,y_predict)
84/41:
x = df_float.iloc[:,[0,1,4,5,6,7,8]]
y = df_float['rate']
#Getting Test and Training Set
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=.1,random_state=353)
x_train.head()
84/42:
from sklearn.linear_model import LinearRegression
LR=LinearRegression()
LR.fit(x_train,y_train)
y_predict=LR.predict(x_test)

r2_score(y_test,y_predict)
84/43:
from sklearn.tree import DecisionTreeRegressor
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=.1,random_state=105)
DTree=DecisionTreeRegressor(min_samples_leaf=.0001)
DTree.fit(x_train,y_train)
y_predict=DTree.predict(x_test)

r2_score(y_test,y_predict)
84/44:
from sklearn.linear_model import LinearRegression
LR=LinearRegression()
LR.fit(x_train,y_train)
y_predict=LR.predict(x_test)

r2_score(y_test,y_predict)
84/45:
from sklearn.tree import DecisionTreeRegressor
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=.1,random_state=105)
DTree=DecisionTreeRegressor(min_samples_leaf=.0001)
DTree.fit(x_train,y_train)
y_predict=DTree.predict(x_test)

r2_score(y_test,y_predict)
84/46:
from sklearn.ensemble import RandomForestRegressor
RForest=RandomForestRegressor(n_estimators=500,random_state=329,min_samples_leaf=.0001)
RForest.fit(x_train,y_train)
y_predict=RForest.predict(x_test)

r2_score(y_test,y_predict)
84/47:
x = df_float.iloc[:,[0,1,4,5,6,7,8]]
y = df_float['rate']
#Getting Test and Training Set
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=.1,random_state=353)
x_train.head()
84/48:
from sklearn.tree import DecisionTreeRegressor
DTree=DecisionTreeRegressor(min_samples_leaf=.0001)
DTree.fit(x_train,y_train)
y_predict=DTree.predict(x_test)

r2_score(y_test,y_predict)
84/49:
from sklearn.tree import DecisionTreeRegressor
DTree=DecisionTreeRegressor(min_samples_leaf=.0001)
DTree.fit(x_train,y_train)
y_predict=DTree.predict(x_test)

r2_score(y_test,y_predict)
84/50:
from sklearn.linear_model import LinearRegression
LR=LinearRegression()
LR.fit(x_train,y_train)
y_predict=LR.predict(x_test)

r2_score(y_test,y_predict)
84/51:
from sklearn.linear_model import LinearRegression
LR=LinearRegression()
LR.fit(x_train,y_train)
y_predict=LR.predict(x_test)

r2_score(y_test,y_predict)
84/52:
from sklearn.ensemble import GradientBoostingRegressor

GBR = GradientBoostingRegressor()
GBR.fit(x_train, y_train)

y_predict=GBR.predict(x_test)

r2_score(y_test,y_predict)
84/53:
x = df_float.iloc[:,[0,1,4,5,6,7,8]]
y = df_float['rate']
#Getting Test and Training Set
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=.1,random_state=353)
x_train.head()
y_train.head()
84/54:
x = df_float.iloc[:,[0,1,4,5,6,7,8]]
y = df_float['rate']
#Getting Test and Training Set
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=.1,random_state=353)
x_train.head()
84/55:
x = df_float.iloc[:,[0,1,4,5,6,7]]
y = df_float['rate']
#Getting Test and Training Set
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=.1,random_state=353)
x_train.head()
84/56:
from sklearn.ensemble import RandomForestRegressor
RForest=RandomForestRegressor(n_estimators=500,random_state=329,min_samples_leaf=.0001)
RForest.fit(x_train,y_train)
y_predict=RForest.predict(x_test)

r2_score(y_test,y_predict)
84/57:
from sklearn.tree import DecisionTreeRegressor
DTree=DecisionTreeRegressor(min_samples_leaf=.0001)
DTree.fit(x_train,y_train)
y_predict=DTree.predict(x_test)

r2_score(y_test,y_predict)
84/58:
from sklearn.linear_model import LinearRegression
LR=LinearRegression()
LR.fit(x_train,y_train)
y_predict=LR.predict(x_test)

r2_score(y_test,y_predict)
84/59:
from sklearn.ensemble import GradientBoostingRegressor

GBR = GradientBoostingRegressor()
GBR.fit(x_train, y_train)

y_predict=GBR.predict(x_test)

r2_score(y_test,y_predict)
84/60:
from sklearn.ensemble import  ExtraTreesRegressor
ETree=ExtraTreesRegressor(n_estimators = 100)
ETree.fit(x_train,y_train)
y_predict=ETree.predict(x_test)

r2_score(y_test,y_predict)
84/61:
from sklearn.tree import DecisionTreeRegressor
DTree=DecisionTreeRegressor()
DTree.fit(x_train,y_train)
y_predict=DTree.predict(x_test)

r2_score(y_test,y_predict)
84/62:
x = df_float.iloc[:,[0,1,4,5,6,7]]
y = df_float.iloc[:,[2, 3]]
#Getting Test and Training Set
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=.1,random_state=353)
x_train.head()
84/63:
from sklearn.ensemble import RandomForestRegressor
RForest=RandomForestRegressor(n_estimators=500,random_state=329,min_samples_leaf=.0001)
RForest.fit(x_train,y_train)
y_predict=RForest.predict(x_test)

r2_score(y_test,y_predict)
84/64:
from sklearn.tree import DecisionTreeRegressor
DTree=DecisionTreeRegressor()
DTree.fit(x_train,y_train)
y_predict=DTree.predict(x_test)

r2_score(y_test,y_predict)
84/65:
from sklearn.linear_model import LinearRegression
LR=LinearRegression()
LR.fit(x_train,y_train)
y_predict=LR.predict(x_test)

r2_score(y_test,y_predict)
84/66:
from sklearn.ensemble import GradientBoostingRegressor

GBR = GradientBoostingRegressor()
GBR.fit(x_train, y_train)

y_predict=GBR.predict(x_test)

r2_score(y_test,y_predict)
84/67:
from sklearn.ensemble import  ExtraTreesRegressor
ETree=ExtraTreesRegressor(n_estimators = 100)
ETree.fit(x_train,y_train)
y_predict=ETree.predict(x_test)

r2_score(y_test,y_predict)
84/68:
import pandas as pd
from geopy.geocoders import Nominatim
import numpy as np
import folium
from folium.plugins import HeatMap, FastMarkerCluster
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score


geolocator=Nominatim(user_agent="app")
84/69:
def read_data():
    df = pd.read_csv("zomato.csv")
    return df

original_df = read_data()
original_df.columns
84/70:
df = original_df.copy()

def drop_unnecessary_columns(df):
    df.drop(columns=['dish_liked', 'cuisines', 'menu_item'], inplace=True)
    df.drop(columns=['name', 'url', 'phone', 'reviews_list', "address"], inplace=True)
    return df

df = drop_unnecessary_columns(df)
df.columns
84/71:
def remove_nan(df):
    df["rate"] = df["rate"].replace("NEW", np.nan)
    df["rate"] = df["rate"].replace("-", np.nan)
    df.dropna(how='any',inplace=True)
    return df

df = remove_nan(df):
df.isnull().sum()
84/72:
def remove_nan(df):
    df["rate"] = df["rate"].replace("NEW", np.nan)
    df["rate"] = df["rate"].replace("-", np.nan)
    df.dropna(how='any',inplace=True)
    return df

df = remove_nan(df)
df.isnull().sum()
84/73:
def rename_features(df):
    df.rename(columns={'approx_cost(for two people)': 'price', 'listed_in(city)': 'city_area',
                      'listed_in(type)': 'meal_type'}, inplace=True)
    return df


df = rename_features(df)
84/74:
df.drop_duplicates(inplace=True)
df.info()
84/75:
def transform_rate(df):
    rates = []
    for _, row in df.iterrows():
        rate_in_float = float(row["rate"].split("/")[0])
        rates.append(rate_in_float)
    df["rate"] = rates
    return df


def transform_price(df):
    prices = []
    for _, row in df.iterrows():
        price_s = row["price"]
        price_s = price_s.replace(",", "")
        price = int(price_s)
        prices.append(price)
    df["price"] = prices
    return df

df = transform_rate(df)
df = transform_price(df)

df.[["rate", "price"]]
84/76:
def transform_rate(df):
    rates = []
    for _, row in df.iterrows():
        rate_in_float = float(row["rate"].split("/")[0])
        rates.append(rate_in_float)
    df["rate"] = rates
    return df


def transform_price(df):
    prices = []
    for _, row in df.iterrows():
        price_s = row["price"]
        price_s = price_s.replace(",", "")
        price = int(price_s)
        prices.append(price)
    df["price"] = prices
    return df

df = transform_rate(df)
df = transform_price(df)

df[["rate", "price"]]
84/77:
def transform_rate(df):
    rates = []
    for _, row in df.iterrows():
        rate_in_float = float(row["rate"].split("/")[0])
        rates.append(rate_in_float)
    df["rate"] = rates
    return df


def transform_price(df):
    prices = []
    for _, row in df.iterrows():
        price_s = row["price"]
        price_s = price_s.replace(",", "")
        price = int(price_s)
        prices.append(price)
    df["price"] = prices
    return df

df = transform_rate(df)
df = transform_price(df)

df["rate", "price"]
84/78:
import pandas as pd
from geopy.geocoders import Nominatim
import numpy as np
import folium
from folium.plugins import HeatMap, FastMarkerCluster
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score


geolocator=Nominatim(user_agent="app")
84/79:
def read_data():
    df = pd.read_csv("zomato.csv")
    return df

original_df = read_data()
original_df.columns
84/80:
df = original_df.copy()

def drop_unnecessary_columns(df):
    df.drop(columns=['dish_liked', 'cuisines', 'menu_item'], inplace=True)
    df.drop(columns=['name', 'url', 'phone', 'reviews_list', "address"], inplace=True)
    return df

df = drop_unnecessary_columns(df)
df.columns
84/81:
def remove_nan(df):
    df["rate"] = df["rate"].replace("NEW", np.nan)
    df["rate"] = df["rate"].replace("-", np.nan)
    df.dropna(how='any',inplace=True)
    return df

df = remove_nan(df)
df.isnull().sum()
84/82:
def rename_features(df):
    df.rename(columns={'approx_cost(for two people)': 'price', 'listed_in(city)': 'city_area',
                      'listed_in(type)': 'meal_type'}, inplace=True)
    return df


df = rename_features(df)
84/83:
df.drop_duplicates(inplace=True)
df.info()
84/84:
def transform_rate(df):
    rates = []
    for _, row in df.iterrows():
        rate_in_float = float(row["rate"].split("/")[0])
        rates.append(rate_in_float)
    df["rate"] = rates
    return df


def transform_price(df):
    prices = []
    for _, row in df.iterrows():
        price_s = row["price"]
        price_s = price_s.replace(",", "")
        price = int(price_s)
        prices.append(price)
    df["price"] = prices
    return df

df = transform_rate(df)
df = transform_price(df)

df["rate", "price"]
85/1:
import pandas as pd
from geopy.geocoders import Nominatim
import numpy as np
import folium
from folium.plugins import HeatMap, FastMarkerCluster
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score


geolocator=Nominatim(user_agent="app")
85/2:
def read_data():
    df = pd.read_csv("zomato.csv")
    return df

original_df = read_data()
original_df.columns
85/3:
df = original_df.copy()

def drop_unnecessary_columns(df):
    df.drop(columns=['dish_liked', 'cuisines', 'menu_item'], inplace=True)
    df.drop(columns=['name', 'url', 'phone', 'reviews_list', "address"], inplace=True)
    return df

df = drop_unnecessary_columns(df)
df.columns
85/4:
def remove_nan(df):
    df["rate"] = df["rate"].replace("NEW", np.nan)
    df["rate"] = df["rate"].replace("-", np.nan)
    df.dropna(how='any',inplace=True)
    return df

df = remove_nan(df)
df.isnull().sum()
85/5:
def rename_features(df):
    df.rename(columns={'approx_cost(for two people)': 'price', 'listed_in(city)': 'city_area',
                      'listed_in(type)': 'meal_type'}, inplace=True)
    return df


df = rename_features(df)
85/6:
df.drop_duplicates(inplace=True)
df.info()
85/7:
def transform_rate(df):
    rates = []
    for _, row in df.iterrows():
        rate_in_float = float(row["rate"].split("/")[0])
        rates.append(rate_in_float)
    df["rate"] = rates
    return df


def transform_price(df):
    prices = []
    for _, row in df.iterrows():
        price_s = row["price"]
        price_s = price_s.replace(",", "")
        price = int(price_s)
        prices.append(price)
    df["price"] = prices
    return df

df = transform_rate(df)
df = transform_price(df)

df[["rate", "price"]
87/1:
import pandas as pd
from geopy.geocoders import Nominatim
import numpy as np
import folium
from folium.plugins import HeatMap, FastMarkerCluster
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score


geolocator=Nominatim(user_agent="app")
87/2:
def read_data():
    df = pd.read_csv("zomato.csv")
    return df

original_df = read_data()
original_df.columns
87/3:
df = original_df.copy()

def drop_unnecessary_columns(df):
    df.drop(columns=['dish_liked', 'cuisines', 'menu_item'], inplace=True)
    df.drop(columns=['name', 'url', 'phone', 'reviews_list', "address"], inplace=True)
    return df

df = drop_unnecessary_columns(df)
df.columns
87/4:
def remove_nan(df):
    df["rate"] = df["rate"].replace("NEW", np.nan)
    df["rate"] = df["rate"].replace("-", np.nan)
    df.dropna(how='any',inplace=True)
    return df

df = remove_nan(df)
df.isnull().sum()
87/5:
def rename_features(df):
    df.rename(columns={'approx_cost(for two people)': 'price', 'listed_in(city)': 'city_area',
                      'listed_in(type)': 'meal_type'}, inplace=True)
    return df


df = rename_features(df)
87/6:
df.drop_duplicates(inplace=True)
df.info()
87/7:
def transform_rate(df):
    rates = []
    for _, row in df.iterrows():
        rate_in_float = float(row["rate"].split("/")[0])
        rates.append(rate_in_float)
    df["rate"] = rates
    return df


def transform_price(df):
    prices = []
    for _, row in df.iterrows():
        price_s = row["price"]
        price_s = price_s.replace(",", "")
        price = int(price_s)
        prices.append(price)
    df["price"] = prices
    return df

df = transform_rate(df)
df = transform_price(df)

df[["rate", "price"]]
87/8:
def get_geo_info_address(address):
    address = row["address"]
    location = geolocator.geocode(address)
    next_index = address.find(",")

    while not location and next_index != -1:
        address = address[(next_index+1):]
        location = geolocator.geocode(address)
        next_index = address.find(",")

    if not location:
        location = geolocator.geocode(row["location"])
    return location.latitude, location.longitude 


def get_geo_info_location(location):
    geo_info = geolocator.geocode(location)
    if geo_info is None:
        return np.nan, np.nan
    return geo_info.latitude, geo_info.longitude


#transform_address(df):
all_location = df["location"].unique()
lat_lon_location = dict()
for location in all_location:
    lat_lon_location[location] = get_geo_info_location(location + ", Bangalore")
    
lat_lon_location
87/9:
location_count = dict(df['location'].value_counts())
map_info = []
for location in location_count:
    if not np.isnan(lat_lon_location[location][0]):
        map_info.append(list(lat_lon_location[location]) + [float(location_count[location])])
base_map = folium.Map(location=[12.9716, 77.5946], zoom_start=11)
HeatMap(map_info ,zoom=20,radius=15).add_to(base_map)
base_map
87/10:
avg_price = dict(df.groupby(['location'])['price'].mean())

map_info = []
for location in location_count:
    if not np.isnan(lat_lon_location[location][0]):
        map_info.append(list(lat_lon_location[location]) + [float(avg_price[location])])
base_map = folium.Map(location=[12.9716, 77.5946], zoom_start=11)
HeatMap(map_info ,zoom=20,radius=15).add_to(base_map)
FastMarkerCluster(map_info).add_to(base_map)
base_map
87/11:
med_price = dict(df.groupby(['location'])['price'].median())

map_info = []
for location in location_count:
    if not np.isnan(lat_lon_location[location][0]):
        map_info.append(list(lat_lon_location[location]) + [float(med_price[location])])
base_map = folium.Map(location=[12.9716, 77.5946], zoom_start=11)
HeatMap(map_info ,zoom=20,radius=15).add_to(base_map)
FastMarkerCluster(map_info).add_to(base_map)
base_map
87/12:
fig, ax = plt.subplots(figsize=[10,4])
sns.distplot(df["price"],ax=ax)
87/13:
fig, ax = plt.subplots(figsize=[10,4])
sns.distplot(df["rate"],ax=ax)
87/14:
plt.plot(df["price"], df["rate"], "ob")
plt.show()
87/15:
def Encode(zomato):
    for column in zomato.columns[~zomato.columns.isin(['rate', 'price', 'votes'])]:
        zomato[column] = zomato[column].factorize()[0]
    return zomato

df_float = Encode(df.copy())
87/16:
x = df_float.iloc[:,[0,1,4,5,6,7]]
y = df_float.iloc[:,[2, 3]]
#Getting Test and Training Set
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=.1,random_state=353)
x_train.head()
87/17:
from sklearn.ensemble import RandomForestRegressor
RForest=RandomForestRegressor(n_estimators=500,random_state=329,min_samples_leaf=.0001)
RForest.fit(x_train,y_train)
y_predict=RForest.predict(x_test)

r2_score(y_test,y_predict)
87/18:
from sklearn.tree import DecisionTreeRegressor
DTree=DecisionTreeRegressor()
DTree.fit(x_train,y_train)
y_predict=DTree.predict(x_test)

r2_score(y_test,y_predict)
87/19:
from sklearn.linear_model import LinearRegression
LR=LinearRegression()
LR.fit(x_train,y_train)
y_predict=LR.predict(x_test)

r2_score(y_test,y_predict)
87/20:
from sklearn.ensemble import  ExtraTreesRegressor
ETree=ExtraTreesRegressor(n_estimators = 100)
ETree.fit(x_train,y_train)
y_predict=ETree.predict(x_test)

r2_score(y_test,y_predict)
87/21:
def fridge_sorted_bar(color='blue'):
    df.sort_values(0).plot.bar( color=color)
87/22: fridge_sorted_bar(color='red')
87/23:
def get_geo_info_location(location):
    geo_info = geolocator.geocode(location)
    if geo_info is None:
        return np.nan, np.nan
    return geo_info.latitude, geo_info.longitude


def get_location_coordinate(df):
    all_location = df["location"].unique()
    coordinate = dict()
    for location in all_location:
        coordinate[location] = get_geo_info_location(location + ", Bangalore")
    return 
    
coordinate = get_location_coordinate(df)
coordinate
87/24:
def get_geo_info_location(location):
    geo_info = geolocator.geocode(location)
    if geo_info is None:
        return np.nan, np.nan
    return geo_info.latitude, geo_info.longitude


def get_location_coordinate(df):
    all_location = df["location"].unique()
    coordinate = dict()
    for location in all_location:
        coordinate[location] = get_geo_info_location(location + ", Bangalore")
    return 
    
lat_lon_location = get_location_coordinate(df)
lat_lon_location
87/25:
geolocator=Nominatim(user_agent="app")   


def get_geo_info_location(location):
    geo_info = geolocator.geocode(location)
    if geo_info is None:
        return np.nan, np.nan
    return geo_info.latitude, geo_info.longitude


def get_location_coordinate(df):
    all_location = df["location"].unique()
    coordinate = dict()
    for location in all_location:
        coordinate[location] = get_geo_info_location(location + ", Bangalore")
    return coordinate



lat_lon_location = get_location_coordinate(df)
lat_lon_location
87/26:
location_count = dict(df['location'].value_counts())
map_info = []
for location in location_count:
    if not np.isnan(lat_lon_location[location][0]):
        map_info.append(list(lat_lon_location[location]) + [float(location_count[location])])
base_map = folium.Map(location=[12.9716, 77.5946], zoom_start=11)
HeatMap(map_info ,zoom=20,radius=15).add_to(base_map)
base_map
87/27:
def heatmap_generate(lat_lon_location, location_info)
    map_info = []
    for location in location_info:
        if not np.isnan(lat_lon_location[location][0]):
            map_info.append(list(lat_lon_location[location]) + [float(location_info[location])])
    base_map = folium.Map(location=[12.9716, 77.5946], zoom_start=11)
    HeatMap(map_info ,zoom=20,radius=15).add_to(base_map)
    return base_map

location_count = dict(df['location'].value_counts())
heatmap_count = heatmap_generate(lat_lon_location, location_count)
heatmap_count
87/28:
def heatmap_generate(lat_lon_location, location_info):
    map_info = []
    for location in location_info:
        if not np.isnan(lat_lon_location[location][0]):
            map_info.append(list(lat_lon_location[location]) + [float(location_info[location])])
    base_map = folium.Map(location=[12.9716, 77.5946], zoom_start=11)
    HeatMap(map_info ,zoom=20,radius=15).add_to(base_map)
    return base_map

location_count = dict(df['location'].value_counts())
heatmap_count = heatmap_generate(lat_lon_location, location_count)
heatmap_count
87/29:
def heatmap_generate(lat_lon_location, location_info):
    map_info = []
    for location in location_info:
        if not np.isnan(lat_lon_location[location][0]):
            map_info.append(list(lat_lon_location[location]) + [float(location_info[location])])
    base_map = folium.Map(location=[12.9716, 77.5946], zoom_start=12)
    HeatMap(map_info ,zoom=20,radius=15).add_to(base_map)
    return base_map

location_count = dict(df['location'].value_counts())
heatmap_count = heatmap_generate(lat_lon_location, location_count)
heatmap_count
87/30:
def heatmap_generate(lat_lon_location, location_info):
    map_info = []
    for location in location_info:
        if not np.isnan(lat_lon_location[location][0]):
            map_info.append(list(lat_lon_location[location]) + [float(location_info[location])])
    base_map = folium.Map(location=[12.9716, 77.5946], zoom_start=10)
    HeatMap(map_info ,zoom=20,radius=15).add_to(base_map)
    return base_map

location_count = dict(df['location'].value_counts())
heatmap_count = heatmap_generate(lat_lon_location, location_count)
heatmap_count
87/31:
def heatmap_generate(lat_lon_location, location_info):
    map_info = []
    for location in location_info:
        if not np.isnan(lat_lon_location[location][0]):
            map_info.append(list(lat_lon_location[location]) + [float(location_info[location])])
    base_map = folium.Map(location=[12.9716, 77.5946], zoom_start=11)
    HeatMap(map_info ,zoom=20,radius=15).add_to(base_map)
    return base_map

location_count = dict(df['location'].value_counts())
heatmap_count = heatmap_generate(lat_lon_location, location_count)
heatmap_count
87/32:
def heatmap_generate(lat_lon_location, location_info):
    map_info = []
    for location in location_info:
        if not np.isnan(lat_lon_location[location][0]):
            map_info.append(list(lat_lon_location[location]) + [float(location_info[location])])
    base_map = folium.Map(location=[12.9716, 77.5946], zoom_start=11)
    HeatMap(map_info ,zoom=20,radius=15).add_to(base_map)
    return base_map

location_count = dict(df['location'].value_counts())
heatmap_count = heatmap_generate(lat_lon_location, location_count)
heatmap_count
87/33:
def heatmap_generate(lat_lon_location, location_info):
    map_info = []
    for location in location_info:
        if not np.isnan(lat_lon_location[location][0]):
            map_info.append(list(lat_lon_location[location]) + [float(location_info[location])])
    base_map = folium.Map(location=[12.9716, 77.5946], zoom_start=11)
    HeatMap(map_info ,zoom=20,radius=15).add_to(base_map)
    return base_map
87/34:
location_count = dict(df['location'].value_counts())
heatmap_count = heatmap_generate(lat_lon_location, location_count)
heatmap_count
87/35:
def heatmap_generate(lat_lon_location, location_info, need_clustering=False):
    map_info = []
    for location in location_info:
        if not np.isnan(lat_lon_location[location][0]):
            map_info.append(list(lat_lon_location[location]) + [float(location_info[location])])
    base_map = folium.Map(location=[12.9716, 77.5946], zoom_start=11)
    HeatMap(map_info ,zoom=20,radius=15).add_to(base_map)
    if need_clustering:
        FastMarkerCluster(map_info).add_to(base_map)
    return base_map
87/36:
location_count = dict(df['location'].value_counts())
heatmap_count = heatmap_generate(lat_lon_location, location_count)
heatmap_count
87/37:
avg_price = dict(df.groupby(['location'])['price'].mean())
heatmap_avg = heatmap_generate(lat_lon_location, avg_price, need_clustering=True)
heatmap_avg
87/38:
med_price = dict(df.groupby(['location'])['price'].median())
heatmap_med = heatmap_generate(lat_lon_location, med_price, need_clustering=True)
heatmap_med
87/39:
def generate_count_heatmap():
    location_count = dict(df['location'].value_counts())
    heatmap_count = heatmap_generate(lat_lon_location, location_count)
generate_count_heatmap():
87/40:
def generate_count_heatmap():
    location_count = dict(df['location'].value_counts())
    heatmap_count = heatmap_generate(lat_lon_location, location_count)
generate_count_heatmap()
87/41:
def generate_count_heatmap():
    location_count = dict(df['location'].value_counts())
    heatmap_count = heatmap_generate(lat_lon_location, location_count)
    return hearmap_count
generate_count_heatmap()
87/42:
def generate_count_heatmap():
    location_count = dict(df['location'].value_counts())
    heatmap_count = heatmap_generate(lat_lon_location, location_count)
    return heatmap_count
generate_count_heatmap()
87/43:
def generate_count_heatmap():
    location_count = dict(df['location'].value_counts())
    heatmap_count = heatmap_generate(lat_lon_location, location_count)
    return heatmap_count
87/44:
def generate_avg_heatmap():
    avg_price = dict(df.groupby(['location'])['price'].mean())
    heatmap_avg = heatmap_generate(lat_lon_location, avg_price, need_clustering=True)
    return heatmap_ap_avg
87/45:
def generate_avg_heatmap():
    avg_price = dict(df.groupby(['location'])['price'].mean())
    heatmap_avg = heatmap_generate(lat_lon_location, avg_price, need_clustering=True)
    return heatmap_ap_avg

generate_avg_heatmap()
87/46:
def generate_avg_heatmap():
    avg_price = dict(df.groupby(['location'])['price'].mean())
    heatmap_avg = heatmap_generate(lat_lon_location, avg_price, need_clustering=True)
    return heatmap_avg

generate_avg_heatmap()
87/47:
def generate_med_heatmap():
    med_price = dict(df.groupby(['location'])['price'].median())
    heatmap_med = heatmap_generate(lat_lon_location, med_price, need_clustering=True)
    return heatmap_med

generate_med_heatmap()
87/48:
def generate_avg_heatmap():
    avg_price = dict(df.groupby(['location'])['price'].mean())
    heatmap_avg = heatmap_generate(lat_lon_location, avg_price, need_clustering=True)
    return heatmap_avg
87/49:
def generate_med_heatmap():
    med_price = dict(df.groupby(['location'])['price'].median())
    heatmap_med = heatmap_generate(lat_lon_location, med_price, need_clustering=True)
    return heatmap_med
87/50:
def generate_price_dist():
    fig, ax = plt.subplots(figsize=[10,4])
    sns.distplot(df["price"],ax=ax)
    
generate_price_dist()
87/51:
def generate_price_dist():
    fig, ax = plt.subplots(figsize=[10,4])
    sns.distplot(df["price"],ax=ax)
87/52:
def generate_rate_dist():
    fig, ax = plt.subplots(figsize=[10,4])
    sns.distplot(df["rate"],ax=ax)

generate_rate_dist()
87/53:
def generate_rate_dist():
    fig, ax = plt.subplots(figsize=[10,4])
    sns.distplot(df["rate"],ax=ax)
87/54:
plt.plot(df["rate"], df["votes"], "ob")
plt.show()
87/55:
def get_train_test_data(df_float):
    x = df_float.iloc[:,[0,1,4,5,6,7]]
    y = df_float.iloc[:,[2, 3]]
    x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=.1,random_state=353)
    
x_train,x_test,y_train,y_test = get_train_test_data(df_float)
87/56:
def Encode(zomato):
    for column in zomato.columns[~zomato.columns.isin(['rate', 'price', 'votes'])]:
        zomato[column] = zomato[column].factorize()[0]
    return zomato

df_float = Encode(df.copy())
df_float.head()
87/57:
def get_train_test_data(df_float):
    x = df_float.iloc[:,[0,1,4,5,6,7]]
    y = df_float.iloc[:,[2, 3]]
    x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=.1,random_state=353)
    
x_train,x_test,y_train,y_test = get_train_test_data(df_float)
87/58:
def get_train_test_data(df_float):
    x = df_float.iloc[:,[0, 1, 4, 5, 6, 7]]
    y = df_float.iloc[:,[2, 3]]
    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.1, random_state=353)
    return x_train, x_test, y_train, y_test
    
x_train, x_test, y_train, y_test = get_train_test_data(df_float)
87/59:
def Encode(df):
    for column in df.columns[~df.columns.isin(['rate', 'price', 'votes'])]:
        df[column] = df[column].factorize()[0]
    return df

df_float = Encode(df.copy())
df_float.head()
87/60:
def get_train_test_data(df_float):
    x = df_float.iloc[:,[0, 1, 4, 5, 6, 7]]
    y = df_float.iloc[:,[2, 3]]
    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.1, random_state=353)
    return x_train, x_test, y_train, y_test
    
x_train, x_test, y_train, y_test = get_train_test_data(df_float)
87/61:
def get_train_test_data(df_float):
    x = df_float.iloc[:,[0, 1, 4, 5, 6, 7, 8]]
    y = df_float.iloc[:,[2, 3]]
    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.1, random_state=353)
    return x_train, x_test, y_train, y_test
    
x_train, x_test, y_train, y_test = get_train_test_data(df_float)
87/62:
from sklearn.ensemble import RandomForestRegressor
RForest=RandomForestRegressor(n_estimators=500,random_state=329,min_samples_leaf=.0001)
RForest.fit(x_train,y_train)
y_predict=RForest.predict(x_test)

r2_score(y_test,y_predict)
87/63:
from sklearn.tree import DecisionTreeRegressor
DTree=DecisionTreeRegressor()
DTree.fit(x_train,y_train)
y_predict=DTree.predict(x_test)

r2_score(y_test,y_predict)
87/64:
from sklearn.linear_model import LinearRegression
LR=LinearRegression()
LR.fit(x_train,y_train)
y_predict=LR.predict(x_test)

r2_score(y_test,y_predict)
87/65:
from sklearn.ensemble import  ExtraTreesRegressor
ETree=ExtraTreesRegressor(n_estimators = 100)
ETree.fit(x_train,y_train)
y_predict=ETree.predict(x_test)

r2_score(y_test,y_predict)
87/66:
def get_train_test_data(df_float):
    x = df_float.iloc[:,[0, 1, 4, 5, 6, 7]]
    y = df_float.iloc[:,[2, 3]]
    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.1, random_state=353)
    return x_train, x_test, y_train, y_test
    
x_train, x_test, y_train, y_test = get_train_test_data(df_float)
87/67:
from sklearn.ensemble import RandomForestRegressor
RForest=RandomForestRegressor(n_estimators=500,random_state=329,min_samples_leaf=.0001)
RForest.fit(x_train,y_train)
y_predict=RForest.predict(x_test)

r2_score(y_test,y_predict)
87/68:
from sklearn.tree import DecisionTreeRegressor
DTree=DecisionTreeRegressor()
DTree.fit(x_train,y_train)
y_predict=DTree.predict(x_test)

r2_score(y_test,y_predict)
87/69:
from sklearn.linear_model import LinearRegression
LR=LinearRegression()
LR.fit(x_train,y_train)
y_predict=LR.predict(x_test)

r2_score(y_test,y_predict)
87/70:
from sklearn.ensemble import  ExtraTreesRegressor
ETree=ExtraTreesRegressor(n_estimators = 100)
ETree.fit(x_train,y_train)
y_predict=ETree.predict(x_test)

r2_score(y_test,y_predict)
87/71:
def plot_rate_votes():
    plt.plot(df["rate"], df["votes"], "ob")
    plt.show()
87/72:
import pandas as pd
from geopy.geocoders import Nominatim
import numpy as np
import folium
from folium.plugins import HeatMap, FastMarkerCluster
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import  ExtraTreesRegressor
87/73:
def read_data():
    df = pd.read_csv("zomato.csv")
    return df

original_df = read_data()
original_df.head()
87/74:
def rename_features(df):
    df.rename(columns={'approx_cost(for two people)': 'price', 'listed_in(city)': 'city_area',
                      'listed_in(type)': 'meal_type'}, inplace=True)
    return df


df = rename_features(df)
df.columns
87/75:
def test_model():

RF = train_random_forest()
    RF.fit(x_train,y_train)
    y_predict=RForest.predict(x_test)

r2_score(y_test,y_predict)
87/76:
def test_model(model, x_test, y_test):
    y_predict=model.predict(x_test)
    score = r2_score(y_test,y_predict)
    return score
87/77:
def train_random_forest(x_train, y_train):
    random_forest=RandomForestRegressor(n_estimators=500,random_state=329,min_samples_leaf=.0001)
    RF.fit(x_train,y_train)
    return random_forest


RF_model = train_random_forest(x_train, y_train)
RF_score = test_model(RF_model, x_test, y_tets)
RF_score
87/78:
def train_random_forest(x_train, y_train):
    random_forest=RandomForestRegressor(n_estimators=500,random_state=329,min_samples_leaf=.0001)
    random_forest.fit(x_train,y_train)
    return random_forest


RF_model = train_random_forest(x_train, y_train)
RF_score = test_model(RF_model, x_test, y_tets)
RF_score
87/79:
def train_random_forest(x_train, y_train):
    random_forest=RandomForestRegressor(n_estimators=500,random_state=329,min_samples_leaf=.0001)
    random_forest.fit(x_train,y_train)
    return random_forest


RF_model = train_random_forest(x_train, y_train)
RF_score = test_model(RF_model, x_test, y_test)
RF_score
87/80:
def get_train_test_data(df_float):
    x = df_float.iloc[:,[0, 1, 4, 5, 6, 7]]
    y = df_float.iloc[:,[2, 3]]
    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.1, random_state=353)
    return x_train, x_test, y_train, y_test
    
x_train, x_test, y_train, y_test = get_train_test_data(df_float)
87/81:
def get_train_test_data(df_float):
    x = df_float.iloc[:,[0, 1, 4, 5, 6, 7]]
    y = df_float.iloc[:,[2, 3]]
    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.1, random_state=353)
    return x_train, x_test, y_train, y_test
    
x_train, x_test, y_train, y_test = get_train_test_data(df_float)
x_train.head()
87/82:
def test_model(model, x_test, y_test):
    y_predict=model.predict(x_test)
    score = r2_score(y_test,y_predict)
    return score
87/83:
def train_random_forest(x_train, y_train):
    random_forest=RandomForestRegressor(n_estimators=500,random_state=329,min_samples_leaf=.0001)
    random_forest.fit(x_train,y_train)
    return random_forest


RF_model = train_random_forest(x_train, y_train)
RF_score = test_model(RF_model, x_test, y_test)
RF_score
87/84:
def train_decision_tree(x_train, y_train):
    decision_tree=DecisionTreeRegressor()
    decision_tree.fit(x_train,y_train)
    return d

r2_score(y_test,y_predict)
87/85:
def train_decision_tree(x_train, y_train):
    decision_tree=DecisionTreeRegressor()
    decision_tree.fit(x_train,y_train)
    return decision_tree

DT_model = train_decision_tree(x_train, y_train)
DT_score = test_model(DT_model, x_test, y_test)
DT_score
87/86:
def train_linear_regression(x_train, y_train):
    linear_regression = LinearRegression()
    linear_regression.fit(x_train,y_train)
    returnlinear_regression

LR_model = train_linear_regression(x_train, y_train)
LR_score = test_model(LR_model, x_test, y_test)
LR_score
87/87:
def train_linear_regression(x_train, y_train):
    linear_regression = LinearRegression()
    linear_regression.fit(x_train,y_train)
    return linear_regression

LR_model = train_linear_regression(x_train, y_train)
LR_score = test_model(LR_model, x_test, y_test)
LR_score
87/88:
def train_extra_tree(x_train, y_train):
    extra_tree = ExtraTreesRegressor(n_estimators = 100)
    extra_tree.fit(x_train, y_train)
    return extra_tree


ET_model = train_extra_tree(x_train, y_train)
ET_score = test_model(ET_model, x_test, y_test)
ET_score
87/89:
df = original_df.copy()

def drop_unnecessary_columns(df):
    df.drop(columns=['dish_liked', 'cuisines', 'menu_item'], inplace=True)
    df.drop(columns=['name', 'url', 'phone', 'reviews_list', "address"], inplace=True)
    return df

df = drop_unnecessary_columns(df)
df.columns
88/1:
import pandas as pd
from geopy.geocoders import Nominatim
import numpy as np
import folium
from folium.plugins import HeatMap, FastMarkerCluster
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import  ExtraTreesRegressor
88/2:
def read_data():
    df = pd.read_csv("zomato.csv")
    return df

original_df = read_data()
original_df.head()
88/3:
df = original_df.copy()

def drop_unnecessary_columns(df):
    df.drop(columns=['dish_liked', 'cuisines', 'menu_item'], inplace=True)
    df.drop(columns=['name', 'url', 'phone', 'reviews_list', "address"], inplace=True)
    return df

df = drop_unnecessary_columns(df)
df.columns
88/4:
def remove_nan(df):
    df["rate"] = df["rate"].replace("NEW", np.nan)
    df["rate"] = df["rate"].replace("-", np.nan)
    df.dropna(how='any',inplace=True)
    return df

df = remove_nan(df)
df.isnull().sum()
88/5:
def rename_features(df):
    df.rename(columns={'approx_cost(for two people)': 'price', 'listed_in(city)': 'city_area',
                      'listed_in(type)': 'meal_type'}, inplace=True)
    return df


df = rename_features(df)
df.columns
88/6:
df.drop_duplicates(inplace=True)
df.info()
88/7:
def transform_rate(df):
    rates = []
    for _, row in df.iterrows():
        rate_in_float = float(row["rate"].split("/")[0])
        rates.append(rate_in_float)
    df["rate"] = rates
    return df


def transform_price(df):
    prices = []
    for _, row in df.iterrows():
        price_s = row["price"]
        price_s = price_s.replace(",", "")
        price = int(price_s)
        prices.append(price)
    df["price"] = prices
    return df

df = transform_rate(df)
df = transform_price(df)

df[["rate", "price"]].head()
88/8:



def get_geo_info_location(location):
    geo_info = geolocator.geocode(location)
    if geo_info is None:
        return np.nan, np.nan
    return geo_info.latitude, geo_info.longitude


def get_location_coordinate(df):
    all_location = df["location"].unique()
    coordinate = dict()
    for location in all_location:
        coordinate[location] = get_geo_info_location(location + ", Bangalore")
    return coordinate


geolocator=Nominatim(user_agent="app")   
lat_lon_location = get_location_coordinate(df)
lat_lon_location
88/9:



def get_geo_info_location(location):
    geo_info = geolocator.geocode(location)
    if geo_info is None:
        return np.nan, np.nan
    return geo_info.latitude, geo_info.longitude


def get_location_coordinate(df):
    all_location = df["location"].unique()
    coordinate = dict()
    for location in all_location:
        coordinate[location] = get_geo_info_location(location + ", Bangalore")
    return coordinate


geolocator=Nominatim(user_agent="app")   
lat_lon_location = get_location_coordinate(df)
lat_lon_location
88/10:
def Encode(df):
    for column in df.columns[~df.columns.isin(['rate', 'price', 'votes'])]:
        df[column] = df[column].factorize()[0]
    return df

df_float = Encode(df.copy())
df_float.head()
88/11:
def get_train_test_data(df_float):
    x = df_float.iloc[:,[0, 1, 4, 5, 6, 7]]
    y = df_float.iloc[:,[2, 3]]
    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.1, random_state=353)
    return x_train, x_test, y_train, y_test
    
x_train, x_test, y_train, y_test = get_train_test_data(df_float)
x_train.head()
88/12:
def test_model(model, x_test, y_test):
    y_predict=model.predict(x_test)
    score = r2_score(y_test, y_predict)
    return score
88/13:
def train_random_forest(x_train, y_train):
    random_forest = RandomForestRegressor(n_estimators=500, random_state=329, min_samples_leaf=.0001)
    random_forest.fit(x_train, y_train)
    return random_forest


RF_model = train_random_forest(x_train, y_train)
RF_score = test_model(RF_model, x_test, y_test)
RF_score
88/14:
def train_random_forest(x_train, y_train):
    random_forest = RandomForestRegressor(n_estimators=500, random_state=329, min_samples_leaf=.0001)
    random_forest.fit(x_train, y_train)
    return random_forest


RF_model = train_random_forest(x_train, y_train)
RF_score = test_model(RF_model, x_test, y_test)
RF_score
88/15:
def train_random_forest(x_train, y_train):
    random_forest = RandomForestRegressor(n_estimators=500, random_state=329, min_samples_leaf=.0001)
    random_forest.fit(x_train, y_train)
    return random_forest


RF_model = train_random_forest(x_train, y_train)
RF_score = test_model(RF_model, x_test, y_test)
RF_score
88/16:
def train_decision_tree(x_train, y_train):
    decision_tree = DecisionTreeRegressor()
    decision_tree.fit(x_train, y_train)
    return decision_tree


DT_model = train_decision_tree(x_train, y_train)
DT_score = test_model(DT_model, x_test, y_test)
DT_score
88/17:
def train_linear_regression(x_train, y_train):
    linear_regression = LinearRegression()
    linear_regression.fit(x_train, y_train)
    return linear_regression


LR_model = train_linear_regression(x_train, y_train)
LR_score = test_model(LR_model, x_test, y_test)
LR_score
88/18:
def train_extra_tree(x_train, y_train):
    extra_tree = ExtraTreesRegressor(n_estimators = 100)
    extra_tree.fit(x_train, y_train)
    return extra_tree


ET_model = train_extra_tree(x_train, y_train)
ET_score = test_model(ET_model, x_test, y_test)
ET_score
88/19:
def train_random_forest(x_train, y_train):
    random_forest = RandomForestRegressor(n_estimators=500, random_state=329, min_samples_leaf=.0001)
    random_forest.fit(x_train, y_train)
    return random_forest
88/20:
def train_decision_tree(x_train, y_train):
    decision_tree = DecisionTreeRegressor()
    decision_tree.fit(x_train, y_train)
    return decision_tree
88/21:
def train_linear_regression(x_train, y_train):
    linear_regression = LinearRegression()
    linear_regression.fit(x_train, y_train)
    return linear_regression
88/22:
def train_extra_tree(x_train, y_train):
    extra_tree = ExtraTreesRegressor(n_estimators = 100)
    extra_tree.fit(x_train, y_train)
    return extra_tree
88/23: RF_model.score(x_test, y_test)
88/24:
RF_model.score(x_test, y_test)
RF_score
88/25: RF_model.score(x_test, y_test)
88/26:
[RF_model.score(x_test, y_test),
DT_model.score(x_test, y_test),
LR_model.score(x_test, y_test),
ET_model.score(x_test, y_test)]
88/27:
[RF_score = test_model(RF_model, x_test, y_test),
DT_score = test_model(DT_model, x_test, y_test),
LR_score = test_model(LR_model, x_test, y_test),
ET_score = test_model(ET_model, x_test, y_test)]
88/28:
RF_score = test_model(RF_model, x_test, y_test)
DT_score = test_model(DT_model, x_test, y_test)
LR_score = test_model(LR_model, x_test, y_test)
ET_score = test_model(ET_model, x_test, y_test)

[RF]
88/29:
RF_score = test_model(RF_model, x_test, y_test)
DT_score = test_model(DT_model, x_test, y_test)
LR_score = test_model(LR_model, x_test, y_test)
ET_score = test_model(ET_model, x_test, y_test)

[RF_score, DT_score, LR_score, ET_score]
88/30:
def test_model(model, x_test, y_test):
    y_predict=model.predict(x_test)
    score = r2_score(y_predict, y_test)
    return score
88/31:
RF_score = test_model(RF_model, x_test, y_test)
DT_score = test_model(DT_model, x_test, y_test)
LR_score = test_model(LR_model, x_test, y_test)
ET_score = test_model(ET_model, x_test, y_test)

[RF_score, DT_score, LR_score, ET_score]
88/32:
def test_model(model, x_test, y_test):
    y_predict=model.predict(x_test)
    score = r2_score(y_test, y_predict)
    return score
88/33:
RF_score = test_model(RF_model, x_test, y_test)
DT_score = test_model(DT_model, x_test, y_test)
LR_score = test_model(LR_model, x_test, y_test)
ET_score = test_model(ET_model, x_test, y_test)

[RF_score, DT_score, LR_score, ET_score]
88/34:
from sklearn.ensemble import GradientBoostingRegressor

gbr = GradientBoostingRegressor()
gbr.fit(x_train, y_train)

test_model(gbr, x_test, y_test)
88/35:
from sklearn.linear_model import Lasso

lasso = Lasso(alpha = lassocv.alpha_)
lasso.fit(x_train, y_train)

r2_score(y_test,lasso.predict(x_test))
88/36:
from sklearn.linear_model import Lasso

lasso = Lasso()
lasso.fit(x_train, y_train)

r2_score(y_test,lasso.predict(x_test))
88/37:
from sklearn.linear_model import Lasso

lasso = Lasso(alphas = None, cv = 10, max_iter = 10000, normalize = True)
lasso.fit(x_train, y_train)

r2_score(y_test,lasso.predict(x_test))
88/38:
from sklearn.linear_model import Lasso

lasso = Lasso(alpha = None, cv = 10, max_iter = 10000, normalize = True)
lasso.fit(x_train, y_train)

r2_score(y_test,lasso.predict(x_test))
88/39:
from sklearn.linear_model import Lasso

lasso = Lasso(alpha = None)
lasso.fit(x_train, y_train)

r2_score(y_test,lasso.predict(x_test))
88/40:
from sklearn.linear_model import Lasso

lasso = Lasso()
lasso.fit(x_train, y_train)

r2_score(y_test,lasso.predict(x_test))
88/41:
RF_score = test_model(RF_model, x_test, y_test)
DT_score = test_model(DT_model, x_test, y_test)
LR_score = test_model(LR_model, x_test, y_test)
ET_score = test_model(ET_model, x_test, y_test)


evaluation = pd.Dataframe("")

[RF_score, DT_score, LR_score, ET_score]
88/42:
RF_score = test_model(RF_model, x_test, y_test)
DT_score = test_model(DT_model, x_test, y_test)
LR_score = test_model(LR_model, x_test, y_test)
ET_score = test_model(ET_model, x_test, y_test)


evaluation = pd.DataFrame({"Model": ["Random Forest", "Decision Tree", "Linear Regression", "Extra Trees"], 
                           "R^2 Score": [RF_score, DT_score, LR_score, ET_score]})

evaluation
88/43:
RF_score = test_model(RF_model, x_test, y_test)
DT_score = test_model(DT_model, x_test, y_test)
LR_score = test_model(LR_model, x_test, y_test)
ET_score = test_model(ET_model, x_test, y_test)


evaluation = pd.DataFrame({"Model": ["Random Forest", "Decision Tree", "Linear Regression", "Extra Trees"], 
                           "R^2 Score": [RF_score, DT_score, LR_score, ET_score]})

evaluation.sort_values(by = 'Score', ascending = False)
88/44:
RF_score = test_model(RF_model, x_test, y_test)
DT_score = test_model(DT_model, x_test, y_test)
LR_score = test_model(LR_model, x_test, y_test)
ET_score = test_model(ET_model, x_test, y_test)


evaluation = pd.DataFrame({"Model": ["Random Forest", "Decision Tree", "Linear Regression", "Extra Trees"], 
                           "R^2 Score": [RF_score, DT_score, LR_score, ET_score]})

evaluation.sort_values(by = 'R^2 Score', ascending = False)
88/45:
RF_score = test_model(RF_model, x_test, y_test)
DT_score = test_model(DT_model, x_test, y_test)
LR_score = test_model(LR_model, x_test, y_test)
ET_score = test_model(ET_model, x_test, y_test)


evaluation = pd.DataFrame({"Model": ["Random Forest", "Decision Tree", "Linear Regression", "Extra Trees"], 
                           "R^2 Score": [RF_score, DT_score, LR_score, ET_score]})

evaluation.sort_values(by = 'R^2 Score', ascending = False, reindex=True)
88/46:
RF_score = test_model(RF_model, x_test, y_test)
DT_score = test_model(DT_model, x_test, y_test)
LR_score = test_model(LR_model, x_test, y_test)
ET_score = test_model(ET_model, x_test, y_test)


evaluation = pd.DataFrame({"Model": ["Random Forest", "Decision Tree", "Linear Regression", "Extra Trees"], 
                           "R^2 Score": [RF_score, DT_score, LR_score, ET_score]})

evaluation.sort_values(by = 'R^2 Score', ascending = False, ignore_index=True)
88/47: df["location"].unique()
88/48: len(df["location"].unique())
88/49: len(df["location"].isna())
88/50: len(df["location"].isna().unique())
88/51: len(df["location"].unique().isna())
88/52: df["location"].isna()
88/53:
def read_data():
    df = pd.read_csv("zomato.csv")
    return df

original_df = read_data()
original_df.head()
88/54: original_df.dropna(how='any',inplace=True)
88/55: len(original_df["location"].unique())
88/56:
def read_data():
    df = pd.read_csv("zomato.csv")
    return df

original_df = read_data()
original_df.head()
88/57: len(original_df["location"].unique())
88/58: len(original_df["rest_type"].unique())
88/59: len(original_df["rest_type"].unique())
88/60: original_df["rest_type"].unique()
88/61: len(original_df["location"].unique())
88/62: original_df["rest_type"].unique()
88/63: original_df["location"].unique()
88/64: original_df["dish_liked"].iloc(0)
88/65: original_df.iloc(0)["dish_liked"]
88/66:
a = original_df.iloc(0)
a["dish_liked"]
88/67:
a = original_df.iloc(0)
a
88/68:
a = original_df.iloc(0)
a.value
88/69:
i, a = original_df.iloc(0)
a
88/70: original_df.iloc[0]["dish_liked"]
88/71: original_df.iloc[0]["cuisines"]
88/72: original_df.iloc[0]["reviews_list"]
88/73: type(original_df.iloc[0]["reviews_list"])
88/74: original_df["approx_cost(for two people)"].max()
88/75: original_df["approx_cost(for two people)"].apply(int).max()
88/76: df["price"].max()
88/77: df["price"].min()
88/78: len(original_df["listed_in(type)"].unique())
88/79: len(original_df["listed_in(city)"].unique())
88/80:
def plot_rate_votes():
    plt.plot(df["rate"], df["votes"], "ob")
    plt.show()
    
plot_rate_votes()
88/81:
def plot_rate_votes():
    plt.plot(df["rate"], df["votes"], "ob")
    plt.show()
88/82:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from folium import Map
from folium.plugins import HeatMap, FastMarkerCluster
from geopy.geocoders import Nominatim
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import  ExtraTreesRegressor
88/83:
def heatmap_generate(lat_lon_location, location_info, need_clustering=False):
    map_info = []
    for location in location_info:
        if not np.isnan(lat_lon_location[location][0]):
            map_info.append(list(lat_lon_location[location]) + [float(location_info[location])])
    base_map = Map(location=[12.9716, 77.5946], zoom_start=11)
    HeatMap(map_info ,zoom=20,radius=15).add_to(base_map)
    if need_clustering:
        FastMarkerCluster(map_info).add_to(base_map)
    return base_map
88/84:
def generate_count_heatmap():
    location_count = dict(df['location'].value_counts())
    heatmap_count = heatmap_generate(lat_lon_location, location_count)
    return heatmap_count
88/85:
def generate_count_heatmap():
    location_count = dict(df['location'].value_counts())
    heatmap_count = heatmap_generate(lat_lon_location, location_count)
    return heatmap_count

generate_count_heatmap()
88/86:
def generate_count_heatmap():
    location_count = dict(df['location'].value_counts())
    heatmap_count = heatmap_generate(lat_lon_location, location_count)
    return heatmap_count
88/87:
def get_train_test_data(df_float):
    x = df_float.iloc[:,[0, 1, 4, 5, 6, 7]]
    y = df_float.iloc[:,[2, 3]]
    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.1, random_state=100)
    return x_train, x_test, y_train, y_test


x_train, x_test, y_train, y_test = get_train_test_data(df_float)
x_train.head()
88/88:
def train_random_forest(x_train, y_train):
    random_forest = RandomForestRegressor(n_estimators=500, random_state=329, min_samples_leaf=.0001)
    random_forest.fit(x_train, y_train)
    return random_forest


RF_model = train_random_forest(x_train, y_train)
88/89:
def train_decision_tree(x_train, y_train):
    decision_tree = DecisionTreeRegressor()
    decision_tree.fit(x_train, y_train)
    return decision_tree


DT_model = train_decision_tree(x_train, y_train)
88/90:
def train_linear_regression(x_train, y_train):
    linear_regression = LinearRegression()
    linear_regression.fit(x_train, y_train)
    return linear_regression


LR_model = train_linear_regression(x_train, y_train)
88/91:
def train_extra_tree(x_train, y_train):
    extra_tree = ExtraTreesRegressor(n_estimators = 100)
    extra_tree.fit(x_train, y_train)
    return extra_tree


ET_model = train_extra_tree(x_train, y_train)
88/92:
RF_score = test_model(RF_model, x_test, y_test)
DT_score = test_model(DT_model, x_test, y_test)
LR_score = test_model(LR_model, x_test, y_test)
ET_score = test_model(ET_model, x_test, y_test)


evaluation = pd.DataFrame({"Model": ["Random Forest", "Decision Tree", "Linear Regression", "Extra Trees"], 
                           "R^2 Score": [RF_score, DT_score, LR_score, ET_score]})

evaluation.sort_values(by = 'R^2 Score', ascending = False, ignore_index=True)
88/93:
def train_random_forest(x_train, y_train):
    random_forest = RandomForestRegressor(n_estimators=500, min_samples_leaf=.0001)
    random_forest.fit(x_train, y_train)
    return random_forest


RF_model = train_random_forest(x_train, y_train)
88/94:
RF_score = test_model(RF_model, x_test, y_test)
DT_score = test_model(DT_model, x_test, y_test)
LR_score = test_model(LR_model, x_test, y_test)
ET_score = test_model(ET_model, x_test, y_test)


evaluation = pd.DataFrame({"Model": ["Random Forest", "Decision Tree", "Linear Regression", "Extra Trees"], 
                           "R^2 Score": [RF_score, DT_score, LR_score, ET_score]})

evaluation.sort_values(by = 'R^2 Score', ascending = False, ignore_index=True)
88/95:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from folium import Map
from folium.plugins import HeatMap, FastMarkerCluster
from geopy.geocoders import Nominatim
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import  ExtraTreesRegressor
from sklearn.cluster import KMeans
88/96:
kmean = KMeans(n_clusters=2, random_state=0).git(df[["rate", "votes"]])
kmean.labels_
88/97:
kmean = KMeans(n_clusters=2, random_state=0).fit(df[["rate", "votes"]])
kmean.labels_
88/98:
kmean = KMeans(n_clusters=2, random_state=0).fit(df[["rate", "votes"]])
kmean.labels_.sum()
88/99:
kmean = KMeans(n_clusters=3, random_state=0).fit(df[["rate", "votes"]])
kmean.labels_.sum()
88/100:
kmean = KMeans(n_clusters=4, random_state=0).fit(df[["rate", "votes"]])
kmean.labels_
88/101:
kmean = KMeans(n_clusters=4, random_state=0).fit(df[["rate", "votes"]])
kmean.labels_.sum()
88/102:
kmean = KMeans(n_clusters=4, random_state=0).fit(df[["rate", "votes"]])
df_copy = df.copy()
df_copy["clustering"] = kmean.labels_

df_copy
88/103:
def plot_rate_votes_cluster(n_cluster=3):
    df_cluster = df.copy()
    df_cluster["cluster"] = kmean.labels_
    kmean = KMeans(n_clusters=n_cluster, random_state=0).fit(df_cluster[["rate", "votes"]])
    colors = ["g", "r", "b", "c", "m", "y"]
    for i in range(n_cluster):
        selected = df_cluster[df_cluster["cluster"] == i]
        plt.plot(selected["rate"], selected["votes"], "ob", color=colors[i])
    plt.show()
    
    
plot_rate_votes_cluster()
88/104:
def plot_rate_votes_cluster(n_cluster=3):
    df_cluster = df.copy()
    kmean = KMeans(n_clusters=n_cluster, random_state=0).fit(df_cluster[["rate", "votes"]])
    df_cluster["cluster"] = kmean.labels_
    colors = ["g", "r", "b", "c", "m", "y"]
    for i in range(n_cluster):
        selected = df_cluster[df_cluster["cluster"] == i]
        plt.plot(selected["rate"], selected["votes"], "ob", color=colors[i])
    plt.show()
    
    
plot_rate_votes_cluster()
88/105:
def plot_rate_votes_cluster(n_cluster=3):
    df_cluster = df.copy()
    kmean = KMeans(n_clusters=n_cluster, random_state=0).fit(df_cluster[["rate", "votes"]])
    df_cluster["cluster"] = kmean.labels_
    colors = ["g", "r", "b", "c", "m", "y"]
    for i in range(n_cluster):
        selected = df_cluster[df_cluster["cluster"] == i]
        plt.plot(selected["rate"], selected["votes"], "ob", color=colors[i])
    plt.show()
    
    
plot_rate_votes_cluster(4)
88/106:
def plot_rate_votes_cluster(n_cluster=3):
    df_cluster = df.copy()
    kmean = KMeans(n_clusters=n_cluster, random_state=0).fit(df_cluster[["rate", "votes"]])
    df_cluster["cluster"] = kmean.labels_
    colors = ["g", "r", "b", "c", "m", "y"]
    for i in range(n_cluster):
        selected = df_cluster[df_cluster["cluster"] == i]
        plt.plot(selected["rate"], selected["votes"], "ob", color=colors[i])
    plt.show()
    
    
plot_rate_votes_cluster(5)
88/107:
def plot_rate_votes_cluster(n_cluster=3):
    df_cluster = df.copy()
    kmean = KMeans(n_clusters=n_cluster, random_state=0).fit(df_cluster[["rate", "votes"]])
    df_cluster["cluster"] = kmean.labels_
    colors = ["g", "r", "b", "c", "m", "y"]
    for i in range(n_cluster):
        selected = df_cluster[df_cluster["cluster"] == i]
        plt.plot(selected["rate"], selected["votes"], "ob", color=colors[i])
    plt.show()
    
    
plot_rate_votes_cluster(6)
88/108:
def plot_rate_votes_cluster(n_cluster=3):
    df_cluster = df.copy()
    kmean = KMeans(n_clusters=n_cluster, random_state=100).fit(df_cluster[["rate", "votes"]])
    df_cluster["cluster"] = kmean.labels_
    colors = ["g", "r", "b", "c", "m", "y"]
    for i in range(n_cluster):
        selected = df_cluster[df_cluster["cluster"] == i]
        plt.plot(selected["rate"], selected["votes"], "ob", color=colors[i])
    plt.show()
    
    
plot_rate_votes_cluster(6)
88/109:
def plot_rate_votes_cluster(n_cluster=3):
    df_cluster = df.copy()
    kmean = KMeans(n_clusters=n_cluster, random_state=100).fit(df_cluster[["rate", "votes"]])
    df_cluster["cluster"] = kmean.labels_
    colors = ["g", "r", "b", "c", "m", "y"]
    for i in range(n_cluster):
        selected = df_cluster[df_cluster["cluster"] == i]
        plt.plot(selected["rate"], selected["votes"], "ob", color=colors[i])
    plt.show()
    
    
plot_rate_votes_cluster(4)
88/110:
def plot_rate_votes_cluster(n_cluster=3):
    df_cluster = df.copy()
    kmean = KMeans(n_clusters=n_cluster, random_state=100).fit(df_cluster[["rate", "votes"]])
    df_cluster["cluster"] = kmean.labels_
    colors = ["g", "r", "b", "c", "m", "y"]
    for i in range(n_cluster):
        selected = df_cluster[df_cluster["cluster"] == i]
        plt.plot(selected["rate"], selected["votes"], "ob", color=colors[i])
    plt.show()
    
    
plot_rate_votes_cluster(5)
88/111:
def plot_rate_price_cluster(n_cluster=3):
    df_cluster = df.copy()
    kmean = KMeans(n_clusters=n_cluster, random_state=100).fit(df_cluster[["rate", "price"]])
    df_cluster["cluster"] = kmean.labels_
    colors = ["g", "r", "b", "c", "m", "y"]
    for i in range(n_cluster):
        selected = df_cluster[df_cluster["cluster"] == i]
        plt.plot(selected["rate"], selected["price"], "ob", color=colors[i])
    plt.show()
    
    
plot_rate_votes_cluster(5)
88/112:
def plot_rate_price_cluster(n_cluster=3):
    df_cluster = df.copy()
    kmean = KMeans(n_clusters=n_cluster, random_state=100).fit(df_cluster[["rate", "price"]])
    df_cluster["cluster"] = kmean.labels_
    colors = ["g", "r", "b", "c", "m", "y"]
    for i in range(n_cluster):
        selected = df_cluster[df_cluster["cluster"] == i]
        plt.plot(selected["rate"], selected["price"], "ob", color=colors[i])
    plt.show()
    
    
plot_rate_price_cluster(5)
88/113:
def plot_rate_price_cluster(n_cluster=3):
    df_cluster = df.copy()
    kmean = KMeans(n_clusters=n_cluster, random_state=100).fit(df_cluster[["rate", "price"]])
    df_cluster["cluster"] = kmean.labels_
    colors = ["g", "r", "b", "c", "m", "y"]
    for i in range(n_cluster):
        selected = df_cluster[df_cluster["cluster"] == i]
        plt.plot(selected["rate"], selected["price"], "ob", color=colors[i])
    plt.show()
    
    
plot_rate_price_cluster(4)
88/114:
def plot_rate_dist():
    fig, ax = plt.subplots(figsize=[10,4])
    sns.distplot(df["rate"],ax=ax)
    
plot_rate_dist()
88/115:
def plot_rate_dist():
    fig, ax = plt.subplots(figsize=[10,4])
    sns.distplot(df["rate"],ax=ax)
88/116:
def plot_rate_price_cluster(n_cluster=3):
    df_cluster = df.copy()
    kmean = KMeans(n_clusters=n_cluster, random_state=100).fit(df_cluster[["rate", "price"]])
    df_cluster["cluster"] = kmean.labels_
    colors = ["g", "r", "b", "c", "m", "y"]
    for i in range(n_cluster):
        selected = df_cluster[df_cluster["cluster"] == i]
        plt.plot(selected["rate"], selected["price"], "ob", color=colors[i])
    plt.show()
88/117:
def plot_rate_votes_cluster(n_cluster=3):
    df_cluster = df.copy()
    kmean = KMeans(n_clusters=n_cluster, random_state=100).fit(df_cluster[["rate", "votes"]])
    df_cluster["cluster"] = kmean.labels_
    colors = ["g", "r", "b", "c", "m", "y"]
    for i in range(n_cluster):
        selected = df_cluster[df_cluster["cluster"] == i]
        plt.plot(selected["rate"], selected["votes"], "ob", color=colors[i])
    plt.show()
    
    
plot_rate_votes_cluster(5)
88/118:

    
def plot_rate_price():
    fig, ax = plt.subplots(figsize=[10,4])
    sns.distplot(df["rate", "price"],ax=ax)
    

plot_rate_price()
88/119:
def plot_rate_price_cluster(n_cluster=3):
    df_cluster = df.copy()
    kmean = KMeans(n_clusters=n_cluster, random_state=100).fit(df_cluster[["rate", "price"]])
    df_cluster["cluster"] = kmean.labels_
    colors = ["g", "r", "b", "c", "m", "y"]
    for i in range(n_cluster):
        selected = df_cluster[df_cluster["cluster"] == i]
        plt.plot(selected["rate"], selected["price"], "ob", color=colors[i])
    plt.show()
    
    
plot_rate_price_cluster()
88/120:
def plot_rate_price_cluster(n_cluster=3):
    df_cluster = df.copy()
    kmean = KMeans(n_clusters=n_cluster, random_state=100).fit(df_cluster[["rate", "price"]])
    df_cluster["cluster"] = kmean.labels_
    colors = ["g", "r", "b", "c", "m", "y"]
    for i in range(n_cluster):
        selected = df_cluster[df_cluster["cluster"] == i]
        plt.plot(selected["rate"], selected["price"], "ob", color=colors[i])
    plt.show()
    
    
plot_rate_price_cluster(6)
88/121:
def plot_rate_price_cluster(n_cluster=3):
    df_cluster = df.copy()
    kmean = KMeans(n_clusters=n_cluster, random_state=100).fit(df_cluster[["rate", "price"]])
    df_cluster["cluster"] = kmean.labels_
    colors = ["g", "r", "b", "c", "m", "y"]
    for i in range(n_cluster):
        selected = df_cluster[df_cluster["cluster"] == i]
        plt.plot(selected["rate"], selected["price"], "ob", color=colors[i])
    plt.show()
    
    
plot_rate_price_cluster(3)
88/122:
def plot_rate_price_cluster(n_cluster=3):
    df_cluster = df.copy()
    kmean = KMeans(n_clusters=n_cluster, random_state=100).fit(df_cluster[["rate", "price"]])
    df_cluster["cluster"] = kmean.labels_
    colors = ["g", "r", "b", "c", "m", "y"]
    for i in range(n_cluster):
        selected = df_cluster[df_cluster["cluster"] == i]
        plt.plot(selected["rate"], selected["price"], "ob", color=colors[i])
    plt.show()
    
    
plot_rate_price_cluster(6)
88/123:
def plot_rate_price_cluster(n_cluster=3):
    df_cluster = df.copy()
    kmean = KMeans(n_clusters=n_cluster, random_state=100).fit(df_cluster[["rate", "votes"]])
    df_cluster["cluster"] = kmean.labels_
    colors = ["g", "r", "b", "c", "m", "y"]
    for i in range(n_cluster):
        selected = df_cluster[df_cluster["cluster"] == i]
        plt.plot(selected["rate"], selected["price"], "ob", color=colors[i])
    plt.show()
    
    
plot_rate_price_cluster(6)
88/124:
def plot_rate_price_cluster(n_cluster=3):
    df_cluster = df.copy()
    kmean = KMeans(n_clusters=n_cluster, random_state=100).fit(df_cluster[["rate", "votes"]])
    df_cluster["cluster"] = kmean.labels_
    colors = ["g", "r", "b", "c", "m", "y"]
    for i in range(n_cluster):
        selected = df_cluster[df_cluster["cluster"] == i]
        plt.plot(selected["rate"], selected["price"], "ob", color=colors[i])
    plt.show()
    
    
plot_rate_price_cluster(4)
88/125:
def plot_rate_price_cluster(n_cluster=3):
    df_cluster = df.copy()
    kmean = KMeans(n_clusters=n_cluster, random_state=100).fit(df_cluster[["rate", "votes"]])
    df_cluster["cluster"] = kmean.labels_
    colors = ["g", "r", "b", "c", "m", "y"]
    for i in range(n_cluster):
        selected = df_cluster[df_cluster["cluster"] == i]
        plt.plot(selected["rate"], selected["price"], "ob", color=colors[i])
    plt.show()
    
    
plot_rate_price_cluster(5)
88/126:
def plot_rate_votes_cluster(n_cluster=5):
    df_cluster = df.copy()
    kmean = KMeans(n_clusters=n_cluster, random_state=100).fit(df_cluster[["rate", "price"]])
    df_cluster["cluster"] = kmean.labels_
    colors = ["g", "r", "b", "c", "m", "y"]
    for i in range(n_cluster):
        selected = df_cluster[df_cluster["cluster"] == i]
        plt.plot(selected["rate"], selected["votes"], "ob", color=colors[i])
    plt.show()
88/127:
def plot_rate_votes_cluster(n_cluster=5):
    df_cluster = df.copy()
    kmean = KMeans(n_clusters=n_cluster, random_state=100).fit(df_cluster[["rate", "price"]])
    df_cluster["cluster"] = kmean.labels_
    colors = ["g", "r", "b", "c", "m", "y"]
    for i in range(n_cluster):
        selected = df_cluster[df_cluster["cluster"] == i]
        plt.plot(selected["rate"], selected["votes"], "ob", color=colors[i])
    plt.show()
    
plot_rate_votes_cluster()
88/128:
def plot_rate_votes_cluster(n_cluster=5):
    df_cluster = df.copy()
    kmean = KMeans(n_clusters=n_cluster, random_state=100).fit(df_cluster[["rate", "price"]])
    df_cluster["cluster"] = kmean.labels_
    colors = ["g", "r", "b", "c", "m", "y"]
    for i in range(n_cluster):
        selected = df_cluster[df_cluster["cluster"] == i]
        plt.plot(selected["rate"], selected["votes"], "ob", color=colors[i])
    plt.show()
    
plot_rate_votes_cluster(3)
88/129:
def plot_rate_votes_cluster(n_cluster=5):
    df_cluster = df.copy()
    kmean = KMeans(n_clusters=n_cluster, random_state=100).fit(df_cluster[["price"]])
    df_cluster["cluster"] = kmean.labels_
    colors = ["g", "r", "b", "c", "m", "y"]
    for i in range(n_cluster):
        selected = df_cluster[df_cluster["cluster"] == i]
        plt.plot(selected["rate"], selected["votes"], "ob", color=colors[i])
    plt.show()
    
plot_rate_votes_cluster(3)
88/130:
def plot_rate_votes_cluster(n_cluster=5):
    df_cluster = df.copy()
    kmean = KMeans(n_clusters=n_cluster, random_state=100).fit(df_cluster[["price"]])
    df_cluster["cluster"] = kmean.labels_
    colors = ["g", "r", "b", "c", "m", "y"]
    for i in range(n_cluster):
        selected = df_cluster[df_cluster["cluster"] == i]
        plt.plot(selected["rate"], selected["votes"], "ob", color=colors[i])
    plt.show()
    
plot_rate_votes_cluster(6)
88/131:
def plot_rate_votes_cluster(n_cluster=5):
    df_cluster = df.copy()
    kmean = KMeans(n_clusters=n_cluster, random_state=100).fit(df_cluster[["rate", "votes"]])
    df_cluster["cluster"] = kmean.labels_
    colors = ["g", "r", "b", "c", "m", "y"]
    for i in range(n_cluster):
        selected = df_cluster[df_cluster["cluster"] == i]
        plt.plot(selected["rate"], selected["votes"], "ob", color=colors[i])
    plt.show()
    
plot_rate_votes_cluster(6)
88/132:
def plot_rate_votes_cluster(n_cluster=5):
    df_cluster = df.copy()
    kmean = KMeans(n_clusters=n_cluster, random_state=100).fit(df_cluster[["rate", "votes"]])
    df_cluster["cluster"] = kmean.labels_
    colors = ["g", "r", "b", "c", "m", "y"]
    for i in range(n_cluster):
        selected = df_cluster[df_cluster["cluster"] == i]
        plt.plot(selected["rate"], selected["votes"], "ob", color=colors[i])
    plt.show()
    
plot_rate_votes_cluster(5)
88/133:
def plot_rate_price_cluster(n_cluster=5):
    df_cluster = df.copy()
    kmean = KMeans(n_clusters=n_cluster, random_state=100).fit(df_cluster[["rate", "votes"]])
    df_cluster["cluster"] = kmean.labels_
    colors = ["g", "r", "b", "c", "m", "y"]
    for i in range(n_cluster):
        selected = df_cluster[df_cluster["cluster"] == i]
        plt.plot(selected["votes"], selected["price"], "ob", color=colors[i])
    plt.show()
    
    
plot_rate_price_cluster(5)
88/134:
def plot_rate_price_cluster(n_cluster=5):
    df_cluster = df.copy()
    kmean = KMeans(n_clusters=n_cluster, random_state=100).fit(df_cluster[["rate", "votes"]])
    df_cluster["cluster"] = kmean.labels_
    colors = ["g", "r", "b", "c", "m", "y"]
    for i in range(n_cluster):
        selected = df_cluster[df_cluster["cluster"] == i]
        plt.plot(selected["rate"], selected["price"], "ob", color=colors[i])
    plt.show()
    
    
plot_rate_price_cluster(5)
88/135:
def plot_rate_votes_cluster(n_cluster=5):
    df_cluster = df.copy()
    kmean = KMeans(n_clusters=n_cluster, random_state=100).fit(df_cluster[["rate", "votes"]])
    df_cluster["cluster"] = kmean.labels_
    colors = ["g", "r", "b", "c", "m", "y"]
    for i in range(n_cluster):
        selected = df_cluster[df_cluster["cluster"] == i]
        plt.plot(selected["rate"], selected["votes"], "ob", color=colors[i])
    plt.show()
    
plot_rate_votes_cluster(5)
88/136:
def plot_votes_price_cluster(n_cluster=5):
    df_cluster = df.copy()
    kmean = KMeans(n_clusters=n_cluster, random_state=100).fit(df_cluster[["rate", "votes"]])
    df_cluster["cluster"] = kmean.labels_
    colors = ["g", "r", "b", "c", "m", "y"]
    for i in range(n_cluster):
        selected = df_cluster[df_cluster["cluster"] == i]
        plt.plot(selected["votes"], selected["price"], "ob", color=colors[i])
    plt.show()
    
    
plot_rate_price_cluster(5)
88/137:
def plot_votes_price_cluster(n_cluster=5):
    df_cluster = df.copy()
    kmean = KMeans(n_clusters=n_cluster, random_state=100).fit(df_cluster[["rate", "votes"]])
    df_cluster["cluster"] = kmean.labels_
    colors = ["g", "r", "b", "c", "m", "y"]
    for i in range(n_cluster):
        selected = df_cluster[df_cluster["cluster"] == i]
        plt.plot(selected["votes"], selected["price"], "ob", color=colors[i])
    plt.show()
    
    
plot_votes_price_cluster(5)
88/138:
def success_cluster(n_cluster=5):
    df_cluster = df.copy()
    kmean = KMeans(n_clusters=n_cluster, random_state=100).fit(df_cluster[["rate", "votes"]])
    df_cluster["cluster"] = kmean.labels_
    return df_cluster
88/139:
def plot_rate_votes_cluster(n_cluster=5):
    df_cluster = success_cluster(n_cluster)
    colors = ["g", "r", "b", "c", "m", "y"]
    for i in range(n_cluster):
        selected = df_cluster[df_cluster["cluster"] == i]
        plt.plot(selected["rate"], selected["votes"], "ob", color=colors[i])
    plt.show()
    
plot_rate_votes_cluster(5)
88/140:
def plot_rate_price_cluster(n_cluster=5):
    df_cluster = success_cluster(n_cluster)
    colors = ["g", "r", "b", "c", "m", "y"]
    for i in range(n_cluster):
        selected = df_cluster[df_cluster["cluster"] == i]
        plt.plot(selected["rate"], selected["price"], "ob", color=colors[i])
    plt.show()
    
    
plot_rate_price_cluster(5)
88/141:
def plot_votes_price_cluster(n_cluster=5):
    df_cluster = success_cluster(n_cluster)
    colors = ["g", "r", "b", "c", "m", "y"]
    for i in range(n_cluster):
        selected = df_cluster[df_cluster["cluster"] == i]
        plt.plot(selected["votes"], selected["price"], "ob", color=colors[i])
    plt.show()
    
    
plot_votes_price_cluster(5)
88/142:
def plot_rate_price_cluster(n_cluster=5):
    df_cluster = success_cluster(n_cluster)
    colors = ["g", "r", "b", "c", "m", "y"]
    for i in range(n_cluster):
        selected = df_cluster[df_cluster["cluster"] == i]
        plt.plot(selected["rate"], selected["price"], "ob", color=colors[i])
    plt.figure(figsize = (10))
    plt.show()
    
    
plot_rate_price_cluster(5)
88/143:
def plot_rate_price_cluster(n_cluster=5):
    df_cluster = success_cluster(n_cluster)
    colors = ["g", "r", "b", "c", "m", "y"]
    for i in range(n_cluster):
        selected = df_cluster[df_cluster["cluster"] == i]
        plt.plot(selected["rate"], selected["price"], "ob", color=colors[i])
    plt.figure(figsize = 10)
    plt.show()
    
    
plot_rate_price_cluster(5)
88/144:
def plot_rate_price_cluster(n_cluster=5):
    df_cluster = success_cluster(n_cluster)
    colors = ["g", "r", "b", "c", "m", "y"]
    for i in range(n_cluster):
        selected = df_cluster[df_cluster["cluster"] == i]
        plt.plot(selected["rate"], selected["price"], "ob", color=colors[i])
    plt.figure(figsize = (10,10))
    plt.show()
    
    
plot_rate_price_cluster(5)
88/145:
def plot_rate_price_cluster(n_cluster=5):
    df_cluster = success_cluster(n_cluster)
    colors = ["g", "r", "b", "c", "m", "y"]
    for i in range(n_cluster):
        selected = df_cluster[df_cluster["cluster"] == i]
        plt.plot(selected["rate"], selected["price"], "ob", color=colors[i])
    plt.figure(figsize = (100,100))
    plt.show()
    
    
plot_rate_price_cluster(5)
88/146:
def plot_rate_price_cluster(n_cluster=5):
    df_cluster = success_cluster(n_cluster)
    colors = ["g", "r", "b", "c", "m", "y"]
    plt.figure(figsize = (10,100))
    for i in range(n_cluster):
        selected = df_cluster[df_cluster["cluster"] == i]
        plt.plot(selected["rate"], selected["price"], "ob", color=colors[i])
    plt.show()
    
    
plot_rate_price_cluster(5)
88/147:
def plot_rate_price_cluster(n_cluster=5):
    df_cluster = success_cluster(n_cluster)
    colors = ["g", "r", "b", "c", "m", "y"]
    plt.figure(figsize = (10,10))
    for i in range(n_cluster):
        selected = df_cluster[df_cluster["cluster"] == i]
        plt.plot(selected["rate"], selected["price"], "ob", color=colors[i])
    plt.show()
    
    
plot_rate_price_cluster(5)
88/148:
def plot_rate_votes_cluster(n_cluster=5):
    df_cluster = success_cluster(n_cluster)
    colors = ["g", "r", "b", "c", "m", "y"]
    
    plt.figure(figsize = (4,6))
    for i in range(n_cluster):
        selected = df_cluster[df_cluster["cluster"] == i]
        plt.plot(selected["rate"], selected["votes"], "ob", color=colors[i])
    plt.show()
    
plot_rate_votes_cluster(5)
88/149:
def plot_rate_votes_cluster(n_cluster=5):
    df_cluster = success_cluster(n_cluster)
    colors = ["g", "r", "b", "c", "m", "y"]
    
    plt.figure(figsize = (6,4))
    
    for i in range(n_cluster):
        selected = df_cluster[df_cluster["cluster"] == i]
        plt.plot(selected["rate"], selected["votes"], "ob", color=colors[i])
    plt.show()
    
plot_rate_votes_cluster(5)
88/150:
def plot_rate_votes_cluster(n_cluster=5):
    df_cluster = success_cluster(n_cluster)
    colors = ["g", "r", "b", "c", "m", "y"]
    
    plt.figure(figsize = (12,8))
    
    for i in range(n_cluster):
        selected = df_cluster[df_cluster["cluster"] == i]
        plt.plot(selected["rate"], selected["votes"], "ob", color=colors[i])
    plt.show()
    
plot_rate_votes_cluster(5)
88/151:
def plot_rate_votes_cluster(n_cluster=5):
    df_cluster = success_cluster(n_cluster)
    colors = ["g", "r", "b", "c", "m", "y"]
    
    plt.figure(figsize = (8,8))
    
    for i in range(n_cluster):
        selected = df_cluster[df_cluster["cluster"] == i]
        plt.plot(selected["rate"], selected["votes"], "ob", color=colors[i])
    plt.show()
    
plot_rate_votes_cluster(5)
88/152:
def plot_rate_votes_cluster(n_cluster=5):
    df_cluster = success_cluster(n_cluster)
    colors = ["g", "r", "b", "c", "m", "y"]
    
    plt.figure(figsize = (8,6))
    
    for i in range(n_cluster):
        selected = df_cluster[df_cluster["cluster"] == i]
        plt.plot(selected["rate"], selected["votes"], "ob", color=colors[i])
    plt.show()
    
plot_rate_votes_cluster(5)
88/153:
def plot_rate_votes_cluster(n_cluster=5):
    df_cluster = success_cluster(n_cluster)
    colors = ["g", "r", "b", "c", "m", "y"]
    
    plt.figure(figsize = (8,5))
    
    for i in range(n_cluster):
        selected = df_cluster[df_cluster["cluster"] == i]
        plt.plot(selected["rate"], selected["votes"], "ob", color=colors[i])
    plt.show()
    
plot_rate_votes_cluster(5)
88/154:
def plot_rate_price_cluster(n_cluster=5):
    df_cluster = success_cluster(n_cluster)
    colors = ["g", "r", "b", "c", "m", "y"]
    plt.figure(figsize = (8,5))
    for i in range(n_cluster):
        selected = df_cluster[df_cluster["cluster"] == i]
        plt.plot(selected["rate"], selected["price"], "ob", color=colors[i])
    plt.show()
    
    
plot_rate_price_cluster(5)
88/155:
def plot_votes_price_cluster(n_cluster=5):
    df_cluster = success_cluster(n_cluster)
    colors = ["g", "r", "b", "c", "m", "y"]
    plt.figure(figsize = (8,5))
    for i in range(n_cluster):
        selected = df_cluster[df_cluster["cluster"] == i]
        plt.plot(selected["votes"], selected["price"], "ob", color=colors[i])
    plt.show()
    
    
plot_votes_price_cluster(5)
88/156:
def plot_votes_price_cluster(n_cluster=5):
    df_cluster = success_cluster(n_cluster)
    colors = ["g", "r", "b", "c", "m", "y"]
    plt.figure(figsize = (8,5))
    for i in range(n_cluster):
        selected = df_cluster[df_cluster["cluster"] == i]
        plt.plot(selected["price"], selected["votes"], "ob", color=colors[i])
    plt.show()
    
    
plot_votes_price_cluster(5)
88/157:
def plot_votes_price_cluster(n_cluster=5):
    df_cluster = success_cluster(n_cluster)
    colors = ["g", "r", "b", "c", "m", "y"]
    plt.figure(figsize = (8,5))
    for i in range(n_cluster):
        selected = df_cluster[df_cluster["cluster"] == i]
        plt.plot(selected["votes"], selected["price"], "ob", color=colors[i])
    plt.show()
    
    
plot_votes_price_cluster(5)
88/158:
def plot_votes_price_cluster(n_cluster=5):
    df_cluster = success_cluster(n_cluster)
    colors = ["g", "r", "b", "c", "m", "y"]
    plt.figure(figsize = (8,5))
    for i in range(n_cluster):
        selected = df_cluster[df_cluster["cluster"] == i]
        plt.plot(selected["votes"], selected["price"], "ob", color=colors[i])
    plt.show()
88/159:
def plot_rate_price_cluster(n_cluster=5):
    df_cluster = success_cluster(n_cluster)
    colors = ["g", "r", "b", "c", "m", "y"]
    plt.figure(figsize = (8,5))
    for i in range(n_cluster):
        selected = df_cluster[df_cluster["cluster"] == i]
        plt.plot(selected["rate"], selected["price"], "ob", color=colors[i])
    plt.show()
88/160:
def plot_rate_votes_cluster(n_cluster=5):
    df_cluster = success_cluster(n_cluster)
    colors = ["g", "r", "b", "c", "m", "y"]
    
    plt.figure(figsize = (8,5))
    
    for i in range(n_cluster):
        selected = df_cluster[df_cluster["cluster"] == i]
        plt.plot(selected["rate"], selected["votes"], "ob", color=colors[i])
    plt.show()
88/161:
def plot_votes_dist():
    fig, ax = plt.subplots(figsize=[10,4])
    sns.distplot(df["votes"],ax=ax)
    

plot_votes_dist()
88/162:
def plot_votes_dist():
    fig, ax = plt.subplots(figsize=[10,4])
    sns.distplot(df["votes"],ax=ax)
    

df["votes"].mean
88/163:
def plot_votes_dist():
    fig, ax = plt.subplots(figsize=[10,4])
    sns.distplot(df["votes"],ax=ax)
    

df["votes"].mean()
88/164:
def plot_votes_dist():
    fig, ax = plt.subplots(figsize=[10,4])
    sns.distplot(df["votes"],ax=ax)
    

df["votes"].median()
88/165:
def plot_votes_dist():
    fig, ax = plt.subplots(figsize=[10,4])
    sns.distplot(df["votes"],ax=ax)
    

df["votes"].median()
df["votes"].mean()
88/166:
def plot_votes_dist():
    fig, ax = plt.subplots(figsize=[10,4])
    sns.distplot(df["votes"],ax=ax)
    

df["votes"].median()
df["votes"].mean()
plot_votes_dist()
88/167:
def plot_votes_dist():
    fig, ax = plt.subplots(figsize=[12,4])
    sns.distplot(df["votes"],ax=ax)
    

df["votes"].median()
df["votes"].mean()
plot_votes_dist()
88/168: df.describe()
88/169:
def success_cluster(n_cluster=5):
    df_cluster = df.copy()
    kmean = KMeans(n_clusters=n_cluster, random_state=100).fit(df_cluster[["rate", "votes"]])
    df_cluster["cluster"] = kmean.labels_
    return df_cluster
88/170:
def plot_votes_dist():
    fig, ax = plt.subplots(figsize=[12,4])
    sns.distplot(df["votes"],ax=ax)
89/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from folium import Map
from folium.plugins import HeatMap, FastMarkerCluster
from geopy.geocoders import Nominatim
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import  ExtraTreesRegressor
from sklearn.cluster import KMeans
89/2:
def read_data():
    df = pd.read_csv("zomato.csv")
    return df


original_df = read_data()
original_df.head()
89/3:
df = original_df.copy()

def drop_unnecessary_columns(df):
    df.drop(columns=['dish_liked', 'cuisines', 'menu_item'], inplace=True)
    df.drop(columns=['name', 'url', 'phone', 'reviews_list', "address"], inplace=True)
    return df


df = drop_unnecessary_columns(df)
df.columns
89/4:
def remove_nan(df):
    df["rate"] = df["rate"].replace("NEW", np.nan)
    df["rate"] = df["rate"].replace("-", np.nan)
    df.dropna(how='any',inplace=True)
    return df


df = remove_nan(df)
df.isnull().sum()
89/5:
def rename_features(df):
    df.rename(columns={'approx_cost(for two people)': 'price', 'listed_in(city)': 'city_area',
                      'listed_in(type)': 'meal_type'}, inplace=True)
    return df


df = rename_features(df)
df.columns
89/6:
df.drop_duplicates(inplace=True)
df.info()
89/7:
def transform_rate(df):
    rates = []
    for _, row in df.iterrows():
        rate_in_float = float(row["rate"].split("/")[0])
        rates.append(rate_in_float)
    df["rate"] = rates
    return df


def transform_price(df):
    prices = []
    for _, row in df.iterrows():
        price_s = row["price"]
        price_s = price_s.replace(",", "")
        price = int(price_s)
        prices.append(price)
    df["price"] = prices
    return df


df = transform_rate(df)
df = transform_price(df)

df[["rate", "price"]].head()
89/8: df["price"].min()
89/9:
def get_geo_info_location(location):
    geo_info = geolocator.geocode(location)
    if geo_info is None:
        return np.nan, np.nan
    return geo_info.latitude, geo_info.longitude


def get_location_coordinate(df):
    all_location = df["location"].unique()
    coordinate = dict()
    for location in all_location:
        coordinate[location] = get_geo_info_location(location + ", Bangalore")
    return coordinate


geolocator=Nominatim(user_agent="app")   
lat_lon_location = get_location_coordinate(df)
lat_lon_location
89/10:
def Encode(df):
    for column in df.columns[~df.columns.isin(['rate', 'price', 'votes'])]:
        df[column] = df[column].factorize()[0]
    return df


df_float = Encode(df.copy())
df_float.head()
89/11:
def success_cluster(n_cluster=5):
    df_cluster = df.copy()
    kmean = KMeans(n_clusters=n_cluster, random_state=100).fit(df_cluster[["rate", "votes"]])
    df_cluster["cluster"] = kmean.labels_
    return df_cluster
89/12: df.describe()
89/13:
def get_train_test_data(df_float):
    x = df_float.iloc[:,[0, 1, 4, 5, 6, 7]]
    y = df_float.iloc[:,[2, 3]]
    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.1, random_state=100)
    return x_train, x_test, y_train, y_test


x_train, x_test, y_train, y_test = get_train_test_data(df_float)
x_train.head()
89/14:
def train_decision_tree(x_train, y_train):
    decision_tree = DecisionTreeRegressor()
    decision_tree.fit(x_train, y_train)
    return decision_tree


DT_model = train_decision_tree(x_train, y_train)
89/15:
def train_linear_regression(x_train, y_train):
    linear_regression = LinearRegression()
    linear_regression.fit(x_train, y_train)
    return linear_regression


LR_model = train_linear_regression(x_train, y_train)
89/16:
def train_random_forest(x_train, y_train):
    random_forest = RandomForestRegressor(n_estimators=500, min_samples_leaf=.0001)
    random_forest.fit(x_train, y_train)
    return random_forest


RF_model = train_random_forest(x_train, y_train)
89/17:
def train_extra_tree(x_train, y_train):
    extra_tree = ExtraTreesRegressor(n_estimators = 100)
    extra_tree.fit(x_train, y_train)
    return extra_tree


ET_model = train_extra_tree(x_train, y_train)
89/18:
def test_model(model, x_test, y_test):
    y_predict=model.predict(x_test)
    score = r2_score(y_test, y_predict)
    return score
89/19:
RF_score = test_model(RF_model, x_test, y_test)
DT_score = test_model(DT_model, x_test, y_test)
LR_score = test_model(LR_model, x_test, y_test)
ET_score = test_model(ET_model, x_test, y_test)


evaluation = pd.DataFrame({"Model": ["Random Forest", "Decision Tree", "Linear Regression", "Extra Trees"], 
                           "R^2 Score": [RF_score, DT_score, LR_score, ET_score]})

evaluation.sort_values(by = 'R^2 Score', ascending = False, ignore_index=True)
89/20:
def heatmap_generate(lat_lon_location, location_info, need_clustering=False):
    map_info = []
    for location in location_info:
        if not np.isnan(lat_lon_location[location][0]):
            map_info.append(list(lat_lon_location[location]) + [float(location_info[location])])
    base_map = Map(location=[12.9716, 77.5946], zoom_start=11)
    HeatMap(map_info ,zoom=20,radius=15).add_to(base_map)
    if need_clustering:
        FastMarkerCluster(map_info).add_to(base_map)
    return base_map
89/21:
def generate_count_heatmap():
    location_count = dict(df['location'].value_counts())
    heatmap_count = heatmap_generate(lat_lon_location, location_count)
    return heatmap_count
89/22:
def generate_avg_heatmap():
    avg_price = dict(df.groupby(['location'])['price'].mean())
    heatmap_avg = heatmap_generate(lat_lon_location, avg_price, need_clustering=True)
    return heatmap_avg
89/23:
def generate_med_heatmap():
    med_price = dict(df.groupby(['location'])['price'].median())
    heatmap_med = heatmap_generate(lat_lon_location, med_price, need_clustering=True)
    return heatmap_med
89/24:
def plot_price_dist():
    fig, ax = plt.subplots(figsize=[10,4])
    sns.distplot(df["price"],ax=ax)
89/25:
def plot_rate_dist():
    fig, ax = plt.subplots(figsize=[10,4])
    sns.distplot(df["rate"],ax=ax)
89/26:
def plot_votes_dist():
    fig, ax = plt.subplots(figsize=[12,4])
    sns.distplot(df["votes"],ax=ax)
89/27:
def plot_rate_votes_cluster(n_cluster=5):
    df_cluster = success_cluster(n_cluster)
    colors = ["g", "r", "b", "c", "m", "y"]
    
    plt.figure(figsize = (8,5))
    
    for i in range(n_cluster):
        selected = df_cluster[df_cluster["cluster"] == i]
        plt.plot(selected["rate"], selected["votes"], "ob", color=colors[i])
    plt.show()
89/28:
def plot_rate_price_cluster(n_cluster=5):
    df_cluster = success_cluster(n_cluster)
    colors = ["g", "r", "b", "c", "m", "y"]
    plt.figure(figsize = (8,5))
    for i in range(n_cluster):
        selected = df_cluster[df_cluster["cluster"] == i]
        plt.plot(selected["rate"], selected["price"], "ob", color=colors[i])
    plt.show()
89/29:
def plot_votes_price_cluster(n_cluster=5):
    df_cluster = success_cluster(n_cluster)
    colors = ["g", "r", "b", "c", "m", "y"]
    plt.figure(figsize = (8,5))
    for i in range(n_cluster):
        selected = df_cluster[df_cluster["cluster"] == i]
        plt.plot(selected["votes"], selected["price"], "ob", color=colors[i])
    plt.show()
89/30:
def Encode(df):
    column_dict = dict{}
    for column in df.columns[~df.columns.isin(['rate', 'price', 'votes'])]:
        df[column], column_dict[column] = df[column].factorize()[0]
    return df, column_dict


df_float, column_dict = Encode(df.copy())
df_float.head()
column_dict
89/31:
def Encode(df):
    column_dict = dict()
    for column in df.columns[~df.columns.isin(['rate', 'price', 'votes'])]:
        df[column], column_dict[column] = df[column].factorize()[0]
    return df, column_dict


df_float, column_dict = Encode(df.copy())
df_float.head()
column_dict
89/32:
def Encode(df):
    column_dict = dict()
    for column in df.columns[~df.columns.isin(['rate', 'price', 'votes'])]:
        df[column], column_dict[column] = df[column].factorize()
    return df, column_dict


df_float, column_dict = Encode(df.copy())
df_float.head()
column_dict
89/33:
def Encode(df):
    column_dict = dict()
    for column in df.columns[~df.columns.isin(['rate', 'price', 'votes'])]:
        df[column], column_dict[column] = df[column].factorize()
    return df, column_dict


df_float, column_dict = Encode(df.copy())
df_float.head()
89/34:
def Encode(df):
    column_dict = Dataframe()
    for column in df.columns[~df.columns.isin(['rate', 'price', 'votes'])]:
        df[column], column_dict[column] = df[column].factorize()
    return df, column_dict


df_float, column_dict = Encode(df.copy())
df_float.head()
column_dict
89/35:
def Encode(df):
    column_dict = DataFrame()
    for column in df.columns[~df.columns.isin(['rate', 'price', 'votes'])]:
        df[column], column_dict[column] = df[column].factorize()
    return df, column_dict


df_float, column_dict = Encode(df.copy())
df_float.head()
column_dict
89/36:
def Encode(df):
    column_dict = pd.DataFrame()
    for column in df.columns[~df.columns.isin(['rate', 'price', 'votes'])]:
        df[column], column_dict[column] = df[column].factorize()
    return df, column_dict


df_float, column_dict = Encode(df.copy())
df_float.head()
column_dict
89/37:
def Encode(df):
    column_dict = dict()
    for column in df.columns[~df.columns.isin(['rate', 'price', 'votes'])]:
        df[column], column_dict[column] = df[column].factorize()
    return df, column_dict


df_float, column_dict = Encode(df.copy())
df_float.head()
89/38:
def Encode(df):
    column_dict = dict()
    for column in df.columns[~df.columns.isin(['rate', 'price', 'votes'])]:
        df[column], column_dict[column] = df[column].factorize()
    return df, column_dict


df_float, column_dict = Encode(df.copy())
df_float.head()
column_dict
89/39: df.head()
89/40: df.iloc[0]
89/41: df.iloc[0]["rate"]
89/42: df.iloc[0].columns
89/43: df.iloc[0].copy()
89/44: df.iloc[0].columns
89/45: df.loc[0].
89/46: df.loc[0]
89/47: df[0]
89/48: df[:1]
89/49: df[:1]["rate"]
89/50: df[:1]
89/51: df[:1]["meal_type"]
89/52: df[:1]["meal_type"] == "Buffet"
89/53: if df[:1]["meal_type"] == "Buffet": print("f")
89/54: if (df[:1]["meal_type"].item() == "Buffet") : print("f")
89/55: df[:1]["meal_type"].item()
89/56: encode_predict(df[:1])
89/57:
def encode_predict(df):
    df_encode = df.copy()
    for column in df.columns[~df.columns.isin(['rate', 'price', 'votes'])]:
        index = column_dict[column].index(df[column].item)
        df_encode[column] = index
    return df_encode


        
    

def predict(restaurant_plan):
    for column in restaurant_plan:
89/58:
def encode_predict(df):
    df_encode = df.copy()
    for column in df.columns[~df.columns.isin(['rate', 'price', 'votes'])]:
        index = column_dict[column].index(df[column].item)
        df_encode[column] = index
    return df_encode


        
    

# def predict(restaurant_plan):
#     for column in restaurant_plan:
89/59: encode_predict(df[:1])
89/60:
# encode_predict(df[:1])
column_dict["meal_type"]
89/61:
# encode_predict(df[:1])
list(column_dict["meal_type"])
89/62:
def encode_predict(df):
    df_encode = df.copy()
    for column in df.columns[~df.columns.isin(['rate', 'price', 'votes'])]:
        index = list(column_dict[column]).index(df[column].item)
        df_encode[column] = index
    return df_encode


        
    

# def predict(restaurant_plan):
#     for column in restaurant_plan:
89/63:
# encode_predict(df[:1])
list(column_dict["meal_type"])
89/64:
encode_predict(df[:1])
# list(column_dict["meal_type"])
89/65:
# encode_predict(df[:1])
df[:1]["online_order"]
89/66:
# encode_predict(df[:1])
df[:1]["online_order"].iten
89/67:
# encode_predict(df[:1])
df[:1]["online_order"].item
89/68:
def encode_predict(df):
    df_encode = df.copy()
    for column in df.columns[~df.columns.isin(['rate', 'price', 'votes'])]:
        index = list(column_dict[column]).index(df[column].item())
        df_encode[column] = index
    return df_encode


        
    

# def predict(restaurant_plan):
#     for column in restaurant_plan:
89/69:
# encode_predict(df[:1])
df[:1]["online_order"].item()
89/70:
encode_predict(df[:1])
# df[:1]["online_order"].item()
89/71:
# encode_predict(df[:1])

predict(ET_model, df[:1])
89/72:
def encode_predict(df):
    df_encode = df.copy()
    for column in df.columns[~df.columns.isin(['rate', 'price', 'votes'])]:
        index = list(column_dict[column]).index(df[column].item())
        df_encode[column] = index
    return df_encode


        
    

def predict(model, restaurant_plan):
    plan_encode = encode_predict(restaurant_plan)
    return model.predict(plan_encode)
89/73:
# encode_predict(df[:1])

predict(ET_model, df[:1])
89/74:
# encode_predict(df[:1])

# predict(ET_model, df[:1])

df[:1]
89/75:
df = original_df.copy()

def drop_unnecessary_columns(df):
    df.drop(columns=['dish_liked', 'cuisines', 'menu_item'], inplace=True)
    df.drop(columns=['name', 'url', 'phone', 'reviews_list', "address", "listed_in(city)"], inplace=True)
    return df


df = drop_unnecessary_columns(df)
df.columns
89/76:
def remove_nan(df):
    df["rate"] = df["rate"].replace("NEW", np.nan)
    df["rate"] = df["rate"].replace("-", np.nan)
    df.dropna(how='any',inplace=True)
    return df


df = remove_nan(df)
df.isnull().sum()
89/77:
def rename_features(df):
    df.rename(columns={'approx_cost(for two people)': 'price',
                      'listed_in(type)': 'meal_type'}, inplace=True)
    return df


df = rename_features(df)
df.columns
89/78:
df.drop_duplicates(inplace=True)
df.info()
90/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from folium import Map
from folium.plugins import HeatMap, FastMarkerCluster
from geopy.geocoders import Nominatim
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import  ExtraTreesRegressor
from sklearn.cluster import KMeans
90/2:
def read_data():
    df = pd.read_csv("zomato.csv")
    return df


original_df = read_data()
original_df.head()
90/3:
df = original_df.copy()

def drop_unnecessary_columns(df):
    df.drop(columns=['dish_liked', 'cuisines', 'menu_item'], inplace=True)
    df.drop(columns=['name', 'url', 'phone', 'reviews_list', "address", "listed_in(city)"], inplace=True)
    return df


df = drop_unnecessary_columns(df)
df.columns
90/4:
def remove_nan(df):
    df["rate"] = df["rate"].replace("NEW", np.nan)
    df["rate"] = df["rate"].replace("-", np.nan)
    df.dropna(how='any',inplace=True)
    return df


df = remove_nan(df)
df.isnull().sum()
90/5:
def rename_features(df):
    df.rename(columns={'approx_cost(for two people)': 'price',
                      'listed_in(type)': 'meal_type'}, inplace=True)
    return df


df = rename_features(df)
df.columns
90/6:
df.drop_duplicates(inplace=True)
df.info()
90/7:
def transform_rate(df):
    rates = []
    for _, row in df.iterrows():
        rate_in_float = float(row["rate"].split("/")[0])
        rates.append(rate_in_float)
    df["rate"] = rates
    return df


def transform_price(df):
    prices = []
    for _, row in df.iterrows():
        price_s = row["price"]
        price_s = price_s.replace(",", "")
        price = int(price_s)
        prices.append(price)
    df["price"] = prices
    return df


df = transform_rate(df)
df = transform_price(df)

df[["rate", "price"]].head()
90/8:
def get_geo_info_location(location):
    geo_info = geolocator.geocode(location)
    if geo_info is None:
        return np.nan, np.nan
    return geo_info.latitude, geo_info.longitude


def get_location_coordinate(df):
    all_location = df["location"].unique()
    coordinate = dict()
    for location in all_location:
        coordinate[location] = get_geo_info_location(location + ", Bangalore")
    return coordinate


geolocator=Nominatim(user_agent="app")   
lat_lon_location = get_location_coordinate(df)
lat_lon_location
90/9:
def Encode(df):
    column_dict = dict()
    for column in df.columns[~df.columns.isin(['rate', 'price', 'votes'])]:
        df[column], column_dict[column] = df[column].factorize()
    return df, column_dict


df_float, column_dict = Encode(df.copy())
df_float.head()
column_dict
90/10:
def get_train_test_data(df_float):
    x = df_float.iloc[:,[0, 1, 4, 5, 6, 7]]
    y = df_float.iloc[:,[2, 3]]
    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.1, random_state=100)
    return x_train, x_test, y_train, y_test


x_train, x_test, y_train, y_test = get_train_test_data(df_float)
x_train.head()
90/11:
def train_decision_tree(x_train, y_train):
    decision_tree = DecisionTreeRegressor()
    decision_tree.fit(x_train, y_train)
    return decision_tree


DT_model = train_decision_tree(x_train, y_train)
90/12:
def train_linear_regression(x_train, y_train):
    linear_regression = LinearRegression()
    linear_regression.fit(x_train, y_train)
    return linear_regression


LR_model = train_linear_regression(x_train, y_train)
90/13:
def train_random_forest(x_train, y_train):
    random_forest = RandomForestRegressor(n_estimators=500, min_samples_leaf=.0001)
    random_forest.fit(x_train, y_train)
    return random_forest


RF_model = train_random_forest(x_train, y_train)
90/14:
def train_extra_tree(x_train, y_train):
    extra_tree = ExtraTreesRegressor(n_estimators = 100)
    extra_tree.fit(x_train, y_train)
    return extra_tree


ET_model = train_extra_tree(x_train, y_train)
90/15:
def test_model(model, x_test, y_test):
    y_predict=model.predict(x_test)
    score = r2_score(y_test, y_predict)
    return score
90/16:
RF_score = test_model(RF_model, x_test, y_test)
DT_score = test_model(DT_model, x_test, y_test)
LR_score = test_model(LR_model, x_test, y_test)
ET_score = test_model(ET_model, x_test, y_test)


evaluation = pd.DataFrame({"Model": ["Random Forest", "Decision Tree", "Linear Regression", "Extra Trees"], 
                           "R^2 Score": [RF_score, DT_score, LR_score, ET_score]})

evaluation.sort_values(by = 'R^2 Score', ascending = False, ignore_index=True)
90/17:
def success_cluster(n_cluster=5):
    df_cluster = df.copy()
    kmean = KMeans(n_clusters=n_cluster, random_state=100).fit(df_cluster[["rate", "votes"]])
    df_cluster["cluster"] = kmean.labels_
    return df_cluster
90/18: df.describe()
90/19:
def encode_predict(df):
    df_encode = df.copy()
    for column in df.columns[~df.columns.isin(['rate', 'price', 'votes'])]:
        index = list(column_dict[column]).index(df[column].item())
        df_encode[column] = index
    return df_encode


        
    

def predict(model, restaurant_plan):
    plan_encode = encode_predict(restaurant_plan)
    return model.predict(plan_encode)
90/20:
# encode_predict(df[:1])

# predict(ET_model, df[:1])

df[:1]
90/21:
def heatmap_generate(lat_lon_location, location_info, need_clustering=False):
    map_info = []
    for location in location_info:
        if not np.isnan(lat_lon_location[location][0]):
            map_info.append(list(lat_lon_location[location]) + [float(location_info[location])])
    base_map = Map(location=[12.9716, 77.5946], zoom_start=11)
    HeatMap(map_info ,zoom=20,radius=15).add_to(base_map)
    if need_clustering:
        FastMarkerCluster(map_info).add_to(base_map)
    return base_map
90/22:
def generate_count_heatmap():
    location_count = dict(df['location'].value_counts())
    heatmap_count = heatmap_generate(lat_lon_location, location_count)
    return heatmap_count
90/23:
def generate_avg_heatmap():
    avg_price = dict(df.groupby(['location'])['price'].mean())
    heatmap_avg = heatmap_generate(lat_lon_location, avg_price, need_clustering=True)
    return heatmap_avg
90/24:
def generate_med_heatmap():
    med_price = dict(df.groupby(['location'])['price'].median())
    heatmap_med = heatmap_generate(lat_lon_location, med_price, need_clustering=True)
    return heatmap_med
90/25:
def plot_price_dist():
    fig, ax = plt.subplots(figsize=[10,4])
    sns.distplot(df["price"],ax=ax)
90/26:
def plot_rate_dist():
    fig, ax = plt.subplots(figsize=[10,4])
    sns.distplot(df["rate"],ax=ax)
90/27:
def plot_votes_dist():
    fig, ax = plt.subplots(figsize=[12,4])
    sns.distplot(df["votes"],ax=ax)
90/28:
def plot_rate_votes_cluster(n_cluster=5):
    df_cluster = success_cluster(n_cluster)
    colors = ["g", "r", "b", "c", "m", "y"]
    
    plt.figure(figsize = (8,5))
    
    for i in range(n_cluster):
        selected = df_cluster[df_cluster["cluster"] == i]
        plt.plot(selected["rate"], selected["votes"], "ob", color=colors[i])
    plt.show()
90/29:
def plot_rate_price_cluster(n_cluster=5):
    df_cluster = success_cluster(n_cluster)
    colors = ["g", "r", "b", "c", "m", "y"]
    plt.figure(figsize = (8,5))
    for i in range(n_cluster):
        selected = df_cluster[df_cluster["cluster"] == i]
        plt.plot(selected["rate"], selected["price"], "ob", color=colors[i])
    plt.show()
90/30:
def plot_votes_price_cluster(n_cluster=5):
    df_cluster = success_cluster(n_cluster)
    colors = ["g", "r", "b", "c", "m", "y"]
    plt.figure(figsize = (8,5))
    for i in range(n_cluster):
        selected = df_cluster[df_cluster["cluster"] == i]
        plt.plot(selected["votes"], selected["price"], "ob", color=colors[i])
    plt.show()
91/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from folium import Map
from folium.plugins import HeatMap, FastMarkerCluster
from geopy.geocoders import Nominatim
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import  ExtraTreesRegressor
from sklearn.cluster import KMeans
91/2:
def read_data():
    df = pd.read_csv("zomato.csv")
    return df


original_df = read_data()
original_df.head()
91/3:
df = original_df.copy()

def drop_unnecessary_columns(df):
    df.drop(columns=['dish_liked', 'cuisines', 'menu_item'], inplace=True)
    df.drop(columns=['name', 'url', 'phone', 'reviews_list', "address", "listed_in(city)"], inplace=True)
    return df


df = drop_unnecessary_columns(df)
df.columns
91/4:
def rename_features(df):
    df.rename(columns={'approx_cost(for two people)': 'price',
                      'listed_in(type)': 'meal_type'}, inplace=True)
    return df


df = rename_features(df)
df.columns
91/5:
df.drop_duplicates(inplace=True)
df.info()
91/6:
def remove_nan(df):
    df["rate"] = df["rate"].replace("NEW", np.nan)
    df["rate"] = df["rate"].replace("-", np.nan)
    df.dropna(how='any',inplace=True)
    return df


df = remove_nan(df)
df.isnull().sum()
91/7:
def transform_rate(df):
    rates = []
    for _, row in df.iterrows():
        rate_in_float = float(row["rate"].split("/")[0])
        rates.append(rate_in_float)
    df["rate"] = rates
    return df


def transform_price(df):
    prices = []
    for _, row in df.iterrows():
        price_s = row["price"]
        price_s = price_s.replace(",", "")
        price = int(price_s)
        prices.append(price)
    df["price"] = prices
    return df


df = transform_rate(df)
df = transform_price(df)

df[["rate", "price"]].head()
91/8:
def get_geo_info_location(location):
    geo_info = geolocator.geocode(location)
    if geo_info is None:
        return np.nan, np.nan
    return geo_info.latitude, geo_info.longitude


def get_location_coordinate(df):
    all_location = df["location"].unique()
    coordinate = dict()
    for location in all_location:
        coordinate[location] = get_geo_info_location(location + ", Bangalore")
    return coordinate


geolocator=Nominatim(user_agent="app")   
lat_lon_location = get_location_coordinate(df)
lat_lon_location
91/9:
def Encode(df):
    column_dict = dict()
    for column in df.columns[~df.columns.isin(['rate', 'price', 'votes'])]:
        df[column], column_dict[column] = df[column].factorize()
    return df, column_dict


df_float, column_dict = Encode(df.copy())
df_float.head()
column_dict
91/10:
def get_train_test_data(df_float):
    x = df_float.iloc[:,[0, 1, 4, 5, 6, 7]]
    y = df_float.iloc[:,[2, 3]]
    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.1, random_state=100)
    return x_train, x_test, y_train, y_test


x_train, x_test, y_train, y_test = get_train_test_data(df_float)
x_train.head()
91/11:
def train_decision_tree(x_train, y_train):
    decision_tree = DecisionTreeRegressor()
    decision_tree.fit(x_train, y_train)
    return decision_tree


DT_model = train_decision_tree(x_train, y_train)
91/12:
def train_linear_regression(x_train, y_train):
    linear_regression = LinearRegression()
    linear_regression.fit(x_train, y_train)
    return linear_regression


LR_model = train_linear_regression(x_train, y_train)
91/13:
def train_random_forest(x_train, y_train):
    random_forest = RandomForestRegressor(n_estimators=500, min_samples_leaf=.0001)
    random_forest.fit(x_train, y_train)
    return random_forest


RF_model = train_random_forest(x_train, y_train)
91/14:
def train_extra_tree(x_train, y_train):
    extra_tree = ExtraTreesRegressor(n_estimators = 100)
    extra_tree.fit(x_train, y_train)
    return extra_tree


ET_model = train_extra_tree(x_train, y_train)
91/15:
def test_model(model, x_test, y_test):
    y_predict=model.predict(x_test)
    score = r2_score(y_test, y_predict)
    return score
91/16:
RF_score = test_model(RF_model, x_test, y_test)
DT_score = test_model(DT_model, x_test, y_test)
LR_score = test_model(LR_model, x_test, y_test)
ET_score = test_model(ET_model, x_test, y_test)


evaluation = pd.DataFrame({"Model": ["Random Forest", "Decision Tree", "Linear Regression", "Extra Trees"], 
                           "R^2 Score": [RF_score, DT_score, LR_score, ET_score]})

evaluation.sort_values(by = 'R^2 Score', ascending = False, ignore_index=True)
91/17:
def success_cluster(n_cluster=5):
    df_cluster = df.copy()
    kmean = KMeans(n_clusters=n_cluster, random_state=100).fit(df_cluster[["rate", "votes"]])
    df_cluster["cluster"] = kmean.labels_
    return df_cluster
91/18: df.describe()
91/19:
def encode_predict(df):
    df_encode = df.copy()
    for column in df.columns[~df.columns.isin(['rate', 'price', 'votes'])]:
        index = list(column_dict[column]).index(df[column].item())
        df_encode[column] = index
    return df_encode


        
    

def predict(model, restaurant_plan):
    plan_encode = encode_predict(restaurant_plan)
    return model.predict(plan_encode)
91/20:
# encode_predict(df[:1])

# predict(ET_model, df[:1])

df[:1]
91/21:
def heatmap_generate(lat_lon_location, location_info, need_clustering=False):
    map_info = []
    for location in location_info:
        if not np.isnan(lat_lon_location[location][0]):
            map_info.append(list(lat_lon_location[location]) + [float(location_info[location])])
    base_map = Map(location=[12.9716, 77.5946], zoom_start=11)
    HeatMap(map_info ,zoom=20,radius=15).add_to(base_map)
    if need_clustering:
        FastMarkerCluster(map_info).add_to(base_map)
    return base_map
91/22:
def generate_count_heatmap():
    location_count = dict(df['location'].value_counts())
    heatmap_count = heatmap_generate(lat_lon_location, location_count)
    return heatmap_count
91/23:
def generate_avg_heatmap():
    avg_price = dict(df.groupby(['location'])['price'].mean())
    heatmap_avg = heatmap_generate(lat_lon_location, avg_price, need_clustering=True)
    return heatmap_avg
91/24:
def generate_med_heatmap():
    med_price = dict(df.groupby(['location'])['price'].median())
    heatmap_med = heatmap_generate(lat_lon_location, med_price, need_clustering=True)
    return heatmap_med
91/25:
def plot_price_dist():
    fig, ax = plt.subplots(figsize=[10,4])
    sns.distplot(df["price"],ax=ax)
91/26:
def plot_rate_dist():
    fig, ax = plt.subplots(figsize=[10,4])
    sns.distplot(df["rate"],ax=ax)
91/27:
def plot_votes_dist():
    fig, ax = plt.subplots(figsize=[12,4])
    sns.distplot(df["votes"],ax=ax)
91/28:
def plot_rate_votes_cluster(n_cluster=5):
    df_cluster = success_cluster(n_cluster)
    colors = ["g", "r", "b", "c", "m", "y"]
    
    plt.figure(figsize = (8,5))
    
    for i in range(n_cluster):
        selected = df_cluster[df_cluster["cluster"] == i]
        plt.plot(selected["rate"], selected["votes"], "ob", color=colors[i])
    plt.show()
91/29:
def plot_rate_price_cluster(n_cluster=5):
    df_cluster = success_cluster(n_cluster)
    colors = ["g", "r", "b", "c", "m", "y"]
    plt.figure(figsize = (8,5))
    for i in range(n_cluster):
        selected = df_cluster[df_cluster["cluster"] == i]
        plt.plot(selected["rate"], selected["price"], "ob", color=colors[i])
    plt.show()
91/30:
def plot_votes_price_cluster(n_cluster=5):
    df_cluster = success_cluster(n_cluster)
    colors = ["g", "r", "b", "c", "m", "y"]
    plt.figure(figsize = (8,5))
    for i in range(n_cluster):
        selected = df_cluster[df_cluster["cluster"] == i]
        plt.plot(selected["votes"], selected["price"], "ob", color=colors[i])
    plt.show()
92/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from folium import Map
from folium.plugins import HeatMap, FastMarkerCluster
from geopy.geocoders import Nominatim
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import  ExtraTreesRegressor
from sklearn.cluster import KMeans
92/2:
def read_data():
    df = pd.read_csv("zomato.csv")
    return df


original_df = read_data()
original_df.head()
92/3:
df = original_df.copy()

def drop_unnecessary_columns(df):
    df.drop(columns=['dish_liked', 'cuisines', 'menu_item'], inplace=True)
    df.drop(columns=['name', 'url', 'phone', 'reviews_list', "listed_in(city)"], inplace=True)
    return df


df = drop_unnecessary_columns(df)
df.columns
92/4:
def rename_features(df):
    df.rename(columns={'approx_cost(for two people)': 'price',
                      'listed_in(type)': 'meal_type'}, inplace=True)
    return df


df = rename_features(df)
df.columns
92/5:
df.drop_duplicates(inplace=True)
df.info()
92/6:
def drop_address(df):
    df.drop(columns=["address"], inplace=True)
    return df


df = drop_address(df)
df.columns
92/7:
def remove_nan(df):
    df["rate"] = df["rate"].replace("NEW", np.nan)
    df["rate"] = df["rate"].replace("-", np.nan)
    df.dropna(how='any',inplace=True)
    return df


df = remove_nan(df)
df.isnull().sum()
92/8:
def transform_rate(df):
    rates = []
    for _, row in df.iterrows():
        rate_in_float = float(row["rate"].split("/")[0])
        rates.append(rate_in_float)
    df["rate"] = rates
    return df


def transform_price(df):
    prices = []
    for _, row in df.iterrows():
        price_s = row["price"]
        price_s = price_s.replace(",", "")
        price = int(price_s)
        prices.append(price)
    df["price"] = prices
    return df


df = transform_rate(df)
df = transform_price(df)

df[["rate", "price"]].head()
92/9:
def get_geo_info_location(location):
    geo_info = geolocator.geocode(location)
    if geo_info is None:
        return np.nan, np.nan
    return geo_info.latitude, geo_info.longitude


def get_location_coordinate(df):
    all_location = df["location"].unique()
    coordinate = dict()
    for location in all_location:
        coordinate[location] = get_geo_info_location(location + ", Bangalore")
    return coordinate


geolocator=Nominatim(user_agent="app")   
lat_lon_location = get_location_coordinate(df)
lat_lon_location
92/10:
def Encode(df):
    column_dict = dict()
    for column in df.columns[~df.columns.isin(['rate', 'price', 'votes'])]:
        df[column], column_dict[column] = df[column].factorize()
    return df, column_dict


df_float, column_dict = Encode(df.copy())
df_float.head()
column_dict
92/11:
def get_train_test_data(df_float):
    x = df_float.iloc[:,[0, 1, 4, 5, 6, 7]]
    y = df_float.iloc[:,[2, 3]]
    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.1, random_state=100)
    return x_train, x_test, y_train, y_test


x_train, x_test, y_train, y_test = get_train_test_data(df_float)
x_train.head()
92/12:
def train_decision_tree(x_train, y_train):
    decision_tree = DecisionTreeRegressor()
    decision_tree.fit(x_train, y_train)
    return decision_tree


DT_model = train_decision_tree(x_train, y_train)
92/13:
def train_linear_regression(x_train, y_train):
    linear_regression = LinearRegression()
    linear_regression.fit(x_train, y_train)
    return linear_regression


LR_model = train_linear_regression(x_train, y_train)
92/14:
def train_random_forest(x_train, y_train):
    random_forest = RandomForestRegressor(n_estimators=500, min_samples_leaf=.0001)
    random_forest.fit(x_train, y_train)
    return random_forest


RF_model = train_random_forest(x_train, y_train)
92/15:
def train_extra_tree(x_train, y_train):
    extra_tree = ExtraTreesRegressor(n_estimators = 100)
    extra_tree.fit(x_train, y_train)
    return extra_tree


ET_model = train_extra_tree(x_train, y_train)
92/16:
def test_model(model, x_test, y_test):
    y_predict=model.predict(x_test)
    score = r2_score(y_test, y_predict)
    return score
92/17:
RF_score = test_model(RF_model, x_test, y_test)
DT_score = test_model(DT_model, x_test, y_test)
LR_score = test_model(LR_model, x_test, y_test)
ET_score = test_model(ET_model, x_test, y_test)


evaluation = pd.DataFrame({"Model": ["Random Forest", "Decision Tree", "Linear Regression", "Extra Trees"], 
                           "R^2 Score": [RF_score, DT_score, LR_score, ET_score]})

evaluation.sort_values(by = 'R^2 Score', ascending = False, ignore_index=True)
92/18:
def success_cluster(n_cluster=5):
    df_cluster = df.copy()
    kmean = KMeans(n_clusters=n_cluster, random_state=100).fit(df_cluster[["rate", "votes"]])
    df_cluster["cluster"] = kmean.labels_
    return df_cluster
92/19: df.describe()
92/20:
def encode_predict(df):
    df_encode = df.copy()
    for column in df.columns[~df.columns.isin(['rate', 'price', 'votes'])]:
        index = list(column_dict[column]).index(df[column].item())
        df_encode[column] = index
    return df_encode


        
    

def predict(model, restaurant_plan):
    plan_encode = encode_predict(restaurant_plan)
    return model.predict(plan_encode)
92/21:
# encode_predict(df[:1])

# predict(ET_model, df[:1])

df[:1]
92/22:
def heatmap_generate(lat_lon_location, location_info, need_clustering=False):
    map_info = []
    for location in location_info:
        if not np.isnan(lat_lon_location[location][0]):
            map_info.append(list(lat_lon_location[location]) + [float(location_info[location])])
    base_map = Map(location=[12.9716, 77.5946], zoom_start=11)
    HeatMap(map_info ,zoom=20,radius=15).add_to(base_map)
    if need_clustering:
        FastMarkerCluster(map_info).add_to(base_map)
    return base_map
92/23:
def generate_count_heatmap():
    location_count = dict(df['location'].value_counts())
    heatmap_count = heatmap_generate(lat_lon_location, location_count)
    return heatmap_count
92/24:
def generate_avg_heatmap():
    avg_price = dict(df.groupby(['location'])['price'].mean())
    heatmap_avg = heatmap_generate(lat_lon_location, avg_price, need_clustering=True)
    return heatmap_avg
92/25:
def generate_med_heatmap():
    med_price = dict(df.groupby(['location'])['price'].median())
    heatmap_med = heatmap_generate(lat_lon_location, med_price, need_clustering=True)
    return heatmap_med
92/26:
def plot_price_dist():
    fig, ax = plt.subplots(figsize=[10,4])
    sns.distplot(df["price"],ax=ax)
92/27:
def plot_rate_dist():
    fig, ax = plt.subplots(figsize=[10,4])
    sns.distplot(df["rate"],ax=ax)
92/28:
def plot_votes_dist():
    fig, ax = plt.subplots(figsize=[12,4])
    sns.distplot(df["votes"],ax=ax)
92/29:
def plot_rate_votes_cluster(n_cluster=5):
    df_cluster = success_cluster(n_cluster)
    colors = ["g", "r", "b", "c", "m", "y"]
    
    plt.figure(figsize = (8,5))
    
    for i in range(n_cluster):
        selected = df_cluster[df_cluster["cluster"] == i]
        plt.plot(selected["rate"], selected["votes"], "ob", color=colors[i])
    plt.show()
92/30:
def plot_rate_price_cluster(n_cluster=5):
    df_cluster = success_cluster(n_cluster)
    colors = ["g", "r", "b", "c", "m", "y"]
    plt.figure(figsize = (8,5))
    for i in range(n_cluster):
        selected = df_cluster[df_cluster["cluster"] == i]
        plt.plot(selected["rate"], selected["price"], "ob", color=colors[i])
    plt.show()
92/31:
def plot_votes_price_cluster(n_cluster=5):
    df_cluster = success_cluster(n_cluster)
    colors = ["g", "r", "b", "c", "m", "y"]
    plt.figure(figsize = (8,5))
    for i in range(n_cluster):
        selected = df_cluster[df_cluster["cluster"] == i]
        plt.plot(selected["votes"], selected["price"], "ob", color=colors[i])
    plt.show()
92/32:
def remove_nan(df):
    df["rate"] = df["rate"].replace("NEW", np.nan)
    df["rate"] = df["rate"].replace("-", np.nan)
    df.dropna(how='any',inplace=True)
    return df


df = remove_nan(df)
df.info()
92/33:
df = original_df.copy()

df.drop_duplicates(inplace=True)
df.info()
92/34:
df = original_df.copy()

def drop_unnecessary_columns(df):
    df.drop(columns=['dish_liked', 'cuisines', 'menu_item'], inplace=True)
    df.drop(columns=['name', 'url', 'phone', 'reviews_list', "listed_in(city)"], inplace=True)
    return df


df = drop_unnecessary_columns(df)
df.columns
92/35:
def rename_features(df):
    df.rename(columns={'approx_cost(for two people)': 'price', 'listed_in(city)': 'city_area',
                      'listed_in(type)': 'meal_type'}, inplace=True)
    return df


df = rename_features(df)
df.columns
92/36:
df.drop_duplicates(inplace=True)
df.info()
92/37:
def drop_address(df):
    df.drop(columns=["address"], inplace=True)
    return df


df = drop_address(df)
df.columns
92/38:
def remove_nan(df):
    df["rate"] = df["rate"].replace("NEW", np.nan)
    df["rate"] = df["rate"].replace("-", np.nan)
    df.dropna(how='any',inplace=True)
    return df


df = remove_nan(df)
df.info()
92/39: df.duplicate()
92/40: df.duplicated()
92/41: original_df.duplicated()
92/42: original_df.duplicated().sum()
92/43: original_df.drop(columns=["url"]).duplicated().sum()
92/44:
df = original_df.copy()

def drop_unnecessary_columns(df):
    df.drop(columns=['dish_liked', 'cuisines', 'menu_item'], inplace=True)
    df.drop(columns=['name', 'url', 'phone', 'reviews_list', "listed_in(city)"], inplace=True)
    return df


df = drop_unnecessary_columns(df)
df.columns
92/45: df.duplicated().sum()
92/46:
def rename_features(df):
    df.rename(columns={'approx_cost(for two people)': 'price', 'listed_in(city)': 'city_area',
                      'listed_in(type)': 'meal_type'}, inplace=True)
    return df


df = rename_features(df)
df.columns
92/47:
df = original_df.copy()

def drop_unnecessary_columns(df):
    df.drop(columns=['dish_liked', 'cuisines', 'menu_item'], inplace=True)
    df.drop(columns=['name', 'url', 'phone', 'reviews_list', "listed_in(city)"], inplace=True)
    return df


df = drop_unnecessary_columns(df)
df.info()
92/48:
def rename_features(df):
    df.rename(columns={'approx_cost(for two people)': 'price', 'listed_in(city)': 'city_area',
                      'listed_in(type)': 'meal_type'}, inplace=True)
    return df


df = rename_features(df)
df.columns
92/49:
def remove_nan(df):
    df["rate"] = df["rate"].replace("NEW", np.nan)
    df["rate"] = df["rate"].replace("-", np.nan)
    df.dropna(how='any',inplace=True)
    return df


df = remove_nan(df)
df.info()
92/50:
df.drop_duplicates(inplace=True)
df.info()
92/51:
def drop_address(df):
    df.drop(columns=["address"], inplace=True)
    return df


df = drop_address(df)
df.columns
92/52:
def transform_rate(df):
    rates = []
    for _, row in df.iterrows():
        rate_in_float = float(row["rate"].split("/")[0])
        rates.append(rate_in_float)
    df["rate"] = rates
    return df


def transform_price(df):
    prices = []
    for _, row in df.iterrows():
        price_s = row["price"]
        price_s = price_s.replace(",", "")
        price = int(price_s)
        prices.append(price)
    df["price"] = prices
    return df


df = transform_rate(df)
df = transform_price(df)

df[["rate", "price"]].head()
92/53:
def Encode(df):
    column_dict = dict()
    for column in df.columns[~df.columns.isin(['rate', 'price', 'votes'])]:
        df[column], column_dict[column] = df[column].factorize()
    return df, column_dict


df_float, column_dict = Encode(df.copy())
df_float.head()
column_dict
92/54:
def Encode(df):
    column_dict = dict()
    for column in df.columns[~df.columns.isin(['rate', 'price', 'votes'])]:
        df[column], column_dict[column] = df[column].factorize()
    return df, column_dict


df_float, column_dict = Encode(df.copy())
df_float.head()
92/55:
def get_train_test_data(df_float):
    x = df_float.iloc[:,[0, 1, 4, 5, 6, 7]]
    y = df_float.iloc[:,[2, 3]]
    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.1, random_state=100)
    return x_train, x_test, y_train, y_test


x_train, x_test, y_train, y_test = get_train_test_data(df_float)
x_train.head()
92/56:
def train_decision_tree(x_train, y_train):
    decision_tree = DecisionTreeRegressor()
    decision_tree.fit(x_train, y_train)
    return decision_tree


DT_model = train_decision_tree(x_train, y_train)
92/57:
def train_linear_regression(x_train, y_train):
    linear_regression = LinearRegression()
    linear_regression.fit(x_train, y_train)
    return linear_regression


LR_model = train_linear_regression(x_train, y_train)
92/58:
def train_random_forest(x_train, y_train):
    random_forest = RandomForestRegressor(n_estimators=500, min_samples_leaf=.0001)
    random_forest.fit(x_train, y_train)
    return random_forest


RF_model = train_random_forest(x_train, y_train)
92/59:
def train_extra_tree(x_train, y_train):
    extra_tree = ExtraTreesRegressor(n_estimators = 100)
    extra_tree.fit(x_train, y_train)
    return extra_tree


ET_model = train_extra_tree(x_train, y_train)
92/60:
RF_score = test_model(RF_model, x_test, y_test)
DT_score = test_model(DT_model, x_test, y_test)
LR_score = test_model(LR_model, x_test, y_test)
ET_score = test_model(ET_model, x_test, y_test)


evaluation = pd.DataFrame({"Model": ["Random Forest", "Decision Tree", "Linear Regression", "Extra Trees"], 
                           "R^2 Score": [RF_score, DT_score, LR_score, ET_score]})

evaluation.sort_values(by = 'R^2 Score', ascending = False, ignore_index=True)
92/61:
def success_cluster(n_cluster=5):
    df_cluster = df.copy()
    kmean = KMeans(n_clusters=n_cluster, random_state=100).fit(df_cluster[["rate", "votes"]])
    df_cluster["cluster"] = kmean.labels_
    return df_cluster
92/62: df.describe()
92/63:
def encode_predict(df):
    df_encode = df.copy()
    for column in df.columns[~df.columns.isin(['rate', 'price', 'votes'])]:
        index = list(column_dict[column]).index(df[column].item())
        df_encode[column] = index
    return df_encode


        
    

def predict(model, restaurant_plan):
    plan_encode = encode_predict(restaurant_plan)
    return model.predict(plan_encode)
92/64:
# encode_predict(df[:1])

predict(ET_model, df[:1].iloc[:,[0, 1, 4, 5, 6, 7]])
92/65:
def get_train_test_data(df_float):
    x = df_float.iloc[:,[0, 1, 4, 5, 6, 7]]
    y = df_float.iloc[:,[2, 3]]
    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.1, random_state=111)
    return x_train, x_test, y_train, y_test


x_train, x_test, y_train, y_test = get_train_test_data(df_float)
x_train.head()
92/66:
def train_decision_tree(x_train, y_train):
    decision_tree = DecisionTreeRegressor()
    decision_tree.fit(x_train, y_train)
    return decision_tree


DT_model = train_decision_tree(x_train, y_train)
92/67:
def train_linear_regression(x_train, y_train):
    linear_regression = LinearRegression()
    linear_regression.fit(x_train, y_train)
    return linear_regression


LR_model = train_linear_regression(x_train, y_train)
92/68:
def train_random_forest(x_train, y_train):
    random_forest = RandomForestRegressor(n_estimators=500, min_samples_leaf=.0001)
    random_forest.fit(x_train, y_train)
    return random_forest


RF_model = train_random_forest(x_train, y_train)
92/69:
def train_extra_tree(x_train, y_train):
    extra_tree = ExtraTreesRegressor(n_estimators = 100)
    extra_tree.fit(x_train, y_train)
    return extra_tree


ET_model = train_extra_tree(x_train, y_train)
92/70:
RF_score = test_model(RF_model, x_test, y_test)
DT_score = test_model(DT_model, x_test, y_test)
LR_score = test_model(LR_model, x_test, y_test)
ET_score = test_model(ET_model, x_test, y_test)


evaluation = pd.DataFrame({"Model": ["Random Forest", "Decision Tree", "Linear Regression", "Extra Trees"], 
                           "R^2 Score": [RF_score, DT_score, LR_score, ET_score]})

evaluation.sort_values(by = 'R^2 Score', ascending = False, ignore_index=True)
92/71: plot_rate_votes_cluster(n_cluster=5)
92/72: plot_rate_price_cluster(n_cluster=5)
92/73: plot_votes_price_cluster(n_cluster=5)
92/74:
def encode_predict(df):
    df_encode = df.copy()
    for column in df.columns[~df.columns.isin(['rate', 'price', 'votes'])]:
        index = list(column_dict[column]).index(df[column].item())
        df_encode[column] = index
    return df_encode


df_example = df[:1].iloc[:,[0, 1, 4, 5, 6, 7]
df_exm_encode = encode_predict(df_example)
df_exm_encode
92/75:
def encode_predict(df):
    df_encode = df.copy()
    for column in df.columns[~df.columns.isin(['rate', 'price', 'votes'])]:
        index = list(column_dict[column]).index(df[column].item())
        df_encode[column] = index
    return df_encode


df_example = df[:1].iloc[:,[0, 1, 4, 5, 6, 7]]
df_exm_encode = encode_predict(df_example)
df_exm_encode
92/76:
def drop_address(df):
    df.drop(columns=["address"], inplace=True)
    return df


df = drop_address(df)
92/77:
df = original_df.copy()

def drop_unnecessary_columns(df):
    df.drop(columns=['dish_liked', 'cuisines', 'menu_item'], inplace=True)
    df.drop(columns=['name', 'url', 'phone', 'reviews_list', "listed_in(city)"], inplace=True)
    return df


df = drop_unnecessary_columns(df)
df.info()
92/78:
def rename_features(df):
    df.rename(columns={'approx_cost(for two people)': 'price', 'listed_in(city)': 'city_area',
                      'listed_in(type)': 'meal_type'}, inplace=True)
    return df


df = rename_features(df)
df.columns
92/79:
def remove_nan(df):
    df["rate"] = df["rate"].replace("NEW", np.nan)
    df["rate"] = df["rate"].replace("-", np.nan)
    df.dropna(how='any',inplace=True)
    return df


df = remove_nan(df)
df.info()
92/80:
df.drop_duplicates(inplace=True)
df.info()
92/81:
def drop_address(df):
    df.drop(columns=["address"], inplace=True)
    return df


df = drop_address(df)
92/82:
def encode_predict(df):
    df_encode = df.copy()
    for column in df.columns[~df.columns.isin(['rate', 'price', 'votes'])]:
        index = list(column_dict[column]).index(df[column].item())
        df_encode[column] = index
    return df_encode


df_example = df[:1].iloc[:,[0, 1, 4, 5, 6, 7]]
df_exm_encode = encode_predict(df_example)
92/83:
# encode_predict(df[:1])

predict(ET_model, df_exm_encode)
92/84:
# encode_predict(df[:1])

predict(ET_model, df_example)
92/85: df.describe()
92/86: df.describe()
92/87: df
92/88: df.describe()
93/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from folium import Map
from folium.plugins import HeatMap, FastMarkerCluster
from geopy.geocoders import Nominatim
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import  ExtraTreesRegressor
from sklearn.cluster import KMeans
93/2:
def read_data():
    df = pd.read_csv("zomato.csv")
    return df


original_df = read_data()
original_df.head()
93/3:
df = original_df.copy()

def drop_unnecessary_columns(df):
    df.drop(columns=['dish_liked', 'cuisines', 'menu_item'], inplace=True)
    df.drop(columns=['name', 'url', 'phone', 'reviews_list', "listed_in(city)"], inplace=True)
    return df


df = drop_unnecessary_columns(df)
df.info()
93/4:
def rename_features(df):
    df.rename(columns={'approx_cost(for two people)': 'price', 'listed_in(city)': 'city_area',
                      'listed_in(type)': 'meal_type'}, inplace=True)
    return df


df = rename_features(df)
df.columns
93/5:
def remove_nan(df):
    df["rate"] = df["rate"].replace("NEW", np.nan)
    df["rate"] = df["rate"].replace("-", np.nan)
    df.dropna(how='any',inplace=True)
    return df


df = remove_nan(df)
df.info()
93/6:
df.drop_duplicates(inplace=True)
df.info()
93/7:
def drop_address(df):
    df.drop(columns=["address"], inplace=True)
    return df


df = drop_address(df)
93/8:
def transform_rate(df):
    rates = []
    for _, row in df.iterrows():
        rate_in_float = float(row["rate"].split("/")[0])
        rates.append(rate_in_float)
    df["rate"] = rates
    return df


def transform_price(df):
    prices = []
    for _, row in df.iterrows():
        price_s = row["price"]
        price_s = price_s.replace(",", "")
        price = int(price_s)
        prices.append(price)
    df["price"] = prices
    return df


df = transform_rate(df)
df = transform_price(df)

df[["rate", "price"]].head()
93/9:
def get_geo_info_location(location):
    geo_info = geolocator.geocode(location)
    if geo_info is None:
        return np.nan, np.nan
    return geo_info.latitude, geo_info.longitude


def get_location_coordinate(df):
    all_location = df["location"].unique()
    coordinate = dict()
    for location in all_location:
        coordinate[location] = get_geo_info_location(location + ", Bangalore")
    return coordinate


geolocator=Nominatim(user_agent="app")   
lat_lon_location = get_location_coordinate(df)
lat_lon_location
93/10:
def Encode(df):
    column_dict = dict()
    for column in df.columns[~df.columns.isin(['rate', 'price', 'votes'])]:
        df[column], column_dict[column] = df[column].factorize()
    return df, column_dict


df_float, column_dict = Encode(df.copy())
df_float.head()
93/11:
def encode_predict(df):
    df_encode = df.copy()
    for column in df.columns[~df.columns.isin(['rate', 'price', 'votes'])]:
        index = list(column_dict[column]).index(df[column].item())
        df_encode[column] = index
    return df_encode


df_example = df[:1].iloc[:,[0, 1, 4, 5, 6, 7]]
df_exm_encode = encode_predict(df_example)
93/12:
def get_train_test_data(df_float):
    x = df_float.iloc[:,[0, 1, 4, 5, 6, 7]]
    y = df_float.iloc[:,[2, 3]]
    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.1, random_state=111)
    return x_train, x_test, y_train, y_test


x_train, x_test, y_train, y_test = get_train_test_data(df_float)
x_train.head()
93/13:
def train_decision_tree(x_train, y_train):
    decision_tree = DecisionTreeRegressor()
    decision_tree.fit(x_train, y_train)
    return decision_tree


DT_model = train_decision_tree(x_train, y_train)
93/14:
def train_linear_regression(x_train, y_train):
    linear_regression = LinearRegression()
    linear_regression.fit(x_train, y_train)
    return linear_regression


LR_model = train_linear_regression(x_train, y_train)
93/15:
def train_random_forest(x_train, y_train):
    random_forest = RandomForestRegressor(n_estimators=500, min_samples_leaf=.0001)
    random_forest.fit(x_train, y_train)
    return random_forest


RF_model = train_random_forest(x_train, y_train)
93/16:
def train_extra_tree(x_train, y_train):
    extra_tree = ExtraTreesRegressor(n_estimators = 100)
    extra_tree.fit(x_train, y_train)
    return extra_tree


ET_model = train_extra_tree(x_train, y_train)
93/17:
def test_model(model, x_test, y_test):
    y_predict=model.predict(x_test)
    score = r2_score(y_test, y_predict)
    return score
93/18:
RF_score = test_model(RF_model, x_test, y_test)
DT_score = test_model(DT_model, x_test, y_test)
LR_score = test_model(LR_model, x_test, y_test)
ET_score = test_model(ET_model, x_test, y_test)


evaluation = pd.DataFrame({"Model": ["Random Forest", "Decision Tree", "Linear Regression", "Extra Trees"], 
                           "R^2 Score": [RF_score, DT_score, LR_score, ET_score]})

evaluation.sort_values(by = 'R^2 Score', ascending = False, ignore_index=True)
93/19:
def success_cluster(n_cluster=5):
    df_cluster = df.copy()
    kmean = KMeans(n_clusters=n_cluster, random_state=100).fit(df_cluster[["rate", "votes"]])
    df_cluster["cluster"] = kmean.labels_
    return df_cluster
93/20: df.describe()
93/21:
def predict(model, restaurant_plan):
    plan_encode = encode_predict(restaurant_plan)
    return model.predict(plan_encode)
93/22:
# encode_predict(df[:1])

predict(ET_model, df_example)
93/23:
def heatmap_generate(lat_lon_location, location_info, need_clustering=False):
    map_info = []
    for location in location_info:
        if not np.isnan(lat_lon_location[location][0]):
            map_info.append(list(lat_lon_location[location]) + [float(location_info[location])])
    base_map = Map(location=[12.9716, 77.5946], zoom_start=11)
    HeatMap(map_info ,zoom=20,radius=15).add_to(base_map)
    if need_clustering:
        FastMarkerCluster(map_info).add_to(base_map)
    return base_map
93/24:
def generate_count_heatmap():
    location_count = dict(df['location'].value_counts())
    heatmap_count = heatmap_generate(lat_lon_location, location_count)
    return heatmap_count
93/25:
def generate_avg_heatmap():
    avg_price = dict(df.groupby(['location'])['price'].mean())
    heatmap_avg = heatmap_generate(lat_lon_location, avg_price, need_clustering=True)
    return heatmap_avg
93/26:
def generate_med_heatmap():
    med_price = dict(df.groupby(['location'])['price'].median())
    heatmap_med = heatmap_generate(lat_lon_location, med_price, need_clustering=True)
    return heatmap_med
93/27:
def plot_price_dist():
    fig, ax = plt.subplots(figsize=[10,4])
    sns.distplot(df["price"],ax=ax)
93/28:
def plot_rate_dist():
    fig, ax = plt.subplots(figsize=[10,4])
    sns.distplot(df["rate"],ax=ax)
93/29:
def plot_votes_dist():
    fig, ax = plt.subplots(figsize=[12,4])
    sns.distplot(df["votes"],ax=ax)
93/30:
def plot_rate_votes_cluster(n_cluster=5):
    df_cluster = success_cluster(n_cluster)
    colors = ["g", "r", "b", "c", "m", "y"]
    
    plt.figure(figsize = (8,5))
    
    for i in range(n_cluster):
        selected = df_cluster[df_cluster["cluster"] == i]
        plt.plot(selected["rate"], selected["votes"], "ob", color=colors[i])
    plt.show()
93/31:
def plot_rate_price_cluster(n_cluster=5):
    df_cluster = success_cluster(n_cluster)
    colors = ["g", "r", "b", "c", "m", "y"]
    plt.figure(figsize = (8,5))
    for i in range(n_cluster):
        selected = df_cluster[df_cluster["cluster"] == i]
        plt.plot(selected["rate"], selected["price"], "ob", color=colors[i])
    plt.show()
93/32:
def plot_votes_price_cluster(n_cluster=5):
    df_cluster = success_cluster(n_cluster)
    colors = ["g", "r", "b", "c", "m", "y"]
    plt.figure(figsize = (8,5))
    for i in range(n_cluster):
        selected = df_cluster[df_cluster["cluster"] == i]
        plt.plot(selected["votes"], selected["price"], "ob", color=colors[i])
    plt.show()
93/33: plot_rate_votes_cluster(n_cluster=5)
93/34: plot_rate_price_cluster(n_cluster=5)
93/35: plot_votes_price_cluster(n_cluster=5)
93/36:
# encode_predict(df[:1])
predict_0 = pd.DataFrame({"online_order": 0, "book_table": 0, "location": 0, 
                        "rest_type": 0, "price": 2000, "meal_type": 0})
predict(ET_model, predict_0)
93/37:
# encode_predict(df[:1])
predict_0 = pd.DataFrame({"online_order": 0, "book_table": 0, "location": 0, 
                        "rest_type": 0, "price": 2000, "meal_type": 0}, index=0)
predict(ET_model, predict_0)
93/38:
# encode_predict(df[:1])
predict_0 = pd.DataFrame([{"online_order": 0, "book_table": 0, "location": 0, 
                        "rest_type": 0, "price": 2000, "meal_type": 0}])
predict(ET_model, predict_0)
93/39:
# encode_predict(df[:1])
predict_0 = pd.DataFrame([{"online_order": "Yes", "book_table": "Yes", "location": "Banashankari", 
                        "rest_type": "Casual Dining", "price": 2000, "meal_type": "Buffet"}])
predict(ET_model, predict_0)
93/40:
# encode_predict(df[:1])
predict_0 = pd.DataFrame([{"online_order": "Yes", "book_table": "No", "location": "Banashankari", 
                        "rest_type": "Casual Dining", "price": 2000, "meal_type": "Buffet"}])
# predict_0 = pd.DataFrame([{"online_order": "Yes", "book_table": "Yes", "location": "Banashankari", 
#                         "rest_type": "Casual Dining", "price": 2000, "meal_type": "Buffet"}])
# array([[   4.1419    , 1435.15866667]])
predict(ET_model, predict_0)
93/41:
# encode_predict(df[:1])
predict_0 = pd.DataFrame([{"online_order": "No", "book_table": "Yes", "location": "Banashankari", 
                        "rest_type": "Casual Dining", "price": 2000, "meal_type": "Buffet"}])
# predict_0 = pd.DataFrame([{"online_order": "Yes", "book_table": "Yes", "location": "Banashankari", 
#                         "rest_type": "Casual Dining", "price": 2000, "meal_type": "Buffet"}])
# array([[   4.1419    , 1435.15866667]])
predict(ET_model, predict_0)
93/42:
# encode_predict(df[:1])
# predict_0 = pd.DataFrame([{"online_order": "Yes", "book_table": "Yes", "location": "Banashankari", 
#                         "rest_type": "Casual Dining", "price": 2000, "meal_type": "Buffet"}])
# array([[   4.1419    , 1435.15866667]])
# predict_0 = pd.DataFrame([{"online_order": "No", "book_table": "Yes", "location": "Banashankari", 
#                         "rest_type": "Casual Dining", "price": 2000, "meal_type": "Buffet"}])
# array([[   4.2827, 1479.095 ]])2

predict_0 = pd.DataFrame([{"online_order": "No", "book_table": "Yes", "location": "Banashankari", 
                        "rest_type": "Casual Dining", "price": 1000, "meal_type": "Buffet"}])
predict(ET_model, predict_0)
93/43:
# encode_predict(df[:1])
# predict_0 = pd.DataFrame([{"online_order": "Yes", "book_table": "Yes", "location": "Banashankari", 
#                         "rest_type": "Casual Dining", "price": 2000, "meal_type": "Buffet"}])
# array([[   4.1419    , 1435.15866667]])
# predict_0 = pd.DataFrame([{"online_order": "No", "book_table": "Yes", "location": "Banashankari", 
#                         "rest_type": "Casual Dining", "price": 2000, "meal_type": "Buffet"}])
# array([[   4.2827, 1479.095 ]])2

predict_0 = pd.DataFrame([{"online_order": "No", "book_table": "Yes", "location": "Banashankari", 
                        "rest_type": "Cafe", "price": 1000, "meal_type": "Buffet"}])
predict(ET_model, predict_0)
93/44:
# encode_predict(df[:1])
# predict_0 = pd.DataFrame([{"online_order": "Yes", "book_table": "Yes", "location": "Banashankari", 
#                         "rest_type": "Casual Dining", "price": 2000, "meal_type": "Buffet"}])
# array([[   4.1419    , 1435.15866667]])
# predict_0 = pd.DataFrame([{"online_order": "No", "book_table": "Yes", "location": "Banashankari", 
#                         "rest_type": "Casual Dining", "price": 2000, "meal_type": "Buffet"}])
# array([[   4.2827, 1479.095 ]])2

predict_0 = pd.DataFrame([{"online_order": "Yes", "book_table": "Yes", "location": "Banashankari", 
                        "rest_type": "Cafe", "price": 1000, "meal_type": "Buffet"}])
predict(ET_model, predict_0)
93/45:
# encode_predict(df[:1])
# predict_0 = pd.DataFrame([{"online_order": "Yes", "book_table": "Yes", "location": "Banashankari", 
#                         "rest_type": "Casual Dining", "price": 2000, "meal_type": "Buffet"}])
# array([[   4.1419    , 1435.15866667]])
# predict_0 = pd.DataFrame([{"online_order": "No", "book_table": "Yes", "location": "Banashankari", 
#                         "rest_type": "Casual Dining", "price": 2000, "meal_type": "Buffet"}])
# array([[   4.2827, 1479.095 ]])2

predict_0 = pd.DataFrame([{"online_order": "Yes", "book_table": "No", "location": "Banashankari", 
                        "rest_type": "Cafe", "price": 1000, "meal_type": "Buffet"}])
predict(ET_model, predict_0)
93/46:
encode_predict(df[:1])
predict_0 = pd.DataFrame([{"online_order": "Yes", "book_table": "No", "location": "Banashankari", 
                        "rest_type": "Cafe", "price": 1000, "meal_type": "Buffet"}])
array([[  3.373, 215.185]])
predict_0 = pd.DataFrame([{"online_order": "Yes", "book_table": "Yes", "location": "Banashankari", 
                         "rest_type": "Casual Dining", "price": 2000, "meal_type": "Buffet"}])
array([[   4.1419    , 1435.15866667]])
predict_0 = pd.DataFrame([{"online_order": "No", "book_table": "Yes", "location": "Banashankari", 
                        "rest_type": "Casual Dining", "price": 2000, "meal_type": "Buffet"}])
array([[   4.2827, 1479.095 ]])

predict(ET_model, predict_0)
93/47:
# encode_predict(df[:1])
# predict_0 = pd.DataFrame([{"online_order": "Yes", "book_table": "No", "location": "Banashankari", 
#                         "rest_type": "Cafe", "price": 1000, "meal_type": "Buffet"}])
# array([[  3.373, 215.185]])
predict_0 = pd.DataFrame([{"online_order": "Yes", "book_table": "Yes", "location": "Banashankari", 
                         "rest_type": "Casual Dining", "price": 2000, "meal_type": "Buffet"}])
# array([[   4.1419    , 1435.15866667]])
# predict_0 = pd.DataFrame([{"online_order": "No", "book_table": "Yes", "location": "Banashankari", 
#                         "rest_type": "Casual Dining", "price": 2000, "meal_type": "Buffet"}])
# array([[   4.2827, 1479.095 ]])

predict(ET_model, predict_0)
93/48:
# encode_predict(df[:1])
# predict_0 = pd.DataFrame([{"online_order": "Yes", "book_table": "No", "location": "Banashankari", 
#                         "rest_type": "Cafe", "price": 1000, "meal_type": "Buffet"}])
# array([[  3.373, 215.185]])
# predict_0 = pd.DataFrame([{"online_order": "Yes", "book_table": "No", "location": "Banashankari", 
#                         "rest_type": "Cafe", "price": 1000, "meal_type": "Buffet"}])
predict_0 = pd.DataFrame([{"online_order": "Yes", "book_table": "Yes", "location": "Banashankari", 
                         "rest_type": "Casual Dining", "price": 1000, "meal_type": "Buffet"}])
# array([[   4.1419    , 1435.15866667]])
# predict_0 = pd.DataFrame([{"online_order": "No", "book_table": "Yes", "location": "Banashankari", 
#                         "rest_type": "Casual Dining", "price": 2000, "meal_type": "Buffet"}])
# array([[   4.2827, 1479.095 ]])

predict(ET_model, predict_0)
93/49:
# encode_predict(df[:1])
# predict_0 = pd.DataFrame([{"online_order": "Yes", "book_table": "No", "location": "Banashankari", 
#                         "rest_type": "Cafe", "price": 1000, "meal_type": "Buffet"}])
# array([[  3.373, 215.185]])
predict_0 = pd.DataFrame([{"online_order": "Yes", "book_table": "No", "location": "Banashankari", 
                        "rest_type": "Cafe", "price": 1000, "meal_type": "Buffet"}])
# predict_0 = pd.DataFrame([{"online_order": "Yes", "book_table": "Yes", "location": "Banashankari", 
#                          "rest_type": "Casual Dining", "price": 1000, "meal_type": "Buffet"}])
# array([[   4.189, 1919.065]])
# predict_0 = pd.DataFrame([{"online_order": "No", "book_table": "Yes", "location": "Banashankari", 
#                         "rest_type": "Casual Dining", "price": 2000, "meal_type": "Buffet"}])
# array([[   4.2827, 1479.095 ]])

predict(ET_model, predict_0)
93/50:
# encode_predict(df[:1])
# predict_0 = pd.DataFrame([{"online_order": "Yes", "book_table": "No", "location": "Banashankari", 
#                         "rest_type": "Cafe", "price": 1000, "meal_type": "Buffet"}])
# array([[  3.373, 215.185]])
predict_0 = pd.DataFrame([{"online_order": "Yes", "book_table": "Yes", "location": "Banashankari", 
                        "rest_type": "Cafe", "price": 1000, "meal_type": "Buffet"}])
# predict_0 = pd.DataFrame([{"online_order": "Yes", "book_table": "Yes", "location": "Banashankari", 
#                          "rest_type": "Casual Dining", "price": 1000, "meal_type": "Buffet"}])
# array([[   4.189, 1919.065]])
# predict_0 = pd.DataFrame([{"online_order": "No", "book_table": "Yes", "location": "Banashankari", 
#                         "rest_type": "Casual Dining", "price": 2000, "meal_type": "Buffet"}])
# array([[   4.2827, 1479.095 ]])

predict(ET_model, predict_0)
93/51:
# encode_predict(df[:1])
# predict_0 = pd.DataFrame([{"online_order": "Yes", "book_table": "No", "location": "Banashankari", 
#                         "rest_type": "Cafe", "price": 1000, "meal_type": "Buffet"}])
# array([[  3.373, 215.185]])
# predict_0 = pd.DataFrame([{"online_order": "Yes", "book_table": "Yes", "location": "Banashankari", 
#                         "rest_type": "Cafe", "price": 1000, "meal_type": "Buffet"}])
# array([[   4.153, 1079.47 ]])
# predict_0 = pd.DataFrame([{"online_order": "Yes", "book_table": "Yes", "location": "Banashankari", 
#                          "rest_type": "Casual Dining", "price": 1000, "meal_type": "Buffet"}])
# array([[   4.189, 1919.065]])
predict_0 = pd.DataFrame([{"online_order": "No", "book_table": "Yes", "location": "Banashankari", 
                        "rest_type": "Casual Dining", "price": 1000, "meal_type": "Buffet"}])
# array([[   4.2827, 1479.095 ]])

predict(ET_model, predict_0)
93/52:
# encode_predict(df[:1])
# predict_0 = pd.DataFrame([{"online_order": "Yes", "book_table": "No", "location": "Banashankari", 
#                         "rest_type": "Cafe", "price": 1000, "meal_type": "Buffet"}])
# array([[  3.373, 215.185]])
# predict_0 = pd.DataFrame([{"online_order": "Yes", "book_table": "Yes", "location": "Banashankari", 
#                         "rest_type": "Cafe", "price": 1000, "meal_type": "Buffet"}])
# array([[   4.153, 1079.47 ]])
# predict_0 = pd.DataFrame([{"online_order": "Yes", "book_table": "Yes", "location": "Banashankari", 
#                          "rest_type": "Casual Dining", "price": 1000, "meal_type": "Buffet"}])
# array([[   4.189, 1919.065]])
predict_0 = pd.DataFrame([{"online_order": "No", "book_table": "Yes", "location": "Banashankari", 
                        "rest_type": "Cafe", "price": 1000, "meal_type": "Buffet"}])
# array([[   4.2827, 1479.095 ]])

predict(ET_model, predict_0)
93/53:
# encode_predict(df[:1])
# predict_0 = pd.DataFrame([{"online_order": "Yes", "book_table": "No", "location": "Banashankari", 
#                         "rest_type": "Cafe", "price": 1000, "meal_type": "Buffet"}])
# array([[  3.373, 215.185]])
# predict_0 = pd.DataFrame([{"online_order": "Yes", "book_table": "Yes", "location": "Banashankari", 
#                         "rest_type": "Cafe", "price": 1000, "meal_type": "Buffet"}])
# array([[   4.153, 1079.47 ]])
predict_0 = pd.DataFrame([{"online_order": "No", "book_table": "Yes", "location": "Banashankari", 
                         "rest_type": "Casual Dining", "price": 1000, "meal_type": "Buffet"}])
# array([[   4.189, 1919.065]])
predict(ET_model, predict_0)
93/54:
# encode_predict(df[:1])
# predict_0 = pd.DataFrame([{"online_order": "Yes", "book_table": "No", "location": "Banashankari", 
#                         "rest_type": "Cafe", "price": 1000, "meal_type": "Buffet"}])
# array([[  3.373, 215.185]])
# predict_0 = pd.DataFrame([{"online_order": "Yes", "book_table": "Yes", "location": "Banashankari", 
#                         "rest_type": "Cafe", "price": 1000, "meal_type": "Buffet"}])
# array([[   4.153, 1079.47 ]])
predict_0 = pd.DataFrame([{"online_order": "Yes", "book_table": "Yes", "location": "Banashankari", 
                         "rest_type": "Casual Dining", "price": 1000, "meal_type": "Buffet"}])
# array([[   4.189, 1919.065]])
predict(ET_model, predict_0)
93/55:
predict_0 = pd.DataFrame([{"online_order": "Yes", "book_table": "No", "location": "Banashankari", 
                        "rest_type": "Cafe", "price": 1000, "meal_type": "Buffet"}])
predict(ET_model, predict_0)
93/56:
predict_0 = pd.DataFrame([{"online_order": "Yes", "book_table": "Yes", "location": "Banashankari", 
                        "rest_type": "Cafe", "price": 1000, "meal_type": "Buffet"}])
predict(ET_model, predict_0)
93/57:
predict_0 = pd.DataFrame([{"online_order": "Yes", "book_table": "Yes", "location": "Banashankari", 
                         "rest_type": "Casual Dining", "price": 1000, "meal_type": "Buffet"}])
predict(ET_model, predict_0)
93/58: plot_price_dist()
93/59: generate_count_heatmap()
93/60: generate_avg_heatmap()
93/61: generate_med_heatmap()
93/62: plot_price_dist()
93/63: plot_rate_dist()
93/64: plot_rate_dist()
93/65: plot_votes_dist()
93/66: df.describe()
93/67: generate_avg_heatmap()
93/68:
def generate_avg_price_heatmap():
    avg_price = dict(df.groupby(['location'])['price'].mean())
    heatmap_avg = heatmap_generate(lat_lon_location, avg_price, need_clustering=True)
    return heatmap_avg
93/69:
def generate_med_price_heatmap():
    med_price = dict(df.groupby(['location'])['price'].median())
    heatmap_med = heatmap_generate(lat_lon_location, med_price, need_clustering=True)
    return heatmap_med
93/70: generate_avg_price_heatmap()
93/71: generate_med_price_heatmap()
93/72: df["price"].describe()
93/73: generate_count_heatmap()
93/74: generate_count_heatmap()
93/75: generate_avg_price_heatmap()
93/76:
def generate_avg_price_heatmap():
    avg_price = dict(df.groupby(['location'])['price'].max())
    heatmap_avg = heatmap_generate(lat_lon_location, avg_price, need_clustering=True)
    return heatmap_avg
93/77: generate_avg_price_heatmap()
93/78: generate_med_price_heatmap()
93/79:
def heatmap_generate(lat_lon_location, location_info, need_clustering=False):
    map_info = []
    for location in location_info:
        if not np.isnan(lat_lon_location[location][0]):
            map_info.append(list(lat_lon_location[location]) + [float(location_info[location]) ** 2])
    base_map = Map(location=[12.9716, 77.5946], zoom_start=11)
    HeatMap(map_info ,zoom=20,radius=15).add_to(base_map)
    if need_clustering:
        FastMarkerCluster(map_info).add_to(base_map)
    return base_map
93/80: generate_count_heatmap()
93/81: generate_avg_price_heatmap()
93/82: generate_avg_price_heatmap()
93/83: generate_med_price_heatmap()
93/84: dict(df['location'].value_counts())
93/85: df['location'].value_counts().max()
93/86: df['location'].value_counts()
93/87: df.groupby(['location'])['price'].mean()
93/88: df.groupby(['location'])['price'].mean().sort_values(["price"])
93/89: df.groupby(['location'])['price'].mean().max()
93/90: df.groupby(['location'])['price'].mean()
93/91: pd.DataFrame(df.groupby(['location'])['price'].mean())
93/92: pd.DataFrame(df.groupby(['location'])['price'].mean()).sort_values(["price"])
93/93:
def generate_avg_price_heatmap():
    avg_price = dict(df.groupby(['location'])['price'].max())
    heatmap_avg = heatmap_generate(lat_lon_location, avg_price, need_clustering=True)
    return heatmap_avg
93/94: generate_avg_price_heatmap()
93/95:
def generate_avg_price_heatmap():
    avg_price = dict(df.groupby(['location'])['price'].mean())
    heatmap_avg = heatmap_generate(lat_lon_location, avg_price, need_clustering=True)
    return heatmap_avg
93/96: pd.DataFrame(df.groupby(['location'])['price'].mean()).sort_values(["price"])
93/97: generate_avg_price_heatmap()
93/98: generate_med_price_heatmap()
93/99: generate_avg_price_heatmap()
93/100:
def generate_avg_price_heatmap():
    avg_price = dict(df.groupby(['location'])['price'].mean())
    heatmap_avg = heatmap_generate(lat_lon_location, avg_price, need_clustering=True)
    return heatmap_avg
93/101:
def generate_med_price_heatmap():
    med_price = dict(df.groupby(['location'])['price'].median())
    heatmap_med = heatmap_generate(lat_lon_location, med_price, need_clustering=True)
    return heatmap_med
93/102: generate_avg_price_heatmap()
93/103: generate_med_price_heatmap()
93/104: pd.DataFrame(df.groupby(['location'])['price'].median()).sort_values(["price"])
93/105: pd.DataFrame(df['location'].value_counts()).sort_values(["price"])
93/106: pd.DataFrame(df['location'].value_counts())
93/107: df['location'].value_counts()[:10]
93/108: df[df["price"] <= 1000]
93/109: df[df["price"] <= 1000].count()
93/110: df[df["price"] <= 1000].count()/ df.count()
93/111: df[1000< df["price"] <= 2000].count()/ df.count()
93/112: df[1000< df["price"] &  df["price"]<= 2000].count()/ df.count()
93/113: df[(1000 < df["price"]) &  (df["price"] <= 2000)].count()/ df.count()
93/114:
predict_0 = pd.DataFrame([{"online_order": "Yes", "book_table": "No", "location": "BTM", 
                        "rest_type": "Cafe", "price": 1000, "meal_type": "Buffet"}])
predict(ET_model, predict_0)
93/115:
predict_0 = pd.DataFrame([{"online_order": "Yes", "book_table": "Yes", "location": "BTM", 
                         "rest_type": "Casual Dining", "price": 1000, "meal_type": "Buffet"}])
predict(ET_model, predict_0)
93/116:
predict_0 = pd.DataFrame([{"online_order": "Yes", "book_table": "Yes", "location": "BTM", 
                        "rest_type": "Cafe", "price": 1000, "meal_type": "Buffet"}])
predict(ET_model, predict_0)
93/117:
predict_0 = pd.DataFrame([{"online_order": "Yes", "book_table": "No", "location": "BTM", 
                        "rest_type": "Cafe", "price": 1000, "meal_type": "Buffet"}])
predict(ET_model, predict_0)
93/118:
predict_0 = pd.DataFrame([{"online_order": "Yes", "book_table": "No", "location": "BTM", 
                        "rest_type": "Cafe", "price": 1000, "meal_type": "Buffet"}])
predict(ET_model, predict_0)
93/119:
predict_0 = pd.DataFrame([{"online_order": "No", "book_table": "Yes", "location": "BTM", 
                         "rest_type": "Casual Dining", "price": 1000, "meal_type": "Buffet"}])
predict(ET_model, predict_0)
93/120:
predict_0 = pd.DataFrame([{"online_order": "No", "book_table": "Yes", "location": "BTM", 
                         "rest_type": "Cafe", "price": 1000, "meal_type": "Buffet"}])
predict(ET_model, predict_0)
93/121:
predict_0 = pd.DataFrame([{"online_order": "Yes", "book_table": "Yes", "location": "BTM", 
                         "rest_type": "Cafe", "price": 1000, "meal_type": "Buffet"}])
predict(ET_model, predict_0)
93/122:
predict_0 = pd.DataFrame([{"online_order": "No", "book_table": "Yes", "location": "BTM", 
                        "rest_type": "Cafe", "price": 1000, "meal_type": "Buffet"}])
predict(ET_model, predict_0)
93/123: generate_med_price_heatmap()
93/124: generate_avg_price_heatmap()
93/125:
#### _Data analysis part IV_

_The following code divide dataframe into n_cluster different clusters._
93/126: plot_rate_votes_cluster(n_cluster=4)
93/127: plot_rate_price_cluster(n_cluster=4)
93/128: plot_votes_price_cluster(n_cluster=5)
93/129: plot_votes_price_cluster(n_cluster=4)
93/130: plot_rate_votes_cluster(n_cluster=5)
93/131: plot_rate_price_cluster(n_cluster=5)
93/132: plot_votes_price_cluster(n_cluster=5)
93/133:
predict_0 = pd.DataFrame([{"online_order": "Yes", "book_table": "No", "location": "BTM", 
                        "rest_type": "Cafe", "price": 1500, "meal_type": "Buffet"}])
predict(ET_model, predict_0)
93/134:
predict_0 = pd.DataFrame([{"online_order": "No", "book_table": "Yes", "location": "BTM", 
                        "rest_type": "Cafe", "price": 1000, "meal_type": "Buffet"}])
predict(ET_model, predict_0)
93/135:
predict_0 = pd.DataFrame([{"online_order": "No", "book_table": "Yes", "location": "BTM", 
                        "rest_type": "Cafe", "price": 1500, "meal_type": "Buffet"}])
predict(ET_model, predict_0)
93/136:
predict_0 = pd.DataFrame([{"online_order": "Yes", "book_table": "Yes", "location": "BTM", 
                         "rest_type": "Cafe", "price": 1500, "meal_type": "Buffet"}])
predict(ET_model, predict_0)
93/137:
predict_0 = pd.DataFrame([{"online_order": "Yes", "book_table": "No", "location": "BTM", 
                        "rest_type": "Cafe", "price": 1000, "meal_type": "Buffet"}])
predict(ET_model, predict_0)
93/138:
predict_0 = pd.DataFrame([{"online_order": "No", "book_table": "Yes", "location": "BTM", 
                        "rest_type": "Cafe", "price": 1000, "meal_type": "Buffet"}])
predict(ET_model, predict_0)
93/139:
predict_0 = pd.DataFrame([{"online_order": "Yes", "book_table": "Yes", "location": "BTM", 
                         "rest_type": "Cafe", "price": 1000, "meal_type": "Buffet"}])
predict(ET_model, predict_0)
93/140: generate_count_heatmap()
93/141: generate_avg_price_heatmap()
93/142: generate_med_price_heatmap()
94/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from folium import Map
from folium.plugins import HeatMap, FastMarkerCluster
from geopy.geocoders import Nominatim
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import  ExtraTreesRegressor
from sklearn.cluster import KMeans
94/2:
def read_data():
    df = pd.read_csv("zomato.csv")
    return df


original_df = read_data()
original_df.head()
94/3:
df = original_df.copy()

def drop_unnecessary_columns(df):
    df.drop(columns=['dish_liked', 'cuisines', 'menu_item'], inplace=True)
    df.drop(columns=['name', 'url', 'phone', 'reviews_list', "listed_in(city)"], inplace=True)
    return df


df = drop_unnecessary_columns(df)
df.info()
94/4:
def rename_features(df):
    df.rename(columns={'approx_cost(for two people)': 'price', 'listed_in(city)': 'city_area',
                      'listed_in(type)': 'meal_type'}, inplace=True)
    return df


df = rename_features(df)
df.columns
94/5:
def remove_nan(df):
    df["rate"] = df["rate"].replace("NEW", np.nan)
    df["rate"] = df["rate"].replace("-", np.nan)
    df.dropna(how='any',inplace=True)
    return df


df = remove_nan(df)
df.info()
94/6:
df.drop_duplicates(inplace=True)
df.info()
94/7:
def drop_address(df):
    df.drop(columns=["address"], inplace=True)
    return df


df = drop_address(df)
94/8:
def transform_rate(df):
    rates = []
    for _, row in df.iterrows():
        rate_in_float = float(row["rate"].split("/")[0])
        rates.append(rate_in_float)
    df["rate"] = rates
    return df


def transform_price(df):
    prices = []
    for _, row in df.iterrows():
        price_s = row["price"]
        price_s = price_s.replace(",", "")
        price = int(price_s)
        prices.append(price)
    df["price"] = prices
    return df


df = transform_rate(df)
df = transform_price(df)

df[["rate", "price"]].head()
94/9:
def get_geo_info_location(location):
    geo_info = geolocator.geocode(location)
    if geo_info is None:
        return np.nan, np.nan
    return geo_info.latitude, geo_info.longitude


def get_location_coordinate(df):
    all_location = df["location"].unique()
    coordinate = dict()
    for location in all_location:
        coordinate[location] = get_geo_info_location(location + ", Bangalore")
    return coordinate


geolocator=Nominatim(user_agent="app")   
lat_lon_location = get_location_coordinate(df)
lat_lon_location
94/10:
def Encode(df):
    column_dict = dict()
    for column in df.columns[~df.columns.isin(['rate', 'price', 'votes'])]:
        df[column], column_dict[column] = df[column].factorize()
    return df, column_dict


df_float, column_dict = Encode(df.copy())
df_float.head()
94/11:
def encode_predict(df):
    df_encode = df.copy()
    for column in df.columns[~df.columns.isin(['rate', 'price', 'votes'])]:
        index = list(column_dict[column]).index(df[column].item())
        df_encode[column] = index
    return df_encode


df_example = df[:1].iloc[:,[0, 1, 4, 5, 6, 7]]
df_exm_encode = encode_predict(df_example)
94/12:
def get_train_test_data(df_float):
    x = df_float.iloc[:,[0, 1, 4, 5, 6, 7]]
    y = df_float.iloc[:,[2, 3]]
    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.1, random_state=111)
    return x_train, x_test, y_train, y_test


x_train, x_test, y_train, y_test = get_train_test_data(df_float)
x_train.head()
94/13:
def train_decision_tree(x_train, y_train):
    decision_tree = DecisionTreeRegressor()
    decision_tree.fit(x_train, y_train)
    return decision_tree


DT_model = train_decision_tree(x_train, y_train)
94/14:
def train_linear_regression(x_train, y_train):
    linear_regression = LinearRegression()
    linear_regression.fit(x_train, y_train)
    return linear_regression


LR_model = train_linear_regression(x_train, y_train)
94/15:
def train_random_forest(x_train, y_train):
    random_forest = RandomForestRegressor(n_estimators=500, min_samples_leaf=.0001)
    random_forest.fit(x_train, y_train)
    return random_forest


RF_model = train_random_forest(x_train, y_train)
94/16:
def train_extra_tree(x_train, y_train):
    extra_tree = ExtraTreesRegressor(n_estimators = 100)
    extra_tree.fit(x_train, y_train)
    return extra_tree


ET_model = train_extra_tree(x_train, y_train)
94/17:
def test_model(model, x_test, y_test):
    y_predict=model.predict(x_test)
    score = r2_score(y_test, y_predict)
    return score
94/18:
RF_score = test_model(RF_model, x_test, y_test)
DT_score = test_model(DT_model, x_test, y_test)
LR_score = test_model(LR_model, x_test, y_test)
ET_score = test_model(ET_model, x_test, y_test)


evaluation = pd.DataFrame({"Model": ["Random Forest", "Decision Tree", "Linear Regression", "Extra Trees"], 
                           "R^2 Score": [RF_score, DT_score, LR_score, ET_score]})

evaluation.sort_values(by = 'R^2 Score', ascending = False, ignore_index=True)
94/19:
def success_cluster(n_cluster=5):
    df_cluster = df.copy()
    kmean = KMeans(n_clusters=n_cluster, random_state=100).fit(df_cluster[["rate", "votes"]])
    df_cluster["cluster"] = kmean.labels_
    return df_cluster
94/20: df.describe()
94/21:
def predict(model, restaurant_plan):
    plan_encode = encode_predict(restaurant_plan)
    return model.predict(plan_encode)
94/22: df["price"].describe()
94/23: df['location'].value_counts()[:10]
94/24: pd.DataFrame(df.groupby(['location'])['price'].mean()).sort_values(["price"])
94/25: pd.DataFrame(df.groupby(['location'])['price'].median()).sort_values(["price"])
94/26: df[(1000 < df["price"]) &  (df["price"] <= 2000)].count()/ df.count()
94/27:
def heatmap_generate(lat_lon_location, location_info, need_clustering=False):
    map_info = []
    for location in location_info:
        if not np.isnan(lat_lon_location[location][0]):
            map_info.append(list(lat_lon_location[location]) + [float(location_info[location]) ** 2])
    base_map = Map(location=[12.9716, 77.5946], zoom_start=11)
    HeatMap(map_info ,zoom=20,radius=15).add_to(base_map)
    if need_clustering:
        FastMarkerCluster(map_info).add_to(base_map)
    return base_map
94/28:
def generate_count_heatmap():
    location_count = dict(df['location'].value_counts())
    heatmap_count = heatmap_generate(lat_lon_location, location_count)
    return heatmap_count
94/29:
def generate_avg_price_heatmap():
    avg_price = dict(df.groupby(['location'])['price'].mean())
    heatmap_avg = heatmap_generate(lat_lon_location, avg_price, need_clustering=True)
    return heatmap_avg
94/30:
def generate_med_price_heatmap():
    med_price = dict(df.groupby(['location'])['price'].median())
    heatmap_med = heatmap_generate(lat_lon_location, med_price, need_clustering=True)
    return heatmap_med
94/31:
def plot_price_dist():
    fig, ax = plt.subplots(figsize=[10,4])
    sns.distplot(df["price"],ax=ax)
94/32:
def plot_rate_dist():
    fig, ax = plt.subplots(figsize=[10,4])
    sns.distplot(df["rate"],ax=ax)
94/33:
def plot_votes_dist():
    fig, ax = plt.subplots(figsize=[12,4])
    sns.distplot(df["votes"],ax=ax)
94/34:
def plot_rate_votes_cluster(n_cluster=5):
    df_cluster = success_cluster(n_cluster)
    colors = ["g", "r", "b", "c", "m", "y"]
    
    plt.figure(figsize = (8,5))
    
    for i in range(n_cluster):
        selected = df_cluster[df_cluster["cluster"] == i]
        plt.plot(selected["rate"], selected["votes"], "ob", color=colors[i])
    plt.show()
94/35:
def plot_rate_price_cluster(n_cluster=5):
    df_cluster = success_cluster(n_cluster)
    colors = ["g", "r", "b", "c", "m", "y"]
    plt.figure(figsize = (8,5))
    for i in range(n_cluster):
        selected = df_cluster[df_cluster["cluster"] == i]
        plt.plot(selected["rate"], selected["price"], "ob", color=colors[i])
    plt.show()
94/36:
def plot_votes_price_cluster(n_cluster=5):
    df_cluster = success_cluster(n_cluster)
    colors = ["g", "r", "b", "c", "m", "y"]
    plt.figure(figsize = (8,5))
    for i in range(n_cluster):
        selected = df_cluster[df_cluster["cluster"] == i]
        plt.plot(selected["votes"], selected["price"], "ob", color=colors[i])
    plt.show()
94/37: plot_price_dist()
94/38: plot_rate_votes_cluster(n_cluster=5)
94/39: plot_rate_price_cluster(n_cluster=5)
94/40: plot_votes_price_cluster(n_cluster=5)
94/41: generate_count_heatmap()
94/42: generate_avg_price_heatmap()
94/43: generate_med_price_heatmap()
94/44:
predict_0 = pd.DataFrame([{"online_order": "Yes", "book_table": "No", "location": "BTM", 
                        "rest_type": "Cafe", "price": 1000, "meal_type": "Buffet"}])
predict(ET_model, predict_0)
94/45:
predict_0 = pd.DataFrame([{"online_order": "No", "book_table": "Yes", "location": "BTM", 
                        "rest_type": "Cafe", "price": 1000, "meal_type": "Buffet"}])
predict(ET_model, predict_0)
94/46:
predict_0 = pd.DataFrame([{"online_order": "Yes", "book_table": "Yes", "location": "BTM", 
                         "rest_type": "Cafe", "price": 1000, "meal_type": "Buffet"}])
predict(ET_model, predict_0)
94/47:
def plot_rate_votes_cluster(n_cluster=5):
    df_cluster = success_cluster(n_cluster)
    colors = ["g", "r", "b", "c", "m", "y"]
    
    plt.figure(figsize = (8,5))
    
    for i in range(n_cluster):
        selected = df_cluster[df_cluster["cluster"] == i]
        plt.plot(selected["rate"], selected["votes"], "ob", color=colors[i])
    plt.xlabel("rate")
    plt.ylabel("votes")
    plt.title("Figure 2")
    plt.show()
94/48:
def plot_rate_price_cluster(n_cluster=5):
    df_cluster = success_cluster(n_cluster)
    colors = ["g", "r", "b", "c", "m", "y"]
    plt.figure(figsize = (8,5))
    for i in range(n_cluster):
        selected = df_cluster[df_cluster["cluster"] == i]
        plt.plot(selected["rate"], selected["price"], "ob", color=colors[i])
    plt.xlabel("rate")
    plt.ylabel("price")
    plt.title("Figure 3")
    plt.show()
94/49:
def plot_votes_price_cluster(n_cluster=5):
    df_cluster = success_cluster(n_cluster)
    colors = ["g", "r", "b", "c", "m", "y"]
    plt.figure(figsize = (8,5))
    for i in range(n_cluster):
        selected = df_cluster[df_cluster["cluster"] == i]
        plt.plot(selected["votes"], selected["price"], "ob", color=colors[i])
    plt.xlabel("votes")
    plt.ylabel("price")
    plt.title("Figure 4")
    plt.show()
94/50: plot_rate_votes_cluster(n_cluster=5)
94/51: plot_rate_price_cluster(n_cluster=5)
94/52: plot_votes_price_cluster(n_cluster=5)
94/53:
def plot_price_dist():
    fig, ax = plt.subplots(figsize=[10,4])
    sns.distplot(df["price"], ax=ax, name="price of restaurant ").set_title("Figure 1")
94/54: plot_price_dist()
94/55:
def plot_price_dist():
    fig, ax = plt.subplots(figsize=[10,4])
    sns.distplot(df["price"], ax=ax, axlabel="price of restaurant ").set_title("Figure 1")
94/56: plot_price_dist()
94/57: generate_count_heatmap()
94/58: df[df["price"] <= 1000].count()/ df.count()
95/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from folium import Map
from folium.plugins import HeatMap, FastMarkerCluster
from geopy.geocoders import Nominatim
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import  ExtraTreesRegressor
from sklearn.cluster import KMeans
95/2:
def read_data():
    df = pd.read_csv("zomato.csv")
    return df


original_df = read_data()
original_df.head()
95/3:
df = original_df.copy()

def drop_unnecessary_columns(df):
    df.drop(columns=['dish_liked', 'cuisines', 'menu_item'], inplace=True)
    df.drop(columns=['name', 'url', 'phone', 'reviews_list', "listed_in(city)"], inplace=True)
    return df


df = drop_unnecessary_columns(df)
df.info()
95/4:
def rename_features(df):
    df.rename(columns={'approx_cost(for two people)': 'price', 'listed_in(city)': 'city_area',
                      'listed_in(type)': 'meal_type'}, inplace=True)
    return df


df = rename_features(df)
df.columns
95/5:
def remove_nan(df):
    df["rate"] = df["rate"].replace("NEW", np.nan)
    df["rate"] = df["rate"].replace("-", np.nan)
    df.dropna(how='any',inplace=True)
    return df


df = remove_nan(df)
df.info()
95/6:
df.drop_duplicates(inplace=True)
df.info()
95/7:
def drop_address(df):
    df.drop(columns=["address"], inplace=True)
    return df


df = drop_address(df)
95/8:
def transform_rate(df):
    rates = []
    for _, row in df.iterrows():
        rate_in_float = float(row["rate"].split("/")[0])
        rates.append(rate_in_float)
    df["rate"] = rates
    return df


def transform_price(df):
    prices = []
    for _, row in df.iterrows():
        price_s = row["price"]
        price_s = price_s.replace(",", "")
        price = int(price_s)
        prices.append(price)
    df["price"] = prices
    return df


df = transform_rate(df)
df = transform_price(df)

df[["rate", "price"]].head()
95/9:
def get_geo_info_location(location):
    geo_info = geolocator.geocode(location)
    if geo_info is None:
        return np.nan, np.nan
    return geo_info.latitude, geo_info.longitude


def get_location_coordinate(df):
    all_location = df["location"].unique()
    coordinate = dict()
    for location in all_location:
        coordinate[location] = get_geo_info_location(location + ", Bangalore")
    return coordinate


geolocator=Nominatim(user_agent="app")   
lat_lon_location = get_location_coordinate(df)
lat_lon_location
95/10:
def Encode(df):
    column_dict = dict()
    for column in df.columns[~df.columns.isin(['rate', 'price', 'votes'])]:
        df[column], column_dict[column] = df[column].factorize()
    return df, column_dict


df_float, column_dict = Encode(df.copy())
df_float.head()
95/11:
def encode_predict(df):
    df_encode = df.copy()
    for column in df.columns[~df.columns.isin(['rate', 'price', 'votes'])]:
        index = list(column_dict[column]).index(df[column].item())
        df_encode[column] = index
    return df_encode


df_example = df[:1].iloc[:,[0, 1, 4, 5, 6, 7]]
df_exm_encode = encode_predict(df_example)
95/12:
def get_train_test_data(df_float):
    x = df_float.iloc[:,[0, 1, 4, 5, 6, 7]]
    y = df_float.iloc[:,[2, 3]]
    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.1, random_state=111)
    return x_train, x_test, y_train, y_test


x_train, x_test, y_train, y_test = get_train_test_data(df_float)
x_train.head()
95/13:
def train_decision_tree(x_train, y_train):
    decision_tree = DecisionTreeRegressor()
    decision_tree.fit(x_train, y_train)
    return decision_tree


DT_model = train_decision_tree(x_train, y_train)
95/14:
def train_linear_regression(x_train, y_train):
    linear_regression = LinearRegression()
    linear_regression.fit(x_train, y_train)
    return linear_regression


LR_model = train_linear_regression(x_train, y_train)
95/15:
def train_random_forest(x_train, y_train):
    random_forest = RandomForestRegressor(n_estimators=500, min_samples_leaf=.0001)
    random_forest.fit(x_train, y_train)
    return random_forest


RF_model = train_random_forest(x_train, y_train)
95/16:
def train_extra_tree(x_train, y_train):
    extra_tree = ExtraTreesRegressor(n_estimators = 100)
    extra_tree.fit(x_train, y_train)
    return extra_tree


ET_model = train_extra_tree(x_train, y_train)
95/17:
def test_model(model, x_test, y_test):
    y_predict=model.predict(x_test)
    score = r2_score(y_test, y_predict)
    return score
95/18:
RF_score = test_model(RF_model, x_test, y_test)
DT_score = test_model(DT_model, x_test, y_test)
LR_score = test_model(LR_model, x_test, y_test)
ET_score = test_model(ET_model, x_test, y_test)


evaluation = pd.DataFrame({"Model": ["Random Forest", "Decision Tree", "Linear Regression", "Extra Trees"], 
                           "R^2 Score": [RF_score, DT_score, LR_score, ET_score]})

evaluation.sort_values(by = 'R^2 Score', ascending = False, ignore_index=True)
95/19:
def success_cluster(n_cluster=5):
    df_cluster = df.copy()
    kmean = KMeans(n_clusters=n_cluster, random_state=100).fit(df_cluster[["rate", "votes"]])
    df_cluster["cluster"] = kmean.labels_
    return df_cluster
95/20: df.describe()
95/21:
def predict(model, restaurant_plan):
    plan_encode = encode_predict(restaurant_plan)
    return model.predict(plan_encode)
95/22: df["price"].describe()
95/23: df['location'].value_counts()[:10]
95/24: pd.DataFrame(df.groupby(['location'])['price'].mean()).sort_values(["price"])
95/25: pd.DataFrame(df.groupby(['location'])['price'].median()).sort_values(["price"])
95/26: df[df["price"] <= 1000].count()/ df.count()
95/27: df[(1000 < df["price"]) &  (df["price"] <= 2000)].count()/ df.count()
95/28:
def heatmap_generate(lat_lon_location, location_info, need_clustering=False):
    map_info = []
    for location in location_info:
        if not np.isnan(lat_lon_location[location][0]):
            map_info.append(list(lat_lon_location[location]) + [float(location_info[location]) ** 2])
    base_map = Map(location=[12.9716, 77.5946], zoom_start=11)
    HeatMap(map_info ,zoom=20,radius=15).add_to(base_map)
    if need_clustering:
        FastMarkerCluster(map_info).add_to(base_map)
    return base_map
95/29:
def generate_count_heatmap():
    location_count = dict(df['location'].value_counts())
    heatmap_count = heatmap_generate(lat_lon_location, location_count)
    return heatmap_count
95/30:
def generate_avg_price_heatmap():
    avg_price = dict(df.groupby(['location'])['price'].mean())
    heatmap_avg = heatmap_generate(lat_lon_location, avg_price, need_clustering=True)
    return heatmap_avg
95/31:
def generate_med_price_heatmap():
    med_price = dict(df.groupby(['location'])['price'].median())
    heatmap_med = heatmap_generate(lat_lon_location, med_price, need_clustering=True)
    return heatmap_med
95/32:
def plot_price_dist():
    fig, ax = plt.subplots(figsize=[10,4])
    sns.distplot(df["price"], ax=ax, axlabel="price of restaurant ").set_title("Figure 1")
95/33:
def plot_rate_votes_cluster(n_cluster=5):
    df_cluster = success_cluster(n_cluster)
    colors = ["g", "r", "b", "c", "m", "y"]
    
    plt.figure(figsize = (8,5))
    
    for i in range(n_cluster):
        selected = df_cluster[df_cluster["cluster"] == i]
        plt.plot(selected["rate"], selected["votes"], "ob", color=colors[i])
    plt.xlabel("rate")
    plt.ylabel("votes")#### _Visualisation --- heat map_

The following cells produce several heat maps based on different information to be used later in Objective 1
    plt.title("Figure 2")
    plt.show()
96/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from folium import Map
from folium.plugins import HeatMap, FastMarkerCluster
from geopy.geocoders import Nominatim
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import  ExtraTreesRegressor
from sklearn.cluster import KMeans
96/2:
def read_data():
    df = pd.read_csv("zomato.csv")
    return df


original_df = read_data()
original_df.head()
96/3:
df = original_df.copy()

def drop_unnecessary_columns(df):
    df.drop(columns=['dish_liked', 'cuisines', 'menu_item'], inplace=True)
    df.drop(columns=['name', 'url', 'phone', 'reviews_list', "listed_in(city)"], inplace=True)
    return df


df = drop_unnecessary_columns(df)
df.info()
96/4:
def rename_features(df):
    df.rename(columns={'approx_cost(for two people)': 'price', 'listed_in(city)': 'city_area',
                      'listed_in(type)': 'meal_type'}, inplace=True)
    return df


df = rename_features(df)
df.columns
96/5:
def remove_nan(df):
    df["rate"] = df["rate"].replace("NEW", np.nan)
    df["rate"] = df["rate"].replace("-", np.nan)
    df.dropna(how='any',inplace=True)
    return df


df = remove_nan(df)
df.info()
96/6:
df.drop_duplicates(inplace=True)
df.info()
96/7:
def drop_address(df):
    df.drop(columns=["address"], inplace=True)
    return df


df = drop_address(df)
96/8:
def transform_rate(df):
    rates = []
    for _, row in df.iterrows():
        rate_in_float = float(row["rate"].split("/")[0])
        rates.append(rate_in_float)
    df["rate"] = rates
    return df


def transform_price(df):
    prices = []
    for _, row in df.iterrows():
        price_s = row["price"]
        price_s = price_s.replace(",", "")
        price = int(price_s)
        prices.append(price)
    df["price"] = prices
    return df


df = transform_rate(df)
df = transform_price(df)

df[["rate", "price"]].head()
96/9:
def get_geo_info_location(location):
    geo_info = geolocator.geocode(location)
    if geo_info is None:
        return np.nan, np.nan
    return geo_info.latitude, geo_info.longitude


def get_location_coordinate(df):
    all_location = df["location"].unique()
    coordinate = dict()
    for location in all_location:
        coordinate[location] = get_geo_info_location(location + ", Bangalore")
    return coordinate


geolocator=Nominatim(user_agent="app")   
lat_lon_location = get_location_coordinate(df)
lat_lon_location
97/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from folium import Map
from folium.plugins import HeatMap, FastMarkerCluster
from geopy.geocoders import Nominatim
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import  ExtraTreesRegressor
from sklearn.cluster import KMeans
97/2:
def read_data():
    df = pd.read_csv("zomato.csv")
    return df


original_df = read_data()
original_df.head()
97/3:
df = original_df.copy()

def drop_unnecessary_columns(df):
    df.drop(columns=['dish_liked', 'cuisines', 'menu_item'], inplace=True)
    df.drop(columns=['name', 'url', 'phone', 'reviews_list', "listed_in(city)"], inplace=True)
    return df


df = drop_unnecessary_columns(df)
df.info()
97/4:
def rename_features(df):
    df.rename(columns={'approx_cost(for two people)': 'price', 'listed_in(city)': 'city_area',
                      'listed_in(type)': 'meal_type'}, inplace=True)
    return df


df = rename_features(df)
df.columns
97/5:
def remove_nan(df):
    df["rate"] = df["rate"].replace("NEW", np.nan)
    df["rate"] = df["rate"].replace("-", np.nan)
    df.dropna(how='any',inplace=True)
    return df


df = remove_nan(df)
df.info()
97/6:
df.drop_duplicates(inplace=True)
df.info()
97/7:
def drop_address(df):
    df.drop(columns=["address"], inplace=True)
    return df


df = drop_address(df)
97/8:
def transform_rate(df):
    rates = []
    for _, row in df.iterrows():
        rate_in_float = float(row["rate"].split("/")[0])
        rates.append(rate_in_float)
    df["rate"] = rates
    return df


def transform_price(df):
    prices = []
    for _, row in df.iterrows():
        price_s = row["price"]
        price_s = price_s.replace(",", "")
        price = int(price_s)
        prices.append(price)
    df["price"] = prices
    return df


df = transform_rate(df)
df = transform_price(df)

df[["rate", "price"]].head()
97/9:
def get_geo_info_location(location):
    geo_info = geolocator.geocode(location)
    if geo_info is None:
        return np.nan, np.nan
    return geo_info.latitude, geo_info.longitude


def get_location_coordinate(df):
    all_location = df["location"].unique()
    coordinate = dict()
    for location in all_location:
        coordinate[location] = get_geo_info_location(location + ", Bangalore")
    return coordinate


geolocator=Nominatim(user_agent="app")   
lat_lon_location = get_location_coordinate(df)
lat_lon_location
97/10:
def Encode(df):
    column_dict = dict()
    for column in df.columns[~df.columns.isin(['rate', 'price', 'votes'])]:
        df[column], column_dict[column] = df[column].factorize()
    return df, column_dict


df_float, column_dict = Encode(df.copy())
df_float.head()
97/11:
def encode_predict(df):
    df_encode = df.copy()
    for column in df.columns[~df.columns.isin(['rate', 'price', 'votes'])]:
        index = list(column_dict[column]).index(df[column].item())
        df_encode[column] = index
    return df_encode


df_example = df[:1].iloc[:,[0, 1, 4, 5, 6, 7]]
df_exm_encode = encode_predict(df_example)
97/12:
def get_train_test_data(df_float):
    x = df_float.iloc[:,[0, 1, 4, 5, 6, 7]]
    y = df_float.iloc[:,[2, 3]]
    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.1, random_state=111)
    return x_train, x_test, y_train, y_test


x_train, x_test, y_train, y_test = get_train_test_data(df_float)
x_train.head()
97/13:
def train_decision_tree(x_train, y_train):
    decision_tree = DecisionTreeRegressor()
    decision_tree.fit(x_train, y_train)
    return decision_tree


DT_model = train_decision_tree(x_train, y_train)
97/14:
def train_linear_regression(x_train, y_train):
    linear_regression = LinearRegression()
    linear_regression.fit(x_train, y_train)
    return linear_regression


LR_model = train_linear_regression(x_train, y_train)
97/15:
def train_random_forest(x_train, y_train):
    random_forest = RandomForestRegressor(n_estimators=500, min_samples_leaf=.0001)
    random_forest.fit(x_train, y_train)
    return random_forest


RF_model = train_random_forest(x_train, y_train)
97/16:
def train_extra_tree(x_train, y_train):
    extra_tree = ExtraTreesRegressor(n_estimators = 100)
    extra_tree.fit(x_train, y_train)
    return extra_tree


ET_model = train_extra_tree(x_train, y_train)
97/17:
def test_model(model, x_test, y_test):
    y_predict=model.predict(x_test)
    score = r2_score(y_test, y_predict)
    return score
97/18:
RF_score = test_model(RF_model, x_test, y_test)
DT_score = test_model(DT_model, x_test, y_test)
LR_score = test_model(LR_model, x_test, y_test)
ET_score = test_model(ET_model, x_test, y_test)


evaluation = pd.DataFrame({"Model": ["Random Forest", "Decision Tree", "Linear Regression", "Extra Trees"], 
                           "R^2 Score": [RF_score, DT_score, LR_score, ET_score]})

evaluation.sort_values(by = 'R^2 Score', ascending = False, ignore_index=True)
97/19:
def success_cluster(n_cluster=5):
    df_cluster = df.copy()
    kmean = KMeans(n_clusters=n_cluster, random_state=100).fit(df_cluster[["rate", "votes"]])
    df_cluster["cluster"] = kmean.labels_
    return df_cluster
97/20: df.describe()
97/21:
def predict(model, restaurant_plan):
    plan_encode = encode_predict(restaurant_plan)
    return model.predict(plan_encode)
97/22: df["price"].describe()
97/23: df['location'].value_counts()[:10]
97/24: pd.DataFrame(df.groupby(['location'])['price'].mean()).sort_values(["price"])
97/25: pd.DataFrame(df.groupby(['location'])['price'].median()).sort_values(["price"])
97/26: df[df["price"] <= 1000].count()/ df.count()
97/27: df[(1000 < df["price"]) &  (df["price"] <= 2000)].count()/ df.count()
97/28:
def heatmap_generate(lat_lon_location, location_info, need_clustering=False):
    map_info = []
    for location in location_info:
        if not np.isnan(lat_lon_location[location][0]):
            map_info.append(list(lat_lon_location[location]) + [float(location_info[location]) ** 2])
    base_map = Map(location=[12.9716, 77.5946], zoom_start=11)
    HeatMap(map_info ,zoom=20,radius=15).add_to(base_map)
    if need_clustering:
        FastMarkerCluster(map_info).add_to(base_map)
    return base_map
97/29:
def generate_count_heatmap():
    location_count = dict(df['location'].value_counts())
    heatmap_count = heatmap_generate(lat_lon_location, location_count)
    return heatmap_count
97/30:
def generate_avg_price_heatmap():
    avg_price = dict(df.groupby(['location'])['price'].mean())
    heatmap_avg = heatmap_generate(lat_lon_location, avg_price, need_clustering=True)
    return heatmap_avg
97/31:
def generate_med_price_heatmap():
    med_price = dict(df.groupby(['location'])['price'].median())
    heatmap_med = heatmap_generate(lat_lon_location, med_price, need_clustering=True)
    return heatmap_med
97/32:
def plot_price_dist():
    fig, ax = plt.subplots(figsize=[10,4])
    sns.distplot(df["price"], ax=ax, axlabel="price of restaurant ").set_title("Figure 1")
97/33:
def plot_rate_votes_cluster(n_cluster=5):
    df_cluster = success_cluster(n_cluster)
    colors = ["g", "r", "b", "c", "m", "y"]
    
    plt.figure(figsize = (8,5))
    
    for i in range(n_cluster):
        selected = df_cluster[df_cluster["cluster"] == i]
        plt.plot(selected["rate"], selected["votes"], "ob", color=colors[i])
    plt.xlabel("rate")
    plt.ylabel("votes")
    plt.title("Figure 2")
    plt.show()
97/34:
def plot_rate_price_cluster(n_cluster=5):
    df_cluster = success_cluster(n_cluster)
    colors = ["g", "r", "b", "c", "m", "y"]
    plt.figure(figsize = (8,5))
    for i in range(n_cluster):
        selected = df_cluster[df_cluster["cluster"] == i]
        plt.plot(selected["rate"], selected["price"], "ob", color=colors[i])
    plt.xlabel("rate")
    plt.ylabel("price")
    plt.title("Figure 3")
    plt.show()
97/35:
def plot_votes_price_cluster(n_cluster=5):
    df_cluster = success_cluster(n_cluster)
    colors = ["g", "r", "b", "c", "m", "y"]
    plt.figure(figsize = (8,5))
    for i in range(n_cluster):
        selected = df_cluster[df_cluster["cluster"] == i]
        plt.plot(selected["votes"], selected["price"], "ob", color=colors[i])
    plt.xlabel("votes")
    plt.ylabel("price")
    plt.title("Figure 4")
    plt.show()
97/36: plot_price_dist()
97/37: plot_rate_votes_cluster(n_cluster=5)
97/38: plot_rate_price_cluster(n_cluster=5)
97/39: plot_votes_price_cluster(n_cluster=5)
97/40: generate_count_heatmap()
97/41: generate_avg_price_heatmap()
97/42: generate_med_price_heatmap()
97/43:
predict_0 = pd.DataFrame([{"online_order": "Yes", "book_table": "No", "location": "BTM", 
                        "rest_type": "Cafe", "price": 1000, "meal_type": "Buffet"}])
predict(ET_model, predict_0)
97/44:
predict_0 = pd.DataFrame([{"online_order": "No", "book_table": "Yes", "location": "BTM", 
                        "rest_type": "Cafe", "price": 1000, "meal_type": "Buffet"}])
predict(ET_model, predict_0)
97/45:
predict_0 = pd.DataFrame([{"online_order": "Yes", "book_table": "Yes", "location": "BTM", 
                         "rest_type": "Cafe", "price": 1000, "meal_type": "Buffet"}])
predict(ET_model, predict_0)
97/46: generate_med_price_heatmap()
98/1:
r = range(-1, -10)
print(list(r))
99/1:
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.colors

%matplotlib inline
99/2:
im = plt.imread('archway.jpg')
print(im.shape)
plt.imshow(im)
100/1:
import matplotlib.pyplot as plt
vals = rng.uniform(size=10000)
plt.hist(vals)
plt.show()
100/2:
from numpy.random import default_rng
rng = default_rng(seed = 12)
vals = rng.uniform(size=10)
print(vals)
100/3:
import matplotlib.pyplot as plt
vals = rng.uniform(size=10000)
plt.hist(vals)
plt.show()
100/4:
import matplotlib.pyplot as plt
import numpy as np

vals = rng.uniform(size=10000)
plt.hist(vals)
plt.show()
100/5:
x = np.random.rand(2,10000)
plt.scatter(x[0],x[1])
plt.show()
100/6:
from numpy import random
x = random.binomial(n=1000, p=0.4, size=1000)

plt.hist(x) 
plt.show()
100/7:
from numpy import random
x = random.binomial(n=1, p=0.4, size=1000)

plt.hist(x) 
plt.show()
100/8:
x = random.binomial(n=10, p=0.25, size=10000)

plt.hist(x) 
plt.show()
100/9:
x = random.binomial(n=10, p=0.25, size=10000)

plt.hist(x) 
plt.show()
100/10:
x = random.binomial(n=10, p=0.25, size=10000)

plt.hist(x) 
plt.show()
100/11:
x = random.binomial(n=10, p=0.25, size=100000)

plt.hist(x) 
plt.show()
100/12:
x = random.binomial(n=10, p=0.25, size=10000)

plt.hist(x) 
plt.show()
100/13:
x = random.binomial(n=10, p=0.25, size=10000)

plt.hist(x) 
plt.show()
100/14:
x = random.binomial(n=10, p=0.25, size=1000)

plt.hist(x) 
plt.show()
100/15:
x = random.binomial(n=10, p=0.25, size=10000)

plt.hist(x) 
plt.show()
100/16:
x = random.rand(50)
y = np.power(x,2)

plt.hist(y)
plt.show()
100/17:
x = random.rand(50)
f = np.power(x,2)
g = 100 / np.power(x, 0.5)

plt.hist(f)
plt.hist(g)
plt.show()
100/18:
x = random.rand(50)
f = np.power(x,2)
g = 100 / np.power(x, 0.5)

plt.subplot(1, 2, 1)
plt.hist(f)

plt.subplot(1, 2, 2)
plt.hist(g)
plt.show()
101/1:
import matplotlib.pyplot as plt
plt.rcParams['figure.figsize'] = [4, 4]
plt.rcParams['figure.dpi'] = 200
import numpy as np

from numpy.random import default_rng
rng = default_rng()

N_bin  = 1 # binomial for N=1 is Bernouilli
p    = 0.75
size = 100
sample = rng.binomial(N_bin, p, size)

print(sample)

n_ones = sample.sum() # the sum is adequate for a total count

mu_ml = float(n_ones)/size

print('mu_ml = ', mu_ml)
101/2:
import matplotlib.pyplot as plt
plt.rcParams['figure.figsize'] = [4, 4]
plt.rcParams['figure.dpi'] = 200
import numpy as np

from numpy.random import default_rng
rng = default_rng()

N_bin  = 1 # binomial for N=1 is Bernouilli
p    = 0.75
size = 100
sample = rng.binomial(N_bin, p, size)

print(sample)

n_ones = sample.sum() # the sum is adequate for a total count

mu_ml = float(n_ones)/size

print('mu_ml = ', mu_ml)
101/3:
import matplotlib.pyplot as plt

mu_values = np.array([0., 0.25, 0.5, 0.75, 1.])
prior   = np.array([0.05, 0.05,   0.7, 0.15,  0.05])

print('If the prior is a proper probability distribution function, it should be normalised:', prior.sum())

# we use the same sample as generated above that was used for the ML estimate

ones  = n_ones
zeros = size - ones

def likelihood(mu, ones, zeros):
    # multiply performs element wise multiplication
    return np.multiply(np.power(mu,ones),np.power(1-mu,zeros))

llh = likelihood(mu_values,ones,zeros)
posterior = np.multiply(llh,prior)
normalised_posterior = posterior/posterior.sum()

plt.plot(mu_values, prior,'b*',label='prior')
plt.plot(mu_values,normalised_posterior,'r+',label='posterior')
plt.xlabel('$\mu$')
plt.legend()
plt.savefig('falsecoin.pdf')
101/4:
%matplotlib inline
from ipywidgets import interactive

N=1
p=0.75
size = 100


def plotPosterior(samplesize):
    sample = rng.binomial(N,p,samplesize)
    ones = sample.sum()
    zeros= samplesize - ones
    
    llh = likelihood(mu_values,ones,zeros)
    posterior = np.multiply(llh,prior)
    normalised_posterior = posterior/posterior.sum()

    plt.plot(mu_values,prior,'k.',label='prior')
    plt.plot(mu_values,normalised_posterior,'r+', label='posterior')
    plt.legend()
    plt.show()
    
interactive_plot = interactive(plotPosterior, samplesize=(0, 50))
interactive_plot.show()
101/5:
%matplotlib inline
from ipywidgets import interactive

N=1
p=0.75
size = 100


def plotPosterior(samplesize):
    sample = rng.binomial(N,p,samplesize)
    ones = sample.sum()
    zeros= samplesize - ones
    
    llh = likelihood(mu_values,ones,zeros)
    posterior = np.multiply(llh,prior)
    normalised_posterior = posterior/posterior.sum()

    plt.plot(mu_values,prior,'k.',label='prior')
    plt.plot(mu_values,normalised_posterior,'r+', label='posterior')
    plt.legend()
    plt.show()
    
interactive_plot = interactive(plotPosterior, samplesize=(0, 50))
interactive_plot
101/6:
%matplotlib inline
from ipywidgets import interactive

N=1
p=0.75
size = 100


def plotPosterior(samplesize):
    sample = rng.binomial(N,p,samplesize)
    ones = sample.sum()
    zeros= samplesize - ones
    
    llh = likelihood(mu_values,ones,zeros)
    posterior = np.multiply(llh,prior)
    normalised_posterior = posterior/posterior.sum()

    plt.plot(mu_values,prior,'k.',label='prior')
    plt.plot(mu_values,normalised_posterior,'r+', label='posterior')
    plt.legend()
    plt.show()
    
interactive_plot = interactive(plotPosterior, samplesize=(0, 50))
interactive_plot
101/7:
N = 100
plotPriorAndPosterior(mu, N, a_prior, b_prior)
plt.savefig('posteriormu2.pdf')
101/8:
def aAndbFromSample(mu, size, a_prior, b_prior):
    N_bin = 1 # use binomial to sample Bernouilli    
    sample = rng.binomial(N_bin, mu, size)
    N=len(sample)
    N_1 = sample.sum()
    a = a_prior + N_1
    b = b_prior + N - N_1
    return a, b

def plotPriorAndPosterior(mu, size, a_prior, b_prior):
    a_post, b_post = aAndbFromSample(mu,size, a_prior, b_prior)   
    plt.plot(x, beta.pdf(x, a_prior, b_prior),'-', lw=2, alpha=0.6, label='a = '+str(a_prior)+' b = '+str(b_prior))    
    plt.plot(x, beta.pdf(x, a_post,  b_post), '-', lw=2, alpha=0.6, label='a = '+str(a_post) +' b = '+str(b_post))
    plt.legend()

mu = 0.75
a_prior = 10
b_prior = 10
N = 3 
plotPriorAndPosterior(mu, N, a_prior, b_prior)

plt.savefig('posteriormu1.pdf')
101/9:
import matplotlib.pyplot as plt
plt.rcParams['figure.figsize'] = [3, 3]
plt.rcParams['figure.dpi'] = 200

from scipy.stats import beta
pairs = [(2,3),(0.1,0.1),(8.,4.),(10.,10.)]
x = np.linspace(0.005, 0.995, 1000)
for pair in pairs:
    plt.plot(x, beta.pdf(x, pair[0], pair[1]),'-', lw=2, alpha=0.6, label='a = '+str(pair[0])+' b = '+str(pair[1]))
plt.legend()

plt.xlabel('x')
plt.ylabel('Beta(x;a,b)')
plt.savefig('beta.pdf')
101/10:
def aAndbFromSample(mu, size, a_prior, b_prior):
    N_bin = 1 # use binomial to sample Bernouilli    
    sample = rng.binomial(N_bin, mu, size)
    N=len(sample)
    N_1 = sample.sum()
    a = a_prior + N_1
    b = b_prior + N - N_1
    return a, b

def plotPriorAndPosterior(mu, size, a_prior, b_prior):
    a_post, b_post = aAndbFromSample(mu,size, a_prior, b_prior)   
    plt.plot(x, beta.pdf(x, a_prior, b_prior),'-', lw=2, alpha=0.6, label='a = '+str(a_prior)+' b = '+str(b_prior))    
    plt.plot(x, beta.pdf(x, a_post,  b_post), '-', lw=2, alpha=0.6, label='a = '+str(a_post) +' b = '+str(b_post))
    plt.legend()

mu = 0.75
a_prior = 10
b_prior = 10
N = 3 
plotPriorAndPosterior(mu, N, a_prior, b_prior)

plt.savefig('posteriormu1.pdf')
101/11:
N = 100
plotPriorAndPosterior(mu, N, a_prior, b_prior)
plt.savefig('posteriormu2.pdf')
101/12:
N = 100
plotPriorAndPosterior(mu, N, a_prior, b_prior)
plt.savefig('posteriormu2.pdf')
101/13:
N = 10000
plotPriorAndPosterior(mu, N, a_prior, b_prior)
plt.savefig('posteriormu3.pdf')
103/1:
import cv2
import math

import numpy as np
import torch
import torch.nn as nn
import torchvision.transforms as transforms
from torch.hub import load_state_dict_from_url

from PIL import Image
import matplotlib.pyplot as plt
103/2:
import cv2
import math

import numpy as np
import torch
import torch.nn as nn
import torchvision.transforms as transforms
from torch.hub import load_state_dict_from_url

from PIL import Image
import matplotlib.pyplot as plt
105/1:
import cv2
import math

import numpy as np
import torch
import torch.nn as nn
import torchvision.transforms as transforms
from torch.hub import load_state_dict_from_url

from PIL import Image
import matplotlib.pyplot as plt
105/2:
from torchvision.datasets import ImageFolder
dataset = ImageFolder(root="./data", transform=transform)
dataloader = DataLoader(dataset)
print(next(iter(dataloader)).shape)
105/3:
from torchvision.datasets import ImageFolder
dataset = ImageFolder(root="./data")
dataloader = DataLoader(dataset)
print(next(iter(dataloader)).shape)
105/4:
from torchvision.datasets import ImageFolder
dataset = ImageFolder(root="./data/train_set/train_set/")
dataloader = DataLoader(dataset)
print(next(iter(dataloader)).shape)
105/5:
from torchvision.datasets import ImageFolder
dataset = ImageFolder(root="data/train_set/train_set/")
dataloader = DataLoader(dataset)
print(next(iter(dataloader)).shape)
105/6:
from torchvision.datasets import ImageFolder
from torch.utils.data import DataLoader

dataset = ImageFolder(root="data/train_set/train_set/")
dataloader = DataLoader(dataset)
print(next(iter(dataloader)).shape)
105/7:
from torchvision.datasets import ImageFolder
from torch.utils.data import DataLoader

dataset = ImageFolder(root="data/")
dataloader = DataLoader(dataset)
print(next(iter(dataloader)).shape)
105/8:
from torchvision.datasets import ImageFolder
from torch.utils.data import DataLoader
105/9:
trnsfrm = transforms.Compose([
    transforms.ToTensor(),
    transforms.ColorJitter(hue=0.2, saturation=0.2, brightness=0.2),
    transforms.RandomAffine(degrees=10, translate=(0.1,0.1), scale=(0.9,1.1)),
    transforms.RandomHorizontalFlip(p=0.5)
])

dataset = ImageFolder(root="data/", transform=trnsfrm)
dataloader = DataLoader(dataset)
print(next(iter(dataloader)).shape)
105/10:
trnsfrm = transforms.Compose([
    transforms.ToTensor(),
    transforms.ColorJitter(hue=0.2, saturation=0.2, brightness=0.2),
    transforms.RandomAffine(degrees=10, translate=(0.1,0.1), scale=(0.9,1.1)),
    transforms.RandomHorizontalFlip(p=0.5)
])

dataset = ImageFolder(root="data/train_set/train_set/", transform=trnsfrm)
dataloader = DataLoader(dataset)
print(next(iter(dataloader)).shape)
105/11:
trnsfrm = transforms.Compose([
    transforms.ToTensor(),
    transforms.ColorJitter(hue=0.2, saturation=0.2, brightness=0.2),
    transforms.RandomAffine(degrees=10, translate=(0.1,0.1), scale=(0.9,1.1)),
    transforms.RandomHorizontalFlip(p=0.5)
])

dataset = ImageFolder(root="data/train_set/train_set/")
dataloader = DataLoader(dataset)
print(next(iter(dataloader)).shape)
105/12:
trnsfrm = transforms.Compose([
    transforms.ToTensor(),
    transforms.ColorJitter(hue=0.2, saturation=0.2, brightness=0.2),
    transforms.RandomAffine(degrees=10, translate=(0.1,0.1), scale=(0.9,1.1)),
    transforms.RandomHorizontalFlip(p=0.5)
])

dataset = ImageFolder(root="data/train_set/train_set/")
dataloader = DataLoader(dataset)
print(next(iter(dataloader)))
105/13:
# trnsfrm = transforms.Compose([
#     transforms.ToTensor(),
#     transforms.ColorJitter(hue=0.2, saturation=0.2, brightness=0.2),
#     transforms.RandomAffine(degrees=10, translate=(0.1,0.1), scale=(0.9,1.1)),
#     transforms.RandomHorizontalFlip(p=0.5)
# ])

dataset = ImageFolder(root="data/train_set/train_set/")
dataloader = DataLoader(dataset)
# print(next(iter(dataloader)))
print(dataloader)
105/14:
# trnsfrm = transforms.Compose([
#     transforms.ToTensor(),
#     transforms.ColorJitter(hue=0.2, saturation=0.2, brightness=0.2),
#     transforms.RandomAffine(degrees=10, translate=(0.1,0.1), scale=(0.9,1.1)),
#     transforms.RandomHorizontalFlip(p=0.5)
# ])

dataset = ImageFolder(root="data/train_set/train_set/", transfrom=transforms.ToTensor())
dataloader = DataLoader(dataset)
# print(next(iter(dataloader)))
print(dataloader)
105/15:
# trnsfrm = transforms.Compose([
#     transforms.ToTensor(),
#     transforms.ColorJitter(hue=0.2, saturation=0.2, brightness=0.2),
#     transforms.RandomAffine(degrees=10, translate=(0.1,0.1), scale=(0.9,1.1)),
#     transforms.RandomHorizontalFlip(p=0.5)
# ])

dataset = ImageFolder(root="data/train_set/train_set/", transform=transforms.ToTensor())
dataloader = DataLoader(dataset)
# print(next(iter(dataloader)))
print(dataloader)
105/16:
# trnsfrm = transforms.Compose([
#     transforms.ToTensor(),
#     transforms.ColorJitter(hue=0.2, saturation=0.2, brightness=0.2),
#     transforms.RandomAffine(degrees=10, translate=(0.1,0.1), scale=(0.9,1.1)),
#     transforms.RandomHorizontalFlip(p=0.5)
# ])

dataset = ImageFolder(root="data/train_set/train_set/", transform=transforms.ToTensor(), batch_size=64)
dataloader = DataLoader(dataset)
# print(next(iter(dataloader)))
print(dataloader)
105/17:
# trnsfrm = transforms.Compose([
#     transforms.ToTensor(),
#     transforms.ColorJitter(hue=0.2, saturation=0.2, brightness=0.2),
#     transforms.RandomAffine(degrees=10, translate=(0.1,0.1), scale=(0.9,1.1)),
#     transforms.RandomHorizontalFlip(p=0.5)
# ])

dataset = ImageFolder(root="data/train_set/train_set/", transform=transforms.ToTensor())
dataloader = DataLoader(dataset, batch_size=64)
# print(next(iter(dataloader)))
print(dataloader)
105/18:
# trnsfrm = transforms.Compose([
#     transforms.ToTensor(),
#     transforms.ColorJitter(hue=0.2, saturation=0.2, brightness=0.2),
#     transforms.RandomAffine(degrees=10, translate=(0.1,0.1), scale=(0.9,1.1)),
#     transforms.RandomHorizontalFlip(p=0.5)
# ])

dataset = ImageFolder(root="data/train_set/train_set/", transform=transforms.ToTensor())
dataloader = DataLoader(dataset, batch_size=64)
# print(next(iter(dataloader)))
print(next(iter(dataloader)).shape)
105/19:
# trnsfrm = transforms.Compose([
#     transforms.ToTensor(),
#     transforms.ColorJitter(hue=0.2, saturation=0.2, brightness=0.2),
#     transforms.RandomAffine(degrees=10, translate=(0.1,0.1), scale=(0.9,1.1)),
#     transforms.RandomHorizontalFlip(p=0.5)
# ])

dataset = ImageFolder(root="data/train_set/train_set/", transform=transforms.ToTensor())
dataloader = DataLoader(dataset, batch_size=64)
# print(next(iter(dataloader)))
print(next(iter(dataloader)))
105/20:
# trnsfrm = transforms.Compose([
#     transforms.ToTensor(),
#     transforms.ColorJitter(hue=0.2, saturation=0.2, brightness=0.2),
#     transforms.RandomAffine(degrees=10, translate=(0.1,0.1), scale=(0.9,1.1)),
#     transforms.RandomHorizontalFlip(p=0.5)
# ])

dataset = ImageFolder(root="data/train_set/train_set/", transform=transforms.ToTensor())
dataloader = DataLoader(dataset, batch_size=64)
# print(next(iter(dataloader)))
print(next(iter(dataloader))[0].shape)
105/21:
# TO COMPLETE
# define a Model class

net = nn.Sequential(
    nn.Conv2d(in_channels=3,out_channels=8, kernel_size=3),    
    nn.ReLU(),
    nn.MaxPool2d(kernel_size=2, stride=2),
    nn.Conv2d(in_channels=8,out_channels=16, kernel_size=3),
    nn.ReLU(),
    nn.MaxPool2d(kernel_size=2, stride=2),
    nn.Flatten(),
    nn.Linear(16*5*5,200),     # with 32x32 input, the feature map size reduces to 5x5 with 16 channels.
    nn.ReLU(),
    nn.Linear(200,30)
)
105/22:
# TO COMPLETE
# define a Model class

net = nn.Sequential(
    nn.Conv2d(in_channels=3,out_channels=8, kernel_size=3),    
    nn.ReLU(),
    nn.MaxPool2d(kernel_size=2, stride=2),
    nn.Conv2d(in_channels=8,out_channels=16, kernel_size=3),
    nn.ReLU(),
    nn.MaxPool2d(kernel_size=2, stride=2),
    nn.Flatten(),
    nn.Linear(16*5*5,200),     # with 32x32 input, the feature map size reduces to 5x5 with 16 channels.
    nn.ReLU(),
    nn.Linear(200,30)
)

for param in net.parameters():
    print(param.shape)
107/1:
net = nn.Sequential(
    nn.Conv2d(in_channels=3,out_channels=8, kernel_size=5),    # no padding, stride=1, dilation=1 by default
    nn.ReLU(),
    nn.MaxPool2d(kernel_size=2, stride=2),
    nn.Conv2d(in_channels=8,out_channels=16, kernel_size=5),
    nn.ReLU(),
    nn.MaxPool2d(kernel_size=2, stride=2),
    nn.Flatten(),
    nn.Linear(16*5*5,512),     # with 32x32 input, the feature map size reduces to 5x5 with 16 channels.
    nn.ReLU(),
    nn.Linear(512,200),
    nn.ReLU(),
    nn.Linear(200,30)
)

for param in net.parameters():
    print(param.shape)
105/23:
# TO COMPLETE
# define a Model class

net = nn.Sequential(
    nn.Conv2d(in_channels=3,out_channels=3, kernel_size=3),    
    nn.ReLU(),
    nn.Conv2d(in_channels=3,out_channels=6, kernel_size=3),
    nn.ReLU(),
    nn.MaxPool2d(kernel_size=2, stride=2),
    nn.Conv2d(in_channels=6,out_channels=6, kernel_size=3),    
    nn.ReLU(),
    nn.Conv2d(in_channels=6,out_channels=12, kernel_size=3),
    nn.ReLU(),
    nn.MaxPool2d(kernel_size=2, stride=2),
    nn.Flatten(),
    nn.Linear(12*13*13,512),     # with 32x32 input, the feature map size reduces to 5x5 with 16 channels.
    nn.ReLU(),
    nn.Linear(512,200),
    nn.Linear(200,30)
)

for param in net.parameters():
    print(param.shape)
107/2:
net = nn.Sequential(
    nn.Conv2d(in_channels=3,out_channels=8, kernel_size=5),    # no padding, stride=1, dilation=1 by default
    nn.ReLU(),
    nn.MaxPool2d(kernel_size=2, stride=2),
    nn.Conv2d(in_channels=8,out_channels=16, kernel_size=5),
    nn.ReLU(),
    nn.MaxPool2d(kernel_size=2, stride=2),
    nn.Flatten(),
    nn.Linear(16*5*5,64),     # with 32x32 input, the feature map size reduces to 5x5 with 16 channels.
    nn.ReLU(),
    nn.Linear(64,10)
)

for param in net.parameters():
    print(param.shape)
107/3:
import numpy as np
import torch
import torchvision
from torch import nn, optim
from torchvision import transforms, datasets
import matplotlib.pyplot as plt

# replace with your own root directory
ROOT="C:/Users/scsdch/OneDrive - University of Leeds/Teaching/Artificial Intelligence/workspace/"
107/4:
net = nn.Sequential(
    nn.Conv2d(in_channels=3,out_channels=8, kernel_size=5),    # no padding, stride=1, dilation=1 by default
    nn.ReLU(),
    nn.MaxPool2d(kernel_size=2, stride=2),
    nn.Conv2d(in_channels=8,out_channels=16, kernel_size=5),
    nn.ReLU(),
    nn.MaxPool2d(kernel_size=2, stride=2),
    nn.Flatten(),
    nn.Linear(16*5*5,64),     # with 32x32 input, the feature map size reduces to 5x5 with 16 channels.
    nn.ReLU(),
    nn.Linear(64,10)
)

for param in net.parameters():
    print(param.shape)
105/24: print(next(dataloader)[0].shape)
105/25: print(next(dataloader))
105/26: print(next(iter(dataloader))[0])
105/27: print(next(iter(dataloader))[0])
105/28: print(next(iter(dataloader))[0])
105/29: print(next(iter(dataloader))[0].shape())
105/30: print(next(iter(dataloader))[0].shape)
105/31: torch.max(0,1)
105/32:
data,_ = next(iter(dataloader))
net(data)
105/33:
data,_ = next(iter(dataloader))
net(data).shape
105/34:
data,_ = next(iter(dataloader))
output = net(data).shape
_, predicted = torch.max(outputs.data, 1)
105/35:
data,_ = next(iter(dataloader))
outputs = net(data).shape
_, predicted = torch.max(outputs.data, 1)
105/36:
data,_ = next(iter(dataloader))
outputs = net(data)
_, predicted = torch.max(outputs.data, 1)
105/37:
data,_ = next(iter(dataloader))
outputs = net(data)
_, predicted = torch.max(outputs.data, 1)
predicted
105/38:
batch_size = 64
total = batch_size
correct = 0
iter_time = 0

data = next(iter(dataloader))
inputs, labels = data


results_path = ROOT+'results/cnnclassifier50epochs.pt'
statsrec = np.zeros((4,nepochs))

loss_fn = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)

while total != correct:
    iter_time += 1
    correct = 0 
    running_loss = 0.0
        
     # Zero the parameter gradients
    optimizer.zero_grad()

    # Forward, backward, and update parameters
    outputs = net(inputs)
    loss = loss_fn(outputs, labels)
    loss.backward()
    optimizer.step()

    # accumulate loss
    running_loss += loss.item()
    n += 1

    # accumulate data for accuracy
    _, predicted = torch.max(outputs.data, 1)
    total += labels.size(0)    # add in the number of labels in this minibatch
    correct += (predicted == labels).sum().item()  # add in the number of correct labels
    
    # collect together statistics for this epoch
    ltrn = running_loss
    atrn = correct/total 
    ltst, atst = stats(test_loader, net)
    statsrec[:,epoch] = (ltrn, atrn, ltst, atst)
    print(f"time: {iter_time} training loss: {ltrn: .3f} training accuracy: {atrn: .1%}  test loss: {ltst: .3f} test accuracy: {atst: .1%}")

# save network parameters, losses and accuracy
torch.save({"state_dict": net.state_dict(), "stats": statsrec}, results_path)
105/39:
batch_size = 64
total = batch_size
correct = 0
iter_time = 0

data = next(iter(dataloader))
inputs, labels = data


results_path = 'results/cnnclassifier50epochs.pt'
statsrec = np.zeros((4,nepochs))

loss_fn = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)

while total != correct:
    iter_time += 1
    correct = 0 
    running_loss = 0.0
        
     # Zero the parameter gradients
    optimizer.zero_grad()

    # Forward, backward, and update parameters
    outputs = net(inputs)
    loss = loss_fn(outputs, labels)
    loss.backward()
    optimizer.step()

    # accumulate loss
    running_loss += loss.item()
    n += 1

    # accumulate data for accuracy
    _, predicted = torch.max(outputs.data, 1)
    total += labels.size(0)    # add in the number of labels in this minibatch
    correct += (predicted == labels).sum().item()  # add in the number of correct labels
    
    # collect together statistics for this epoch
    ltrn = running_loss
    atrn = correct/total 
    ltst, atst = stats(test_loader, net)
    statsrec[:,epoch] = (ltrn, atrn, ltst, atst)
    print(f"time: {iter_time} training loss: {ltrn: .3f} training accuracy: {atrn: .1%}  test loss: {ltst: .3f} test accuracy: {atst: .1%}")

# save network parameters, losses and accuracy
torch.save({"state_dict": net.state_dict(), "stats": statsrec}, results_path)
105/40:
batch_size = 64
total = batch_size
correct = 0
iter_time = 0

data = next(iter(dataloader))
inputs, labels = data


results_path = 'results/cnnclassifier50epochs.pt'
statsrec = np.zeros((4,1))

loss_fn = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)

while total != correct:
    correct = 0 
    running_loss = 0.0
        
     # Zero the parameter gradients
    optimizer.zero_grad()

    # Forward, backward, and update parameters
    outputs = net(inputs)
    loss = loss_fn(outputs, labels)
    loss.backward()
    optimizer.step()

    # accumulate loss
    running_loss += loss.item()
    n += 1

    # accumulate data for accuracy
    _, predicted = torch.max(outputs.data, 1)
    total += labels.size(0)    # add in the number of labels in this minibatch
    correct += (predicted == labels).sum().item()  # add in the number of correct labels
    
    # collect together statistics for this epoch
    ltrn = running_loss
    atrn = correct/total 
    ltst, atst = stats(test_loader, net)
    statsrec.append((ltrn, atrn, ltst, atst))
    print(f"time: {iter_time} training loss: {ltrn: .3f} training accuracy: {atrn: .1%}  test loss: {ltst: .3f} test accuracy: {atst: .1%}")
    iter_time += 1

# save network parameters, losses and accuracy
torch.save({"state_dict": net.state_dict(), "stats": statsrec}, results_path)
105/41: from torch import nn, optim
105/42:
batch_size = 64
total = batch_size
correct = 0
iter_time = 0

data = next(iter(dataloader))
inputs, labels = data


results_path = 'results/cnnclassifier50epochs.pt'
statsrec = np.zeros((4,1))

loss_fn = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)

while total != correct:
    correct = 0 
    running_loss = 0.0
        
     # Zero the parameter gradients
    optimizer.zero_grad()

    # Forward, backward, and update parameters
    outputs = net(inputs)
    loss = loss_fn(outputs, labels)
    loss.backward()
    optimizer.step()

    # accumulate loss
    running_loss += loss.item()
    n += 1

    # accumulate data for accuracy
    _, predicted = torch.max(outputs.data, 1)
    total += labels.size(0)    # add in the number of labels in this minibatch
    correct += (predicted == labels).sum().item()  # add in the number of correct labels
    
    # collect together statistics for this epoch
    ltrn = running_loss
    atrn = correct/total 
    ltst, atst = stats(test_loader, net)
    statsrec.append((ltrn, atrn, ltst, atst))
    print(f"time: {iter_time} training loss: {ltrn: .3f} training accuracy: {atrn: .1%}  test loss: {ltst: .3f} test accuracy: {atst: .1%}")
    iter_time += 1

# save network parameters, losses and accuracy
torch.save({"state_dict": net.state_dict(), "stats": statsrec}, results_path)
105/43:
batch_size = 64
total = batch_size
correct = 0
iter_time = 0

data = next(iter(dataloader))
inputs, labels = data


results_path = 'results/cnnclassifier50epochs.pt'
statsrec = np.zeros((4,1))

loss_fn = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)

while total != correct:
    correct = 0 
    running_loss = 0.0
        
     # Zero the parameter gradients
    optimizer.zero_grad()

    # Forward, backward, and update parameters
    outputs = net(inputs)
    loss = loss_fn(outputs, labels)
    loss.backward()
    optimizer.step()

    # accumulate loss
    running_loss += loss.item()

    # accumulate data for accuracy
    _, predicted = torch.max(outputs.data, 1)
    total += labels.size(0)    # add in the number of labels in this minibatch
    correct += (predicted == labels).sum().item()  # add in the number of correct labels
    
    # collect together statistics for this epoch
    ltrn = running_loss
    atrn = correct/total 
    ltst, atst = stats(test_loader, net)
    statsrec.append((ltrn, atrn, ltst, atst))
    print(f"time: {iter_time} training loss: {ltrn: .3f} training accuracy: {atrn: .1%}  test loss: {ltst: .3f} test accuracy: {atst: .1%}")
    iter_time += 1

# save network parameters, losses and accuracy
torch.save({"state_dict": net.state_dict(), "stats": statsrec}, results_path)
105/44:
batch_size = 64
total = batch_size
correct = 0
iter_time = 0

data = next(iter(dataloader))
inputs, labels = data


results_path = 'results/cnnclassifier50epochs.pt'
statsrec = np.zeros((4,1))

loss_fn = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)

while total != correct:
    correct = 0 
    running_loss = 0.0
        
     # Zero the parameter gradients
    optimizer.zero_grad()

    # Forward, backward, and update parameters
    outputs = net(inputs)
    loss = loss_fn(outputs, labels)
    loss.backward()
    optimizer.step()

    # accumulate loss
    running_loss += loss.item()

    # accumulate data for accuracy
    _, predicted = torch.max(outputs.data, 1)
    total += labels.size(0)    # add in the number of labels in this minibatch
    correct += (predicted == labels).sum().item()  # add in the number of correct labels
    
    # collect together statistics for this epoch
    ltrn = running_loss
    atrn = correct/total 
    statsrec.append((ltrn, atrn, ltst, atst))
    print(f"time: {iter_time} training loss: {ltrn: .3f} training accuracy: {atrn: .1%} ")
    iter_time += 1

# save network parameters, losses and accuracy
torch.save({"state_dict": net.state_dict(), "stats": statsrec}, results_path)
105/45:
batch_size = 64
total = batch_size
correct = 0
iter_time = 0

data = next(iter(dataloader))
inputs, labels = data


results_path = 'results/cnnclassifier50epochs.pt'
statsrec = np.zeros((4,1))

loss_fn = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)

while total != correct:
    correct = 0 
    running_loss = 0.0
        
     # Zero the parameter gradients
    optimizer.zero_grad()

    # Forward, backward, and update parameters
    outputs = net(inputs)
    loss = loss_fn(outputs, labels)
    loss.backward()
    optimizer.step()

    # accumulate loss
    running_loss += loss.item()

    # accumulate data for accuracy
    _, predicted = torch.max(outputs.data, 1)
    total += labels.size(0)    # add in the number of labels in this minibatch
    correct += (predicted == labels).sum().item()  # add in the number of correct labels
    
    # collect together statistics for this epoch
    ltrn = running_loss
    atrn = correct/total 
    print(f"time: {iter_time} training loss: {ltrn: .3f} training accuracy: {atrn: .1%} ")
    iter_time += 1

# save network parameters, losses and accuracy
torch.save({"state_dict": net.state_dict()}, results_path)
105/46:
batch_size = 64
total = batch_size
correct = 0
iter_time = 0

data = next(iter(dataloader))
inputs, labels = data


results_path = 'results/cnnclassifier50epochs.pt'
statsrec = np.zeros((4,1))

loss_fn = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)

while total != correct:
    correct = 0 
    running_loss = 0.0
        
     # Zero the parameter gradients
    optimizer.zero_grad()

    # Forward, backward, and update parameters
    outputs = net(inputs)
    loss = loss_fn(outputs, labels)
    loss.backward()
    optimizer.step()

    # accumulate loss
    running_loss += loss.item()

    # accumulate data for accuracy
    _, predicted = torch.max(outputs.data, 1)
    correct = (predicted == labels).sum().item()  # add in the number of correct labels
    
    # collect together statistics for this epoch
    ltrn = running_loss
    atrn = correct/total 
    print(f"time: {iter_time} training loss: {ltrn: .3f} training accuracy: {atrn: .1%} ")
    iter_time += 1

# save network parameters, losses and accuracy
torch.save({"state_dict": net.state_dict()}, results_path)
105/47:
batch_size = 64
total = batch_size
correct = 0
iter_time = 0

data = next(iter(dataloader))
inputs, labels = data


results_path = 'results/cnnclassifier50epochs.pt'
statsrec = np.zeros((4,1))

loss_fn = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)

while total != correct:
    correct = 0 
    running_loss = 0.0
        
     # Zero the parameter gradients
    optimizer.zero_grad()

    # Forward, backward, and update parameters
    outputs = net(inputs)
    loss = loss_fn(outputs, labels)
    loss.backward()
    optimizer.step()

    # accumulate loss
    running_loss += loss.item()

    # accumulate data for accuracy
    _, predicted = torch.max(outputs.data, 1)
    correct = (predicted == labels).sum().item()  # add in the number of correct labels
    
    # collect together statistics for this epoch
    ltrn = running_loss
    atrn = correct/total 
    print(f"time: {iter_time} training loss: {ltrn: .3f} training accuracy: {atrn: .1%} ")
    iter_time += 1

# save network parameters, losses and accuracy
torch.save({"state_dict": net.state_dict()}, results_path)
105/48:
batch_size = 64
total = batch_size
correct = 0
iter_time = 0

data = next(iter(dataloader))
inputs, labels = data


results_path = 'results/cnnclassifier50epochs.jpg'
statsrec = np.zeros((4,1))

loss_fn = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)

while total != correct:
    correct = 0 
    running_loss = 0.0
        
     # Zero the parameter gradients
    optimizer.zero_grad()

    # Forward, backward, and update parameters
    outputs = net(inputs)
    loss = loss_fn(outputs, labels)
    loss.backward()
    optimizer.step()

    # accumulate loss
    running_loss += loss.item()

    # accumulate data for accuracy
    _, predicted = torch.max(outputs.data, 1)
    correct = (predicted == labels).sum().item()  # add in the number of correct labels
    
    # collect together statistics for this epoch
    ltrn = running_loss
    atrn = correct/total 
    print(f"time: {iter_time} training loss: {ltrn: .3f} training accuracy: {atrn: .1%} ")
    iter_time += 1

# save network parameters, losses and accuracy
torch.save({"state_dict": net.state_dict()}, results_path)
109/1:
import cv2
import math

import numpy as np
import torch
import torch.nn as nn
import torchvision.transforms as transforms
from torch.hub import load_state_dict_from_url

from PIL import Image
import matplotlib.pyplot as plt
109/2:
from torchvision.datasets import ImageFolder
from torch.utils.data import DataLoader
109/3:
dataset = ImageFolder(root="data/train_set/train_set/", transform=transforms.ToTensor())
dataloader = DataLoader(dataset, batch_size=64)
109/4: print(next(iter(dataloader))[0].shape)
109/5:
# TO COMPLETE
# define a Model class

net = nn.Sequential(
    nn.Conv2d(in_channels=3,out_channels=3, kernel_size=3),    
    nn.ReLU(),
    nn.Conv2d(in_channels=3,out_channels=6, kernel_size=3),
    nn.ReLU(),
    nn.MaxPool2d(kernel_size=2, stride=2),
    nn.Conv2d(in_channels=6,out_channels=6, kernel_size=3),    
    nn.ReLU(),
    nn.Conv2d(in_channels=6,out_channels=12, kernel_size=3),
    nn.ReLU(),
    nn.MaxPool2d(kernel_size=2, stride=2),
    nn.Flatten(),
    nn.Linear(12*13*13,512),     # with 32x32 input, the feature map size reduces to 5x5 with 16 channels.
    nn.ReLU(),
    nn.Linear(512,200),
    nn.Linear(200,30)
)

for param in net.parameters():
    print(param.shape)
109/6: from torch import nn, optim
109/7:
batch_size = 64
total = batch_size
correct = 0
iter_time = 0

data = next(iter(dataloader))
inputs, labels = data


results_path = 'results/cnnclassifier50epochs.pt'
statsrec = np.zeros((4,1))

loss_fn = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)

while total != correct:
    correct = 0 
    running_loss = 0.0
        
     # Zero the parameter gradients
    optimizer.zero_grad()

    # Forward, backward, and update parameters
    outputs = net(inputs)
    loss = loss_fn(outputs, labels)
    loss.backward()
    optimizer.step()

    # accumulate loss
    running_loss += loss.item()

    # accumulate data for accuracy
    _, predicted = torch.max(outputs.data, 1)
    correct = (predicted == labels).sum().item()  # add in the number of correct labels
    
    # collect together statistics for this epoch
    ltrn = running_loss
    atrn = correct/total 
    print(f"time: {iter_time} training loss: {ltrn: .3f} training accuracy: {atrn: .1%} ")
    iter_time += 1

# save network parameters, losses and accuracy
torch.save({"state_dict": net.state_dict()}, results_path)
109/8:
val_data, val_labels = next(next(iter(dataloader)))
val_output = net(val_data)
_, val_predicted = torch.max(outputs.data, 1)
correct = (val_predicted == val_labels).sum().item()
print(correct)
109/9:
val_data, val_labels = next(dataloader)
val_output = net(val_data)
_, val_predicted = torch.max(outputs.data, 1)
correct = (val_predicted == val_labels).sum().item()
print(correct)
109/10:
val_data, val_labels = next(iter(dataloader))
val_output = net(val_data)
_, val_predicted = torch.max(outputs.data, 1)
correct = (val_predicted == val_labels).sum().item()
print(correct)
109/11:
val_data, val_labels = next(iter(dataloader))
val_output = net(val_data)
_, val_predicted = torch.max(outputs.data, 1)
correct = (val_predicted == val_labels).sum().item()
print(correct)
109/12:
val_data, val_labels = next(iter(dataloader))
val_output = net(val_data)
_, val_predicted = torch.max(outputs.data, 1)
correct = (val_predicted == val_labels).sum().item()
print(correct)
109/13:
# print(next(iter(dataloader))[0].shape) 
print(dataloader)
109/14:
n = 0
for val_data, val_labels in dataloader
    val_output = net(val_data)
    _, val_predicted = torch.max(outputs.data, 1)
    correct = (val_predicted == val_labels).sum().item()
    print(correct)
    n += 1
    if n ==4:
        break
109/15:
n = 0
for val_data, val_labels in dataloader:
    val_output = net(val_data)
    _, val_predicted = torch.max(outputs.data, 1)
    correct = (val_predicted == val_labels).sum().item()
    print(correct)
    n += 1
    if n ==4:
        break
109/16:
n = 0
for val_data, val_labels in dataloader:
    val_output = net(val_data)
    _, val_predicted = torch.max(outputs.data, 1)
    correct = (val_predicted == val_labels).sum().item()
    print(correct)
    n += 1
    if n ==20:
        break
109/17:
# print(next(iter(dataloader))[0].shape) 
print(iter(dataloader))
109/18:
# print(next(iter(dataloader))[0].shape) 
print(iter(iter(dataloader))
109/19:
# print(next(iter(dataloader))[0].shape) 
print(iter(iter(dataloader)))
109/20:
from torch import nn, optim
int(64*0.8)
109/21: from torch import nn, optim
109/22:
from torch import nn, optim
batch_size = 64
validation_index = int(batch_size * 0.8)

data = next(iter(dataloader))
inputs, labels = data
train_input = inputs[:validation_index]
validation_input = inputs[validation_index:]
train_label = labels[:validation_index]
validation_label = labels[validation_index:]


test_size = validation_index
test_accuracy = None
epoch = 0

results_path = 'results/cnnclassifier50epochs.pt'
statsrec = np.zeros((4,1))

loss_fn = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)

train_label.shape
109/23:
from torch import nn, optim
batch_size = 64
validation_index = int(batch_size * 0.8)

data = next(iter(dataloader))
inputs, labels = data
train_input = inputs[:validation_index]
validation_input = inputs[validation_index:]
train_label = labels[:validation_index]
validation_label = labels[validation_index:]


test_size = validation_index
test_accuracy = None
epoch = 0

results_path = 'results/cnnclassifier50epochs.pt'
statsrec = np.zeros((4,1))

loss_fn = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)

102 / train_label.shape
109/24:
from torch import nn, optim
batch_size = 64
validation_index = int(batch_size * 0.8)

data = next(iter(dataloader))
inputs, labels = data
train_input = inputs[:validation_index]
validation_input = inputs[validation_index:]
train_label = labels[:validation_index]
validation_label = labels[validation_index:]


test_size = validation_index
test_accuracy = None
epoch = 0

results_path = 'results/cnnclassifier50epochs.pt'
statsrec = np.zeros((4,1))

loss_fn = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)

102 / train_label.shape[0]
109/25:
from torch import nn, optim
batch_size = 64
validation_index = int(batch_size * 0.8)

data = next(iter(dataloader))
inputs, labels = data
train_inputs = inputs[:validation_index]
validation_inputs = inputs[validation_index:]
train_label = labels[:validation_index]
validation_label = labels[validation_index:]


test_size = validation_index
test_accuracy = None
epoch = 0

results_path = 'results/cnnclassifier50epochs.pt'
statsrec = np.zeros((4,1))

loss_fn = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)
109/26:
def accuracy(outputs, labels):
    _, predicted = torch.max(outputs.data, 1)
    correct = (predicted == labels).sum().item()
    return correct/labels.shape[0]
    

while test_accuracy != 0:
    running_loss = 0.0
        
     # Zero the parameter gradients
    optimizer.zero_grad()

    # Forward, backward, and update parameters
    train_output = net(train_inputs)
    loss = loss_fn(train_output, train_labels)
    loss.backward()
    optimizer.step()

    train_loss = loss.item()
    train_accuracy = accuracy(train_output, train_labels)
    
    
    validation_outputs = net(validation_inputs)
    validation_loss = loss_fn(validation_output, validation_labels).item()
    validation_accuracy = accuracy(validation_output, validation_labels)
    
    
    print(f"time: {epoch} training loss: {train_loss: .3f} training accuracy: {atrn: .1%} validation loss: {validation_loss: .3f} validation accuracy: {validation_accuracy: .1%}")
    epoch += 1

# save network parameters, losses and accuracy
torch.save({"state_dict": net.state_dict()}, results_path)
109/27:
def accuracy(outputs, labels):
    _, predicted = torch.max(outputs.data, 1)
    correct = (predicted == labels).sum().item()
    return correct/labels.shape[0]
    

while test_accuracy != 0:
    running_loss = 0.0
        
     # Zero the parameter gradients
    optimizer.zero_grad()

    # Forward, backward, and update parameters
    train_output = net(train_inputs)
    loss = loss_fn(train_output, train_labels)
    loss.backward()
    optimizer.step()

    train_loss = loss.item()
    train_accuracy = accuracy(train_output, train_labels)
    
    
    validation_outputs = net(validation_inputs, validation_labels)
    validation_loss = loss_fn(validation_output, validation_labels).item()
    validation_accuracy = accuracy(validation_output, validation_labels)
    
    
    print(f"time: {epoch} training loss: {train_loss: .3f} training accuracy: {atrn: .1%} validation loss: {validation_loss: .3f} validation accuracy: {validation_accuracy: .1%}")
    epoch += 1

# save network parameters, losses and accuracy
torch.save({"state_dict": net.state_dict()}, results_path)
109/28:
from torch import nn, optim
batch_size = 64
validation_index = int(batch_size * 0.8)

data = next(iter(dataloader))
inputs, labels = data
train_inputs = inputs[:validation_index]
validation_inputs = inputs[validation_index:]
train_labels = labels[:validation_index]
validation_labels = labels[validation_index:]


test_size = validation_index
test_accuracy = None
epoch = 0

results_path = 'results/cnnclassifier50epochs.pt'
statsrec = np.zeros((4,1))

loss_fn = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)
109/29:
def accuracy(outputs, labels):
    _, predicted = torch.max(outputs.data, 1)
    correct = (predicted == labels).sum().item()
    return correct/labels.shape[0]
    

while test_accuracy != 0:
    running_loss = 0.0
        
     # Zero the parameter gradients
    optimizer.zero_grad()

    # Forward, backward, and update parameters
    train_output = net(train_inputs)
    loss = loss_fn(train_output, train_labels)
    loss.backward()
    optimizer.step()

    train_loss = loss.item()
    train_accuracy = accuracy(train_output, train_labels)
    
    
    validation_outputs = net(validation_inputs, validation_labels)
    validation_loss = loss_fn(validation_output, validation_labels).item()
    validation_accuracy = accuracy(validation_output, validation_labels)
    
    
    print(f"time: {epoch} training loss: {train_loss: .3f} training accuracy: {atrn: .1%} validation loss: {validation_loss: .3f} validation accuracy: {validation_accuracy: .1%}")
    epoch += 1

# save network parameters, losses and accuracy
torch.save({"state_dict": net.state_dict()}, results_path)
109/30:
def accuracy(outputs, labels):
    _, predicted = torch.max(outputs.data, 1)
    correct = (predicted == labels).sum().item()
    return correct/labels.shape[0]
    

while test_accuracy != 0:
    running_loss = 0.0
        
     # Zero the parameter gradients
    optimizer.zero_grad()

    # Forward, backward, and update parameters
    train_output = net(train_inputs)
    loss = loss_fn(train_output, train_labels)
    loss.backward()
    optimizer.step()

    train_loss = loss.item()
    train_accuracy = accuracy(train_output, train_labels)
    
    
    validation_outputs = net(validation_inputs)
    validation_loss = loss_fn(validation_output, validation_labels).item()
    validation_accuracy = accuracy(validation_output, validation_labels)
    
    
    print(f"time: {epoch} training loss: {train_loss: .3f} training accuracy: {atrn: .1%} validation loss: {validation_loss: .3f} validation accuracy: {validation_accuracy: .1%}")
    epoch += 1

# save network parameters, losses and accuracy
torch.save({"state_dict": net.state_dict()}, results_path)
109/31:
def accuracy(outputs, labels):
    _, predicted = torch.max(outputs.data, 1)
    correct = (predicted == labels).sum().item()
    return correct/labels.shape[0]
    

while test_accuracy != 0:
    running_loss = 0.0
        
     # Zero the parameter gradients
    optimizer.zero_grad()

    # Forward, backward, and update parameters
    train_output = net(train_inputs)
    loss = loss_fn(train_output, train_labels)
    loss.backward()
    optimizer.step()

    train_loss = loss.item()
    train_accuracy = accuracy(train_output, train_labels)
    
    
    validation_output = net(validation_inputs)
    validation_loss = loss_fn(validation_output, validation_labels).item()
    validation_accuracy = accuracy(validation_output, validation_labels)
    
    
    print(f"time: {epoch} training loss: {train_loss: .3f} training accuracy: {atrn: .1%} validation loss: {validation_loss: .3f} validation accuracy: {validation_accuracy: .1%}")
    epoch += 1

# save network parameters, losses and accuracy
torch.save({"state_dict": net.state_dict()}, results_path)
110/1:
import cv2
import math

import numpy as np
import torch
import torch.nn as nn
import torchvision.transforms as transforms
from torch.hub import load_state_dict_from_url

from PIL import Image
import matplotlib.pyplot as plt
110/2:
from torchvision.datasets import ImageFolder
from torch.utils.data import DataLoader
110/3:
dataset = ImageFolder(root="data/train_set/train_set/", transform=transforms.ToTensor())
dataloader = DataLoader(dataset, batch_size=64)
110/4:
# print(next(iter(dataloader))[0].shape) 
print(iter(iter(dataloader)))
110/5:
# TO COMPLETE
# define a Model class

net = nn.Sequential(
    nn.Conv2d(in_channels=3,out_channels=3, kernel_size=3),    
    nn.ReLU(),
    nn.Conv2d(in_channels=3,out_channels=6, kernel_size=3),
    nn.ReLU(),
    nn.MaxPool2d(kernel_size=2, stride=2),
    nn.Conv2d(in_channels=6,out_channels=6, kernel_size=3),    
    nn.ReLU(),
    nn.Conv2d(in_channels=6,out_channels=12, kernel_size=3),
    nn.ReLU(),
    nn.MaxPool2d(kernel_size=2, stride=2),
    nn.Flatten(),
    nn.Linear(12*13*13,512),     # with 32x32 input, the feature map size reduces to 5x5 with 16 channels.
    nn.ReLU(),
    nn.Linear(512,200),
    nn.Linear(200,30)
)

for param in net.parameters():
    print(param.shape)
110/6:
from torch import nn, optim
batch_size = 64
validation_index = int(batch_size * 0.8)

data = next(iter(dataloader))
inputs, labels = data
train_inputs = inputs[:validation_index]
validation_inputs = inputs[validation_index:]
train_labels = labels[:validation_index]
validation_labels = labels[validation_index:]


test_size = validation_index
test_accuracy = None
epoch = 0

results_path = 'results/cnnclassifier50epochs.pt'
statsrec = np.zeros((4,1))

loss_fn = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)
110/7:
def accuracy(outputs, labels):
    _, predicted = torch.max(outputs.data, 1)
    correct = (predicted == labels).sum().item()
    return correct/labels.shape[0]
    

while test_accuracy != 1:
    running_loss = 0.0
        
     # Zero the parameter gradients
    optimizer.zero_grad()

    # Forward, backward, and update parameters
    train_output = net(train_inputs)
    loss = loss_fn(train_output, train_labels)
    loss.backward()
    optimizer.step()

    train_loss = loss.item()
    train_accuracy = accuracy(train_output, train_labels)
    
    
    validation_output = net(validation_inputs)
    validation_loss = loss_fn(validation_output, validation_labels).item()
    validation_accuracy = accuracy(validation_output, validation_labels)
    
    
    print(f"time: {epoch} training loss: {train_loss: .3f} training accuracy: {atrn: .1%} validation loss: {validation_loss: .3f} validation accuracy: {validation_accuracy: .1%}")
    epoch += 1

# save network parameters, losses and accuracy
torch.save({"state_dict": net.state_dict()}, results_path)
110/8:
def accuracy(outputs, labels):
    _, predicted = torch.max(outputs.data, 1)
    correct = (predicted == labels).sum().item()
    return correct/labels.shape[0]
    

while test_accuracy != 1:
    running_loss = 0.0
        
     # Zero the parameter gradients
    optimizer.zero_grad()

    # Forward, backward, and update parameters
    train_output = net(train_inputs)
    loss = loss_fn(train_output, train_labels)
    loss.backward()
    optimizer.step()

    train_loss = loss.item()
    train_accuracy = accuracy(train_output, train_labels)
    
    
    validation_output = net(validation_inputs)
    validation_loss = loss_fn(validation_output, validation_labels).item()
    validation_accuracy = accuracy(validation_output, validation_labels)
    
    
    print(f"time: {epoch} training loss: {train_loss: .3f} training accuracy: {train_accuracy: .1%} validation loss: {validation_loss: .3f} validation accuracy: {validation_accuracy: .1%}")
    epoch += 1

# save network parameters, losses and accuracy
torch.save({"state_dict": net.state_dict()}, results_path)
110/9:
# TO COMPLETE
# define a Model class

net = nn.Sequential(
    nn.Conv2d(in_channels=3,out_channels=3, kernel_size=3),    
    nn.ReLU(),
    nn.Conv2d(in_channels=3,out_channels=6, kernel_size=3),
    nn.ReLU(),
    nn.MaxPool2d(kernel_size=2, stride=2),
    nn.Conv2d(in_channels=6,out_channels=6, kernel_size=3),    
    nn.ReLU(),
    nn.Conv2d(in_channels=6,out_channels=12, kernel_size=3),
    nn.ReLU(),
    nn.MaxPool2d(kernel_size=2, stride=2),
    nn.Flatten(),
    nn.Linear(12*13*13,512),     # with 32x32 input, the feature map size reduces to 5x5 with 16 channels.
    nn.ReLU(),
    nn.Linear(512,200),
    nn.Linear(200,30)
)

for param in net.parameters():
    print(param.shape)
110/10:
from torch import nn, optim
batch_size = 64
validation_index = int(batch_size * 0.8)

data = next(iter(dataloader))
inputs, labels = data
train_inputs = inputs[:validation_index]
validation_inputs = inputs[validation_index:]
train_labels = labels[:validation_index]
validation_labels = labels[validation_index:]


test_size = validation_index
test_accuracy = None
epoch = 0

results_path = 'results/cnnclassifier50epochs.pt'
statsrec = np.zeros((4,1))

loss_fn = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)
110/11:
from torch import nn, optim
batch_size = 64
validation_index = int(batch_size * 0.8)

data = next(iter(dataloader))
inputs, labels = data
train_inputs = inputs[:validation_index]
validation_inputs = inputs[validation_index:]
train_labels = labels[:validation_index]
validation_labels = labels[validation_index:]

train_accuracy = None
epoch = 0


results_path = 'results/cnnclassifier50epochs.pt'
statsrec = np.zeros((4,1))

loss_fn = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)
110/12:
def accuracy(outputs, labels):
    _, predicted = torch.max(outputs.data, 1)
    correct = (predicted == labels).sum().item()
    return correct/labels.shape[0]
    

while test_accuracy != 1.0:
        
    optimizer.zero_grad()
    train_output = net(train_inputs)
    loss = loss_fn(train_output, train_labels)
    loss.backward()
    optimizer.step()

    train_loss = loss.item()
    train_accuracy = accuracy(train_output, train_labels)
    
    
    validation_output = net(validation_inputs)
    validation_loss = loss_fn(validation_output, validation_labels).item()
    validation_accuracy = accuracy(validation_output, validation_labels)
    
    
    print(f"epoch: {epoch} training loss: {train_loss: .3f} training accuracy: {train_accuracy: .1%} validation loss: {validation_loss: .3f} validation accuracy: {validation_accuracy: .1%}")
    epoch += 1

# save network parameters, losses and accuracy
torch.save({"state_dict": net.state_dict()}, results_path)
110/13:
# TO COMPLETE
# define a Model class

net = nn.Sequential(
    nn.Conv2d(in_channels=3,out_channels=3, kernel_size=3),    
    nn.ReLU(),
    nn.Conv2d(in_channels=3,out_channels=6, kernel_size=3),
    nn.ReLU(),
    nn.MaxPool2d(kernel_size=2, stride=2),
    nn.Conv2d(in_channels=6,out_channels=6, kernel_size=3),    
    nn.ReLU(),
    nn.Conv2d(in_channels=6,out_channels=12, kernel_size=3),
    nn.ReLU(),
    nn.MaxPool2d(kernel_size=2, stride=2),
    nn.Flatten(),
    nn.Linear(12*13*13,512),     # with 32x32 input, the feature map size reduces to 5x5 with 16 channels.
    nn.ReLU(),
    nn.Linear(512,200),
    nn.Linear(200,30)
)

for param in net.parameters():
    print(param.shape)
110/14:
from torch import nn, optim
batch_size = 64
validation_index = int(batch_size * 0.8)

data = next(iter(dataloader))
inputs, labels = data
train_inputs = inputs[:validation_index]
validation_inputs = inputs[validation_index:]
train_labels = labels[:validation_index]
validation_labels = labels[validation_index:]

train_accuracy = None
epoch = 0


results_path = 'results/cnnclassifier50epochs.pt'
statsrec = np.zeros((4,1))

loss_fn = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)
110/15:
def accuracy(outputs, labels):
    _, predicted = torch.max(outputs.data, 1)
    correct = (predicted == labels).sum().item()
    return correct/labels.shape[0]
    

while train_accuracy != 1.0:
        
    optimizer.zero_grad()
    train_output = net(train_inputs)
    loss = loss_fn(train_output, train_labels)
    loss.backward()
    optimizer.step()

    train_loss = loss.item()
    train_accuracy = accuracy(train_output, train_labels)
    
    
    validation_output = net(validation_inputs)
    validation_loss = loss_fn(validation_output, validation_labels).item()
    validation_accuracy = accuracy(validation_output, validation_labels)
    
    
    print(f"epoch: {epoch} training loss: {train_loss: .3f} training accuracy: {train_accuracy: .1%} validation loss: {validation_loss: .3f} validation accuracy: {validation_accuracy: .1%}")
    epoch += 1

# save network parameters, losses and accuracy
torch.save({"state_dict": net.state_dict()}, results_path)
110/16:
# TO COMPLETE
# define a Model class

class Simple_CNN(nn.Squential):
    def __init__(self):
        super(Sequential, self).__init__()
        layers = [    
            nn.Conv2d(in_channels=3,out_channels=3, kernel_size=3),    
            nn.ReLU(),
            nn.Conv2d(in_channels=3,out_channels=6, kernel_size=3),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(in_channels=6,out_channels=6, kernel_size=3),    
            nn.ReLU(),
            nn.Conv2d(in_channels=6,out_channels=12, kernel_size=3),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Flatten(),
            nn.Linear(12*13*13,512)，
            nn.ReLU(),
            nn.Linear(512,200),
            nn.Linear(200,30)
        ]
        for idx, module in enumerate(args):
            self.add_module(str(idx), module)

net = Simple_CNN()

for param in net.parameters():
    print(param.shape)
110/17:
# TO COMPLETE
# define a Model class

class Simple_CNN(nn.Squential):
    def __init__(self):
        super(Sequential, self).__init__()
        layers = [    
            nn.Conv2d(in_channels=3,out_channels=3, kernel_size=3),    
            nn.ReLU(),
            nn.Conv2d(in_channels=3,out_channels=6, kernel_size=3),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(in_channels=6,out_channels=6, kernel_size=3),    
            nn.ReLU(),
            nn.Conv2d(in_channels=6,out_channels=12, kernel_size=3),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Flatten(),
            nn.Linear(12*13*13,512),
            nn.ReLU(),
            nn.Linear(512,200),
            nn.Linear(200,30)
        ]
        for idx, module in enumerate(args):
            self.add_module(str(idx), module)

net = Simple_CNN()

for param in net.parameters():
    print(param.shape)
110/18:
# TO COMPLETE
# define a Model class

class Simple_CNN(nn.Sequential):
    def __init__(self):
        super(Sequential, self).__init__()
        layers = [    
            nn.Conv2d(in_channels=3,out_channels=3, kernel_size=3),    
            nn.ReLU(),
            nn.Conv2d(in_channels=3,out_channels=6, kernel_size=3),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(in_channels=6,out_channels=6, kernel_size=3),    
            nn.ReLU(),
            nn.Conv2d(in_channels=6,out_channels=12, kernel_size=3),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Flatten(),
            nn.Linear(12*13*13,512),
            nn.ReLU(),
            nn.Linear(512,200),
            nn.Linear(200,30)
        ]
        for idx, module in enumerate(args):
            self.add_module(str(idx), module)

net = Simple_CNN()

for param in net.parameters():
    print(param.shape)
110/19:
# TO COMPLETE
# define a Model class

class Simple_CNN(nn.Sequential):
    def __init__(self):
        super(nn.Sequential, self).__init__()
        layers = [    
            nn.Conv2d(in_channels=3,out_channels=3, kernel_size=3),    
            nn.ReLU(),
            nn.Conv2d(in_channels=3,out_channels=6, kernel_size=3),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(in_channels=6,out_channels=6, kernel_size=3),    
            nn.ReLU(),
            nn.Conv2d(in_channels=6,out_channels=12, kernel_size=3),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Flatten(),
            nn.Linear(12*13*13,512),
            nn.ReLU(),
            nn.Linear(512,200),
            nn.Linear(200,30)
        ]
        for idx, module in enumerate(args):
            self.add_module(str(idx), module)

net = Simple_CNN()

for param in net.parameters():
    print(param.shape)
110/20:
# TO COMPLETE
# define a Model class

class Simple_CNN(nn.Sequential):
    def __init__(self):
        super(nn.Sequential, self).__init__()
        layers = [    
            nn.Conv2d(in_channels=3,out_channels=3, kernel_size=3),    
            nn.ReLU(),
            nn.Conv2d(in_channels=3,out_channels=6, kernel_size=3),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(in_channels=6,out_channels=6, kernel_size=3),    
            nn.ReLU(),
            nn.Conv2d(in_channels=6,out_channels=12, kernel_size=3),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Flatten(),
            nn.Linear(12*13*13,512),
            nn.ReLU(),
            nn.Linear(512,200),
            nn.Linear(200,30)
        ]
        for idx, module in enumerate(layers):
            self.add_module(str(idx), module)

net = Simple_CNN()

for param in net.parameters():
    print(param.shape)
110/21:
from torch import nn, optim
batch_size = 64
validation_index = int(batch_size * 0.8)

data = next(iter(dataloader))
inputs, labels = data
train_inputs = inputs[:validation_index]
validation_inputs = inputs[validation_index:]
train_labels = labels[:validation_index]
validation_labels = labels[validation_index:]

train_accuracy = None
epoch = 0

net = Simple_CNN()


results_path = 'results/cnnclassifier50epochs.pt'
statsrec = np.zeros((4,1))

loss_fn = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)
110/22:
def accuracy(outputs, labels):
    _, predicted = torch.max(outputs.data, 1)
    correct = (predicted == labels).sum().item()
    return correct/labels.shape[0]
    

while train_accuracy != 1.0:
        
    optimizer.zero_grad()
    train_output = net(train_inputs)
    loss = loss_fn(train_output, train_labels)
    loss.backward()
    optimizer.step()

    train_loss = loss.item()
    train_accuracy = accuracy(train_output, train_labels)
    
    
    validation_output = net(validation_inputs)
    validation_loss = loss_fn(validation_output, validation_labels).item()
    validation_accuracy = accuracy(validation_output, validation_labels)
    
    
    print(f"epoch: {epoch} training loss: {train_loss: .3f} training accuracy: {train_accuracy: .1%} validation loss: {validation_loss: .3f} validation accuracy: {validation_accuracy: .1%}")
    epoch += 1

# save network parameters, losses and accuracy
torch.save({"state_dict": net.state_dict()}, results_path)
110/23:
def accuracy(outputs, labels):
    _, predicted = torch.max(outputs.data, 1)
    correct = (predicted == labels).sum().item()
    return correct/labels.shape[0]
    

while round(train_loss,3) != 0.0:
        
    optimizer.zero_grad()
    train_output = net(train_inputs)
    loss = loss_fn(train_output, train_labels)
    loss.backward()
    optimizer.step()
    scheduler.step()

    train_loss = loss.item()
    train_accuracy = accuracy(train_output, train_labels)
    
    
    validation_output = net(validation_inputs)
    validation_loss = loss_fn(validation_output, validation_labels).item()
    validation_accuracy = accuracy(validation_output, validation_labels)
    
    
    print(f"epoch: {epoch} training loss: {train_loss: .3f} training accuracy: {train_accuracy: .1%} validation loss: {validation_loss: .3f} validation accuracy: {validation_accuracy: .1%}")
    epoch += 1

# save network parameters, losses and accuracy
torch.save({"state_dict": net.state_dict()}, results_path)
110/24:
from torch import nn, optim
batch_size = 64
validation_index = int(batch_size * 0.8)

data = next(iter(dataloader))
inputs, labels = data
train_inputs = inputs[:validation_index]
validation_inputs = inputs[validation_index:]
train_labels = labels[:validation_index]
validation_labels = labels[validation_index:]

train_accuracy = None
epoch = 0

net = Simple_CNN()


results_path = 'results/cnnclassifier50epochs.pt'
statsrec = np.zeros((4,1))

loss_fn = nn.CrossEntropyLoss()
optimizer = optim.Adam(net.parameters(), lr=0.05)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.7, last_epoch=330)
111/1:
import cv2
import math

import numpy as np
import torch
import torch.nn as nn
import torchvision.transforms as transforms
from torch.hub import load_state_dict_from_url

from PIL import Image
import matplotlib.pyplot as plt
111/2:
from torchvision.datasets import ImageFolder
from torch.utils.data import DataLoader
111/3:
dataset = ImageFolder(root="data/train_set/train_set/", transform=transforms.ToTensor())
dataloader = DataLoader(dataset, batch_size=64)
111/4:
# print(next(iter(dataloader))[0].shape) 
print(iter(iter(dataloader)))
111/5:
# TO COMPLETE
# define a Model class

class Simple_CNN(nn.Sequential):
    def __init__(self):
        super(nn.Sequential, self).__init__()
        layers = [    
            nn.Conv2d(in_channels=3,out_channels=3, kernel_size=3),    
            nn.ReLU(),
            nn.Conv2d(in_channels=3,out_channels=6, kernel_size=3),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(in_channels=6,out_channels=6, kernel_size=3),    
            nn.ReLU(),
            nn.Conv2d(in_channels=6,out_channels=12, kernel_size=3),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Flatten(),
            nn.Linear(12*13*13,512),
            nn.ReLU(),
            nn.Linear(512,200),
            nn.Linear(200,30)
        ]
        for idx, module in enumerate(layers):
            self.add_module(str(idx), module)
111/6:
from torch import nn, optim
batch_size = 64
validation_index = int(batch_size * 0.8)

data = next(iter(dataloader))
inputs, labels = data
train_inputs = inputs[:validation_index]
validation_inputs = inputs[validation_index:]
train_labels = labels[:validation_index]
validation_labels = labels[validation_index:]

train_accuracy = None
epoch = 0

net = Simple_CNN()


results_path = 'results/cnnclassifier50epochs.pt'
statsrec = np.zeros((4,1))

loss_fn = nn.CrossEntropyLoss()
optimizer = optim.Adam(net.parameters(), lr=0.05)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.7, last_epoch=330)
111/7:
from torch import nn, optim
batch_size = 64
validation_index = int(batch_size * 0.8)

data = next(iter(dataloader))
inputs, labels = data
train_inputs = inputs[:validation_index]
validation_inputs = inputs[validation_index:]
train_labels = labels[:validation_index]
validation_labels = labels[validation_index:]

train_accuracy = None
epoch = 0

net = Simple_CNN()


results_path = 'results/cnnclassifier50epochs.pt'
statsrec = np.zeros((4,1))

loss_fn = nn.CrossEntropyLoss()
optimizer = optim.Adam(net.parameters(), lr=0.05)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.7)
111/8:
from torch import nn, optim
batch_size = 64
validation_index = int(batch_size * 0.8)

data = next(iter(dataloader))
inputs, labels = data
train_inputs = inputs[:validation_index]
validation_inputs = inputs[validation_index:]
train_labels = labels[:validation_index]
validation_labels = labels[validation_index:]

train_accuracy = None
epoch = 0

net = Simple_CNN()


results_path = 'results/cnnclassifier50epochs.pt'
statsrec = np.zeros((4,1))

loss_fn = nn.CrossEntropyLoss()
optimizer = optim.Adam(net.parameters(), lr=0.05)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.7)
111/9:
def accuracy(outputs, labels):
    _, predicted = torch.max(outputs.data, 1)
    correct = (predicted == labels).sum().item()
    return correct/labels.shape[0]
    

while round(train_loss,3) != 0.0:
        
    optimizer.zero_grad()
    train_output = net(train_inputs)
    loss = loss_fn(train_output, train_labels)
    loss.backward()
    optimizer.step()
    scheduler.step()

    train_loss = loss.item()
    train_accuracy = accuracy(train_output, train_labels)
    
    
    validation_output = net(validation_inputs)
    validation_loss = loss_fn(validation_output, validation_labels).item()
    validation_accuracy = accuracy(validation_output, validation_labels)
    
    
    print(f"epoch: {epoch} training loss: {train_loss: .3f} training accuracy: {train_accuracy: .1%} validation loss: {validation_loss: .3f} validation accuracy: {validation_accuracy: .1%}")
    epoch += 1

# save network parameters, losses and accuracy
torch.save({"state_dict": net.state_dict()}, results_path)
111/10:
from torch import nn, optim
batch_size = 64
validation_index = int(batch_size * 0.8)

data = next(iter(dataloader))
inputs, labels = data
train_inputs = inputs[:validation_index]
validation_inputs = inputs[validation_index:]
train_labels = labels[:validation_index]
validation_labels = labels[validation_index:]

train_loss = None
epoch = 0

net = Simple_CNN()


results_path = 'results/cnnclassifier50epochs.pt'
statsrec = np.zeros((4,1))

loss_fn = nn.CrossEntropyLoss()
optimizer = optim.Adam(net.parameters(), lr=0.05)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.7)
111/11:
def accuracy(outputs, labels):
    _, predicted = torch.max(outputs.data, 1)
    correct = (predicted == labels).sum().item()
    return correct/labels.shape[0]
    

while round(train_loss,3) != 0.0:
        
    optimizer.zero_grad()
    train_output = net(train_inputs)
    loss = loss_fn(train_output, train_labels)
    loss.backward()
    optimizer.step()
    scheduler.step()

    train_loss = loss.item()
    train_accuracy = accuracy(train_output, train_labels)
    
    
    validation_output = net(validation_inputs)
    validation_loss = loss_fn(validation_output, validation_labels).item()
    validation_accuracy = accuracy(validation_output, validation_labels)
    
    
    print(f"epoch: {epoch} training loss: {train_loss: .3f} training accuracy: {train_accuracy: .1%} validation loss: {validation_loss: .3f} validation accuracy: {validation_accuracy: .1%}")
    epoch += 1

# save network parameters, losses and accuracy
torch.save({"state_dict": net.state_dict()}, results_path)
111/12:
from torch import nn, optim
batch_size = 64
validation_index = int(batch_size * 0.8)

data = next(iter(dataloader))
inputs, labels = data
train_inputs = inputs[:validation_index]
validation_inputs = inputs[validation_index:]
train_labels = labels[:validation_index]
validation_labels = labels[validation_index:]

train_loss = 10
epoch = 0

net = Simple_CNN()


results_path = 'results/cnnclassifier50epochs.pt'
statsrec = np.zeros((4,1))

loss_fn = nn.CrossEntropyLoss()
optimizer = optim.Adam(net.parameters(), lr=0.05)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.7)
111/13:
def accuracy(outputs, labels):
    _, predicted = torch.max(outputs.data, 1)
    correct = (predicted == labels).sum().item()
    return correct/labels.shape[0]
    

while round(train_loss,3) != 0.0:
        
    optimizer.zero_grad()
    train_output = net(train_inputs)
    loss = loss_fn(train_output, train_labels)
    loss.backward()
    optimizer.step()
    scheduler.step()

    train_loss = loss.item()
    train_accuracy = accuracy(train_output, train_labels)
    
    
    validation_output = net(validation_inputs)
    validation_loss = loss_fn(validation_output, validation_labels).item()
    validation_accuracy = accuracy(validation_output, validation_labels)
    
    
    print(f"epoch: {epoch} training loss: {train_loss: .3f} training accuracy: {train_accuracy: .1%} validation loss: {validation_loss: .3f} validation accuracy: {validation_accuracy: .1%}")
    epoch += 1

# save network parameters, losses and accuracy
torch.save({"state_dict": net.state_dict()}, results_path)
111/14:
from torch import nn, optim
batch_size = 64
validation_index = int(batch_size * 0.8)

data = next(iter(dataloader))
inputs, labels = data
train_inputs = inputs[:validation_index]
validation_inputs = inputs[validation_index:]
train_labels = labels[:validation_index]
validation_labels = labels[validation_index:]

train_loss = 10
epoch = 0

net = Simple_CNN()


results_path = 'results/cnnclassifier50epochs.pt'
statsrec = np.zeros((4,1))

loss_fn = nn.CrossEntropyLoss()
optimizer = optim.Adam(net.parameters(), lr=0.05)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.7)
111/15:
def accuracy(outputs, labels):
    _, predicted = torch.max(outputs.data, 1)
    correct = (predicted == labels).sum().item()
    return correct/labels.shape[0]
    

while round(train_loss,3) != 0.0:
        
    optimizer.zero_grad()
    train_output = net(train_inputs)
    loss = loss_fn(train_output, train_labels)
    loss.backward()
    optimizer.step()
    scheduler.step()

    train_loss = loss.item()
    train_accuracy = accuracy(train_output, train_labels)
    
    
    validation_output = net(validation_inputs)
    validation_loss = loss_fn(validation_output, validation_labels).item()
    validation_accuracy = accuracy(validation_output, validation_labels)
    
    
    print(f"epoch: {epoch} training loss: {train_loss: .3f} training accuracy: {train_accuracy: .1%} validation loss: {validation_loss: .3f} validation accuracy: {validation_accuracy: .1%}")
    epoch += 1

# save network parameters, losses and accuracy
torch.save({"state_dict": net.state_dict()}, results_path)
111/16:
from torch import nn, optim
batch_size = 64
validation_index = int(batch_size * 0.8)

data = next(iter(dataloader))
inputs, labels = data
train_inputs = inputs[:validation_index]
validation_inputs = inputs[validation_index:]
train_labels = labels[:validation_index]
validation_labels = labels[validation_index:]

train_loss = 10
epoch = 0

net = Simple_CNN()


results_path = 'results/cnnclassifier50epochs.pt'
statsrec = np.zeros((4,1))

loss_fn = nn.CrossEntropyLoss()
optimizer = optim.Adam(net.parameters(), lr=0.05)
# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.7)
111/17:
def accuracy(outputs, labels):
    _, predicted = torch.max(outputs.data, 1)
    correct = (predicted == labels).sum().item()
    return correct/labels.shape[0]
    

while round(train_loss,3) != 0.0:
        
    optimizer.zero_grad()
    train_output = net(train_inputs)
    loss = loss_fn(train_output, train_labels)
    loss.backward()
    optimizer.step()
#     scheduler.step()

    train_loss = loss.item()
    train_accuracy = accuracy(train_output, train_labels)
    
    
    validation_output = net(validation_inputs)
    validation_loss = loss_fn(validation_output, validation_labels).item()
    validation_accuracy = accuracy(validation_output, validation_labels)
    
    
    print(f"epoch: {epoch} training loss: {train_loss: .3f} training accuracy: {train_accuracy: .1%} validation loss: {validation_loss: .3f} validation accuracy: {validation_accuracy: .1%}")
    epoch += 1

# save network parameters, losses and accuracy
torch.save({"state_dict": net.state_dict()}, results_path)
111/18:
for inputs, labels in dataloader:
    print(accuracy(net(inputs), labels))
111/19:
import cv2
import math

import numpy as np
import torch
import torch.nn as nn
import torchvision.transforms as transforms
from torch.hub import load_state_dict_from_url

from PIL import Image
import matplotlib.pyplot as plt
111/20:
from torchvision.datasets import ImageFolder
from torch.utils.data import DataLoader
111/21:
dataset = ImageFolder(root="data/train_set/train_set/", transform=transforms.ToTensor())
dataloader = DataLoader(dataset, batch_size=64)
111/22:
# print(next(iter(dataloader))[0].shape) 
print(iter(iter(dataloader)))
111/23:
# TO COMPLETE
# define a Model class

class Simple_CNN(nn.Sequential):
    def __init__(self):
        super(nn.Sequential, self).__init__()
        layers = [    
            nn.Conv2d(in_channels=3,out_channels=3, kernel_size=3),    
            nn.ReLU(),
            nn.Conv2d(in_channels=3,out_channels=6, kernel_size=3),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(in_channels=6,out_channels=6, kernel_size=3),    
            nn.ReLU(),
            nn.Conv2d(in_channels=6,out_channels=12, kernel_size=3),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Flatten(),
            nn.Linear(12*13*13,512),
            nn.ReLU(),
            nn.Linear(512,200),
            nn.Linear(200,30)
        ]
        for idx, module in enumerate(layers):
            self.add_module(str(idx), module)
111/24:
from torch import nn, optim
batch_size = 64
validation_index = int(batch_size * 0.8)

data = next(iter(dataloader))
inputs, labels = data
train_inputs = inputs[:validation_index]
validation_inputs = inputs[validation_index:]
train_labels = labels[:validation_index]
validation_labels = labels[validation_index:]

train_loss = 10
epoch = 0

net = Simple_CNN()


results_path = 'results/cnnclassifier50epochs.pt'
statsrec = np.zeros((4,1))

loss_fn = nn.CrossEntropyLoss()
optimizer = optim.Adam(net.parameters(), lr=0.05)
# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.7)
111/25:
for inputs, labels in dataloader:
    print(accuracy(net(inputs), labels))
111/26:
def accuracy(outputs, labels):
    _, predicted = torch.max(outputs.data, 1)
    correct = (predicted == labels).sum().item()
    return correct/labels.shape[0]
    

while round(train_loss,3) != 0.0:
        
    optimizer.zero_grad()
    train_output = net(train_inputs)
    loss = loss_fn(train_output, train_labels)
    loss.backward()
    optimizer.step()
#     scheduler.step()

    train_loss = loss.item()
    train_accuracy = accuracy(train_output, train_labels)
    
    
    validation_output = net(validation_inputs)
    validation_loss = loss_fn(validation_output, validation_labels).item()
    validation_accuracy = accuracy(validation_output, validation_labels)
    
    
    print(f"epoch: {epoch} training loss: {train_loss: .3f} training accuracy: {train_accuracy: .1%} validation loss: {validation_loss: .3f} validation accuracy: {validation_accuracy: .1%}")
    epoch += 1

# save network parameters, losses and accuracy
torch.save({"state_dict": net.state_dict()}, results_path)
111/27:
for inputs, labels in dataloader:
    print(accuracy(net(inputs), labels))
111/28:
from torch import nn, optim
batch_size = 64
validation_index = int(batch_size * 0.8)

data = next(iter(dataloader))
inputs, labels = data
train_inputs = inputs[:validation_index]
validation_inputs = inputs[validation_index:]
train_labels = labels[:validation_index]
validation_labels = labels[validation_index:]

train_loss = 10
epoch = 0

net = Simple_CNN()


results_path = 'results/cnnclassifier50epochs.pt'
statsrec = np.zeros((4,1))

loss_fn = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)
# optimizer = optim.Adam(net.parameters(), lr=0.05)
# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.7)
111/29:
def accuracy(outputs, labels):
    _, predicted = torch.max(outputs.data, 1)
    correct = (predicted == labels).sum().item()
    return correct/labels.shape[0]
    

while round(train_loss,3) != 0.0:
        
    optimizer.zero_grad()
    train_output = net(train_inputs)
    loss = loss_fn(train_output, train_labels)
    loss.backward()
    optimizer.step()
#     scheduler.step()

    train_loss = loss.item()
    train_accuracy = accuracy(train_output, train_labels)
    
    
    validation_output = net(validation_inputs)
    validation_loss = loss_fn(validation_output, validation_labels).item()
    validation_accuracy = accuracy(validation_output, validation_labels)
    
    
    print(f"epoch: {epoch} training loss: {train_loss: .3f} training accuracy: {train_accuracy: .1%} validation loss: {validation_loss: .3f} validation accuracy: {validation_accuracy: .1%}")
    epoch += 1

# save network parameters, losses and accuracy
torch.save({"state_dict": net.state_dict()}, results_path)
111/30:
for inputs, labels in dataloader:
    print(accuracy(net(inputs), labels))
111/31:
from torch import nn, optim
batch_size = 64
validation_index = int(batch_size * 0.8)

data = next(iter(dataloader))
inputs, labels = data
train_inputs = inputs[:validation_index]
validation_inputs = inputs[validation_index:]
train_labels = labels[:validation_index]
validation_labels = labels[validation_index:]

train_loss = 10
epoch = 0

net = Simple_CNN()


results_path = 'results/cnnclassifier50epochs.pt'
statsrec = np.zeros((4,1))

loss_fn = nn.CrossEntropyLoss()
# optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)
optimizer = optim.Adam(net.parameters(), lr=0.05)
# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.7)
111/32:
def accuracy(outputs, labels):
    _, predicted = torch.max(outputs.data, 1)
    correct = (predicted == labels).sum().item()
    return correct/labels.shape[0]
    

while round(train_loss,3) != 0.0:
        
    optimizer.zero_grad()
    train_output = net(train_inputs)
    loss = loss_fn(train_output, train_labels)
    loss.backward()
    optimizer.step()
#     scheduler.step()

    train_loss = loss.item()
    train_accuracy = accuracy(train_output, train_labels)
    
    
    validation_output = net(validation_inputs)
    validation_loss = loss_fn(validation_output, validation_labels).item()
    validation_accuracy = accuracy(validation_output, validation_labels)
    
    
    print(f"epoch: {epoch} training loss: {train_loss: .3f} training accuracy: {train_accuracy: .1%} validation loss: {validation_loss: .3f} validation accuracy: {validation_accuracy: .1%}")
    epoch += 1

# save network parameters, losses and accuracy
torch.save({"state_dict": net.state_dict()}, results_path)
111/33:
for inputs, labels in dataloader:
    print(accuracy(net(inputs), labels))
111/34:
plt.plot(statsrec[0], 'r', label = 'training loss', )
plt.plot(statsrec[2], 'g', label = 'test loss' )
plt.legend(loc='lower right')
plt.xlabel('epoch')
plt.ylabel('loss')
plt.title('Training and test loss, and test accuracy')
112/1:
import cv2
import math

import numpy as np
import torch
import torch.nn as nn
import torchvision.transforms as transforms
from torch.hub import load_state_dict_from_url

from PIL import Image
import matplotlib.pyplot as plt
112/2:
from torchvision.datasets import ImageFolder
from torch.utils.data import DataLoader
112/3:
dataset = ImageFolder(root="data/train_set/train_set/", transform=transforms.ToTensor())
dataloader = DataLoader(dataset, batch_size=64)
112/4:
# print(next(iter(dataloader))[0].shape) 
print(iter(iter(dataloader)))
112/5:
# TO COMPLETE
# define a Model class

class Simple_CNN(nn.Sequential):
    def __init__(self):
        super(nn.Sequential, self).__init__()
        layers = [    
            nn.Conv2d(in_channels=3,out_channels=3, kernel_size=3),    
            nn.ReLU(),
            nn.Conv2d(in_channels=3,out_channels=6, kernel_size=3),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(in_channels=6,out_channels=6, kernel_size=3),    
            nn.ReLU(),
            nn.Conv2d(in_channels=6,out_channels=12, kernel_size=3),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Flatten(),
            nn.Linear(12*13*13,512),
            nn.ReLU(),
            nn.Linear(512,200),
            nn.Linear(200,30)
        ]
        for idx, module in enumerate(layers):
            self.add_module(str(idx), module)
112/6:
from torch import nn, optim
batch_size = 64
validation_index = int(batch_size * 0.8)

data = next(iter(dataloader))
inputs, labels = data
train_inputs = inputs[:validation_index]
validation_inputs = inputs[validation_index:]
train_labels = labels[:validation_index]
validation_labels = labels[validation_index:]

train_loss = 10
epoch = 0

net = Simple_CNN()


results_path = 'results/cnnclassifier50epochs.pt'
statsrec = np.zeros((4,1))

loss_fn = nn.CrossEntropyLoss()
# optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)
optimizer = optim.Adam(net.parameters(), lr=0.05)
# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.7)
112/7:
from torch import nn, optim
batch_size = 64
validation_index = int(batch_size * 0.8)

data = next(iter(dataloader))
inputs, labels = data
train_inputs = inputs[:validation_index]
validation_inputs = inputs[validation_index:]
train_labels = labels[:validation_index]
validation_labels = labels[validation_index:]

train_loss = 10
epoch = 0

net = Simple_CNN()


results_path = 'results/cnnclassifier50epochs.pt'
statsrec = np.zeros((4,1))

loss_fn = nn.CrossEntropyLoss()
# optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)
optimizer = optim.Adam(net.parameters(), lr=0.001)
# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.7)
112/8:
def accuracy(outputs, labels):
    _, predicted = torch.max(outputs.data, 1)
    correct = (predicted == labels).sum().item()
    return correct/labels.shape[0]
    

while round(train_loss,3) != 0.0:
        
    optimizer.zero_grad()
    train_output = net(train_inputs)
    loss = loss_fn(train_output, train_labels)
    loss.backward()
    optimizer.step()
#     scheduler.step()

    train_loss = loss.item()
    train_accuracy = accuracy(train_output, train_labels)
    
    
    validation_output = net(validation_inputs)
    validation_loss = loss_fn(validation_output, validation_labels).item()
    validation_accuracy = accuracy(validation_output, validation_labels)
    
    
    print(f"epoch: {epoch} training loss: {train_loss: .3f} training accuracy: {train_accuracy: .1%} validation loss: {validation_loss: .3f} validation accuracy: {validation_accuracy: .1%}")
    epoch += 1

# save network parameters, losses and accuracy
torch.save({"state_dict": net.state_dict()}, results_path)
112/9:
for inputs, labels in dataloader:
    print(accuracy(net(inputs), labels))
112/10:
plt.plot(train_loss, 'r', label = 'training loss', )
plt.plot(validation_loss, 'g', label = 'test loss' )
plt.xlabel('epoch')
plt.ylabel('loss')
plt.title('Training and test loss')
113/1:
import cv2
import math

import numpy as np
import torch
import torch.nn as nn
import torchvision.transforms as transforms
from torch.hub import load_state_dict_from_url

from PIL import Image
import matplotlib.pyplot as plt
import os
os.environ["KMP_DUPLICATE_LIB_OK"]="TRUE"
114/1:
import cv2
import math

import numpy as np
import torch
import torch.nn as nn
import torchvision.transforms as transforms
from torch.hub import load_state_dict_from_url

from PIL import Image
import matplotlib.pyplot as plt
import os
os.environ["KMP_DUPLICATE_LIB_OK"]="TRUE"
114/2:
from torchvision.datasets import ImageFolder
from torch.utils.data import DataLoader
114/3:
dataset = ImageFolder(root="data/train_set/train_set/", transform=transforms.ToTensor())
dataloader = DataLoader(dataset, batch_size=64)
114/4:
# print(next(iter(dataloader))[0].shape) 
print(iter(iter(dataloader)))
114/5:
# TO COMPLETE
# define a Model class

class Simple_CNN(nn.Sequential):
    def __init__(self):
        super(nn.Sequential, self).__init__()
        layers = [    
            nn.Conv2d(in_channels=3,out_channels=3, kernel_size=3),    
            nn.ReLU(),
            nn.Conv2d(in_channels=3,out_channels=6, kernel_size=3),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(in_channels=6,out_channels=6, kernel_size=3),    
            nn.ReLU(),
            nn.Conv2d(in_channels=6,out_channels=12, kernel_size=3),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Flatten(),
            nn.Linear(12*13*13,512),
            nn.ReLU(),
            nn.Linear(512,200),
            nn.Linear(200,30)
        ]
        for idx, module in enumerate(layers):
            self.add_module(str(idx), module)
114/6:
from torch import nn, optim
batch_size = 64
validation_index = int(batch_size * 0.8)

data = next(iter(dataloader))
inputs, labels = data
train_inputs = inputs[:validation_index]
validation_inputs = inputs[validation_index:]
train_labels = labels[:validation_index]
validation_labels = labels[validation_index:]

train_loss = 10
epoch = 0

net = Simple_CNN()


results_path = 'results/cnnclassifier50epochs.pt'
statsrec = np.zeros((4,1))

loss_fn = nn.CrossEntropyLoss()
# optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)
optimizer = optim.Adam(net.parameters(), lr=0.001)
# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.7)
114/7:
def accuracy(outputs, labels):
    _, predicted = torch.max(outputs.data, 1)
    correct = (predicted == labels).sum().item()
    return correct/labels.shape[0]
    

while round(train_loss,3) != 0.0:
        
    optimizer.zero_grad()
    train_output = net(train_inputs)
    loss = loss_fn(train_output, train_labels)
    loss.backward()
    optimizer.step()
#     scheduler.step()

    train_loss = loss.item()
    train_accuracy = accuracy(train_output, train_labels)
    
    
    validation_output = net(validation_inputs)
    validation_loss = loss_fn(validation_output, validation_labels).item()
    validation_accuracy = accuracy(validation_output, validation_labels)
    
    
    print(f"epoch: {epoch} training loss: {train_loss: .3f} training accuracy: {train_accuracy: .1%} validation loss: {validation_loss: .3f} validation accuracy: {validation_accuracy: .1%}")
    epoch += 1

# save network parameters, losses and accuracy
torch.save({"state_dict": net.state_dict()}, results_path)
114/8:
plt.plot(train_loss, 'r', label = 'training loss', )
plt.plot(validation_loss, 'g', label = 'test loss' )
plt.xlabel('epoch')
plt.ylabel('loss')
plt.title('Training and test loss')
114/9:
from torch import nn, optim
batch_size = 64
validation_index = int(batch_size * 0.8)

data = next(iter(dataloader))
inputs, labels = data
train_inputs = inputs[:validation_index]
validation_inputs = inputs[validation_index:]
train_labels = labels[:validation_index]
validation_labels = labels[validation_index:]

train_loss = 999
stat = {"train_loss":[], "validation_loss":[]}
epoch = 0



results_path = 'results/cnnclassifier50epochs.pt'
statsrec = np.zeros((4,1))


net = Simple_CNN()
loss_fn = nn.CrossEntropyLoss()
optimizer = optim.Adam(net.parameters(), lr=0.001)
# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.7)
114/10:
from torch import nn, optim
batch_size = 64
validation_index = int(batch_size * 0.8)

data = next(iter(dataloader))
inputs, labels = data
train_inputs = inputs[:validation_index]
validation_inputs = inputs[validation_index:]
train_labels = labels[:validation_index]
validation_labels = labels[validation_index:]

train_loss = 999
stat = {"train_loss":[], "validation_loss":[]}
epoch = 0



results_path = 'results/cnnclassifier50epochs.pt'
statsrec = np.zeros((4,1))


net = Simple_CNN()
loss_fn = nn.CrossEntropyLoss()
optimizer = optim.Adam(net.parameters(), lr=0.001)
# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.7)
114/11:
def accuracy(outputs, labels):
    _, predicted = torch.max(outputs.data, 1)
    correct = (predicted == labels).sum().item()
    return correct/labels.shape[0]
    

while round(train_loss,3) != 0.0:
        
    optimizer.zero_grad()
    train_output = net(train_inputs)
    loss = loss_fn(train_output, train_labels)
    loss.backward()
    optimizer.step()
#     scheduler.step()

    train_loss = loss.item()
    train_accuracy = accuracy(train_output, train_labels)
    
    
    validation_output = net(validation_inputs)
    validation_loss = loss_fn(validation_output, validation_labels).item()
    validation_accuracy = accuracy(validation_output, validation_labels)
    
    stat["train_loss"].append(train_loss)
    stat["validation_loss"].append(validation_loss)
    print(f"epoch: {epoch} training loss: {train_loss: .3f} training accuracy: {train_accuracy: .1%} validation loss: {validation_loss: .3f} validation accuracy: {validation_accuracy: .1%}")
    epoch += 1

# save network parameters, losses and accuracy
torch.save({"state_dict": net.state_dict()}, results_path)
114/12:
plt.plot(stat["train_loss"], 'r', label = 'training loss', )
plt.plot(stat["validation_loss"], 'g', label = 'test loss' )
plt.xlabel('epoch')
plt.ylabel('loss')
plt.title('Training and test loss')
114/13:
from torch import nn, optim
batch_size = 64
validation_index = int(batch_size * 0.8)

data = next(iter(dataloader))
inputs, labels = data
train_inputs = inputs[:validation_index]
validation_inputs = inputs[validation_index:]
train_labels = labels[:validation_index]
validation_labels = labels[validation_index:]

train_loss = 999
stat = {"train_loss":[], "validation_loss":[]}
epoch = 0



results_path = 'results/cnnclassifier50epochs.pt'
statsrec = np.zeros((4,1))


net = Simple_CNN()
loss_fn = nn.CrossEntropyLoss()
optimizer = optim.Adam(net.parameters(), lr=0.001)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.7)
114/14:
def accuracy(outputs, labels):
    _, predicted = torch.max(outputs.data, 1)
    correct = (predicted == labels).sum().item()
    return correct/labels.shape[0]
    

while round(train_loss,3) != 0.0:
        
    optimizer.zero_grad()
    train_output = net(train_inputs)
    loss = loss_fn(train_output, train_labels)
    loss.backward()
    optimizer.step()
    scheduler.step()

    train_loss = loss.item()
    train_accuracy = accuracy(train_output, train_labels)
    
    
    validation_output = net(validation_inputs)
    validation_loss = loss_fn(validation_output, validation_labels).item()
    validation_accuracy = accuracy(validation_output, validation_labels)
    
    stat["train_loss"].append(train_loss)
    stat["validation_loss"].append(validation_loss)
    print(f"epoch: {epoch} training loss: {train_loss: .3f} training accuracy: {train_accuracy: .1%} validation loss: {validation_loss: .3f} validation accuracy: {validation_accuracy: .1%}")
    epoch += 1

# save network parameters, losses and accuracy
torch.save({"state_dict": net.state_dict()}, results_path)
114/15:
from torch import nn, optim
batch_size = 64
validation_index = int(batch_size * 0.8)

data = next(iter(dataloader))
inputs, labels = data
train_inputs = inputs[:validation_index]
validation_inputs = inputs[validation_index:]
train_labels = labels[:validation_index]
validation_labels = labels[validation_index:]

train_loss = 999
stat = {"train_loss":[], "validation_loss":[]}
epoch = 0



results_path = 'results/cnnclassifier50epochs.pt'
statsrec = np.zeros((4,1))


net = Simple_CNN()
loss_fn = nn.CrossEntropyLoss()
optimizer = optim.Adam(net.parameters(), lr=0.001)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.7)
114/16:
def accuracy(outputs, labels):
    _, predicted = torch.max(outputs.data, 1)
    correct = (predicted == labels).sum().item()
    return correct/labels.shape[0]
    

while round(train_loss,3) != 0.0:
        
    optimizer.zero_grad()
    train_output = net(train_inputs)
    loss = loss_fn(train_output, train_labels)
    loss.backward()
    optimizer.step()
    if epoch <= 340:
        scheduler.step()

    train_loss = loss.item()
    train_accuracy = accuracy(train_output, train_labels)
    
    
    validation_output = net(validation_inputs)
    validation_loss = loss_fn(validation_output, validation_labels).item()
    validation_accuracy = accuracy(validation_output, validation_labels)
    
    stat["train_loss"].append(train_loss)
    stat["validation_loss"].append(validation_loss)
    print(f"epoch: {epoch} training loss: {train_loss: .3f} training accuracy: {train_accuracy: .1%} validation loss: {validation_loss: .3f} validation accuracy: {validation_accuracy: .1%}")
    epoch += 1

# save network parameters, losses and accuracy
torch.save({"state_dict": net.state_dict()}, results_path)
114/17:
from torch import nn, optim
batch_size = 64
validation_index = int(batch_size * 0.8)

data = next(iter(dataloader))
inputs, labels = data
train_inputs = inputs[:validation_index]
validation_inputs = inputs[validation_index:]
train_labels = labels[:validation_index]
validation_labels = labels[validation_index:]

train_loss = 999
stat = {"train_loss":[], "validation_loss":[]}
epoch = 0



results_path = 'results/cnnclassifier50epochs.pt'
statsrec = np.zeros((4,1))


net = Simple_CNN()
loss_fn = nn.CrossEntropyLoss()
optimizer = optim.Adam(net.parameters(), lr=0.05)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.7)
114/18:
def accuracy(outputs, labels):
    _, predicted = torch.max(outputs.data, 1)
    correct = (predicted == labels).sum().item()
    return correct/labels.shape[0]
    

while round(train_loss,3) != 0.0:
        
    optimizer.zero_grad()
    train_output = net(train_inputs)
    loss = loss_fn(train_output, train_labels)
    loss.backward()
    optimizer.step()
    if epoch <= 340:
        scheduler.step()

    train_loss = loss.item()
    train_accuracy = accuracy(train_output, train_labels)
    
    
    validation_output = net(validation_inputs)
    validation_loss = loss_fn(validation_output, validation_labels).item()
    validation_accuracy = accuracy(validation_output, validation_labels)
    
    stat["train_loss"].append(train_loss)
    stat["validation_loss"].append(validation_loss)
    print(f"epoch: {epoch} training loss: {train_loss: .3f} training accuracy: {train_accuracy: .1%} validation loss: {validation_loss: .3f} validation accuracy: {validation_accuracy: .1%}")
    epoch += 1

# save network parameters, losses and accuracy
torch.save({"state_dict": net.state_dict()}, results_path)
114/19:
plt.plot(stat["train_loss"], 'r', label = 'training loss', )
plt.plot(stat["validation_loss"], 'g', label = 'test loss' )
plt.xlabel('epoch')
plt.ylabel('loss')
plt.title('Training and test loss')
114/20:
from torch import nn, optim

class foundation():
    def __init__():
batch_size = 64
validation_index = int(batch_size * 0.8)

data = next(iter(dataloader))
inputs, labels = data
train_inputs = inputs[:validation_index]
validation_inputs = inputs[validation_index:]
train_labels = labels[:validation_index]
validation_labels = labels[validation_index:]

train_loss = 999
stat = {"train_loss":[], "validation_loss":[]}
epoch = 0



results_path = 'results/cnnclassifier50epochs.pt'


net = Simple_CNN()
loss_fn = nn.CrossEntropyLoss()
optimizer = optim.Adam(net.parameters(), lr=0.05)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.7)
114/21:
from torch import nn, optim

class foundation():
    def __init__(self,dataloader, validation_ratio=0.8):    
        self.batch_size = 64
        self.validation_index = int(self.batch_size * validation_ratio)
        self.stat = {"train_loss":[], "validation_loss":[]}
        self.epoch = 0
        self.results_path = 'results/cnnclassifier50epochs.pt'
        self.dataloader = dataloader
        
    def simple_net_init(self):     
        self.net = Simple_CNN()
        self.loss_fn = nn.CrossEntropyLoss()
        self.optimizer = optim.Adam(self.net.parameters(), lr=0.05)
        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=30, gamma=0.7)
        
        
    def single_batch_init(self):
        data = next(iter(dataloader))
        inputs, labels = data
        train_inputs = inputs[:validation_index]
        validation_inputs = inputs[validation_index:]
        train_labels = labels[:validation_index]
        validation_labels = labels[validation_index:]
        train_loss = 999
        
    def accuracy(outputs, labels):
        _, predicted = torch.max(outputs.data, 1)
        correct = (predicted == labels).sum().item()
        return correct/labels.shape[0]
114/22:
from torch import nn, optim

class Foundation_Classification():
    def __init__(self,dataloader, validation_ratio=0.8):    
        self.batch_size = 64
        self.validation_index = int(self.batch_size * validation_ratio)
        self.stat = {"train_loss":[], "validation_loss":[]}
        self.epoch = 0
        self.results_path = 'results/cnnclassifier50epochs.pt'
        self.dataloader = dataloader
        
    def simple_net_init(self):     
        self.net = Simple_CNN()
        self.loss_fn = nn.CrossEntropyLoss()
        self.optimizer = optim.Adam(self.net.parameters(), lr=0.05)
        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=30, gamma=0.7)
        
        
    def single_batch_init(self):
        data = next(iter(dataloader))
        inputs, labels = data
        train_inputs = inputs[:validation_index]
        validation_inputs = inputs[validation_index:]
        train_labels = labels[:validation_index]
        validation_labels = labels[validation_index:]
        train_loss = 999
        
    def accuracy(outputs, labels):
        _, predicted = torch.max(outputs.data, 1)
        correct = (predicted == labels).sum().item()
        return correct/labels.shape[0]

class Simple_Batch(Foundation_Classification):
    def __init__(self,dataloader, validation_ratio=0.8):
        super.__init__(self,dataloader, validation_ratio)
114/23:
from torch import nn, optim

class Foundation_Classification():
    def __init__(self,dataloader, validation_ratio=0.8):    
        self.batch_size = 64
        self.validation_index = int(self.batch_size * validation_ratio)
        self.stat = {"train_loss":[], "validation_loss":[]}
        self.epoch = 0
        self.results_path = 'results/cnnclassifier50epochs.pt'
        self.dataloader = dataloader
        
    def simple_net_init(self):     
        self.net = Simple_CNN()
        self.loss_fn = nn.CrossEntropyLoss()
        self.optimizer = optim.Adam(self.net.parameters(), lr=0.05)
        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=30, gamma=0.7)
        
        
    def single_batch_init(self):
        data = next(iter(dataloader))
        inputs, labels = data
        train_inputs = inputs[:validation_index]
        validation_inputs = inputs[validation_index:]
        train_labels = labels[:validation_index]
        validation_labels = labels[validation_index:]
        train_loss = 999
        
    def accuracy(outputs, labels):
        _, predicted = torch.max(outputs.data, 1)
        correct = (predicted == labels).sum().item()
        return correct/labels.shape[0]

class Simple_Batch(Foundation_Classification):
    def __init__(self,dataloader, validation_ratio=0.8):
        super.__init__(self,dataloader, validation_ratio)
        
a = Simple_Batch(dataloader)
114/24:
from torch import nn, optim

class Foundation_Classification():
    def __init__(self,dataloader, validation_ratio=0.8):    
        self.batch_size = 64
        self.validation_index = int(self.batch_size * validation_ratio)
        self.stat = {"train_loss":[], "validation_loss":[]}
        self.epoch = 0
        self.results_path = 'results/cnnclassifier50epochs.pt'
        self.dataloader = dataloader
        
    def simple_net_init(self):     
        self.net = Simple_CNN()
        self.loss_fn = nn.CrossEntropyLoss()
        self.optimizer = optim.Adam(self.net.parameters(), lr=0.05)
        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=30, gamma=0.7)
        
        
    def single_batch_init(self):
        data = next(iter(dataloader))
        inputs, labels = data
        train_inputs = inputs[:validation_index]
        validation_inputs = inputs[validation_index:]
        train_labels = labels[:validation_index]
        validation_labels = labels[validation_index:]
        train_loss = 999
        
    def accuracy(outputs, labels):
        _, predicted = torch.max(outputs.data, 1)
        correct = (predicted == labels).sum().item()
        return correct/labels.shape[0]

class Simple_Batch(Foundation_Classification):
    def __init__(self,dataloader, validation_ratio=0.8):
        super.__init__(dataloader, validation_ratio)
        
a = Simple_Batch(dataloader)
114/25:
from torch import nn, optim

class Foundation_Classification():
    def __init__(self,dataloader, validation_ratio=0.8):    
        self.batch_size = 64
        self.validation_index = int(self.batch_size * validation_ratio)
        self.stat = {"train_loss":[], "validation_loss":[]}
        self.epoch = 0
        self.results_path = 'results/cnnclassifier50epochs.pt'
        self.dataloader = dataloader
        
    def simple_net_init(self):     
        self.net = Simple_CNN()
        self.loss_fn = nn.CrossEntropyLoss()
        self.optimizer = optim.Adam(self.net.parameters(), lr=0.05)
        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=30, gamma=0.7)
        
        
    def single_batch_init(self):
        data = next(iter(dataloader))
        inputs, labels = data
        train_inputs = inputs[:validation_index]
        validation_inputs = inputs[validation_index:]
        train_labels = labels[:validation_index]
        validation_labels = labels[validation_index:]
        train_loss = 999
        
    def accuracy(outputs, labels):
        _, predicted = torch.max(outputs.data, 1)
        correct = (predicted == labels).sum().item()
        return correct/labels.shape[0]

class Simple_Batch(Foundation_Classification):
    def __init__(self,dataloader, validation_ratio=0.8):
        super(self, dataloader, validation_ratio).__init__()
        
a = Simple_Batch(dataloader)
114/26:
from torch import nn, optim

class Foundation_Classification():
    def __init__(self,dataloader, validation_ratio=0.8):    
        self.batch_size = 64
        self.validation_index = int(self.batch_size * validation_ratio)
        self.stat = {"train_loss":[], "validation_loss":[]}
        self.epoch = 0
        self.results_path = 'results/cnnclassifier50epochs.pt'
        self.dataloader = dataloader
        
    def simple_net_init(self):     
        self.net = Simple_CNN()
        self.loss_fn = nn.CrossEntropyLoss()
        self.optimizer = optim.Adam(self.net.parameters(), lr=0.05)
        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=30, gamma=0.7)
        
        
    def single_batch_init(self):
        data = next(iter(dataloader))
        inputs, labels = data
        train_inputs = inputs[:validation_index]
        validation_inputs = inputs[validation_index:]
        train_labels = labels[:validation_index]
        validation_labels = labels[validation_index:]
        train_loss = 999
        
    def accuracy(outputs, labels):
        _, predicted = torch.max(outputs.data, 1)
        correct = (predicted == labels).sum().item()
        return correct/labels.shape[0]

class Simple_Batch(Foundation_Classification):
    def __init__(self,dataloader, validation_ratio=0.8):
        super(dataloader, validation_ratio).__init__()
        
a = Simple_Batch(dataloader)
114/27:
from torch import nn, optim

class Foundation_Classification():
    def __init__(self,dataloader, validation_ratio=0.8):    
        self.batch_size = 64
        self.validation_index = int(self.batch_size * validation_ratio)
        self.stat = {"train_loss":[], "validation_loss":[]}
        self.epoch = 0
        self.results_path = 'results/cnnclassifier50epochs.pt'
        self.dataloader = dataloader
        
    def simple_net_init(self):     
        self.net = Simple_CNN()
        self.loss_fn = nn.CrossEntropyLoss()
        self.optimizer = optim.Adam(self.net.parameters(), lr=0.05)
        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=30, gamma=0.7)
        
        
    def single_batch_init(self):
        data = next(iter(dataloader))
        inputs, labels = data
        train_inputs = inputs[:validation_index]
        validation_inputs = inputs[validation_index:]
        train_labels = labels[:validation_index]
        validation_labels = labels[validation_index:]
        train_loss = 999
        
    def accuracy(outputs, labels):
        _, predicted = torch.max(outputs.data, 1)
        correct = (predicted == labels).sum().item()
        return correct/labels.shape[0]

class Simple_Batch(Foundation_Classification):
    def __init__(self,dataloader, validation_ratio=0.8):
        super(self).__init__(dataloader, validation_ratio)
        
a = Simple_Batch(dataloader)
114/28:
from torch import nn, optim

class Foundation_Classification():
    def __init__(self,dataloader, validation_ratio=0.8):    
        self.batch_size = 64
        self.validation_index = int(self.batch_size * validation_ratio)
        self.stat = {"train_loss":[], "validation_loss":[]}
        self.epoch = 0
        self.results_path = 'results/cnnclassifier50epochs.pt'
        self.dataloader = dataloader
        
    def simple_net_init(self):     
        self.net = Simple_CNN()
        self.loss_fn = nn.CrossEntropyLoss()
        self.optimizer = optim.Adam(self.net.parameters(), lr=0.05)
        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=30, gamma=0.7)
        
        
    def single_batch_init(self):
        data = next(iter(dataloader))
        inputs, labels = data
        train_inputs = inputs[:validation_index]
        validation_inputs = inputs[validation_index:]
        train_labels = labels[:validation_index]
        validation_labels = labels[validation_index:]
        train_loss = 999
        
    def accuracy(outputs, labels):
        _, predicted = torch.max(outputs.data, 1)
        correct = (predicted == labels).sum().item()
        return correct/labels.shape[0]

class Simple_Batch(Foundation_Classification):
    def __init__(self,dataloader, validation_ratio=0.8):
        super().__init__(dataloader, validation_ratio)
        
a = Simple_Batch(dataloader)
114/29:
from torch import nn, optim

class Foundation_Classification():
    def __init__(self,dataloader, validation_ratio=0.8):    
        self.batch_size = 64
        self.validation_index = int(self.batch_size * validation_ratio)
        self.stat = {"train_loss":[], "validation_loss":[]}
        self.epoch = 0
        self.results_path = 'results/cnnclassifier50epochs.pt'
        self.dataloader = dataloader
        
    def simple_net_init(self):     
        self.net = Simple_CNN()
        self.loss_fn = nn.CrossEntropyLoss()
        self.optimizer = optim.Adam(self.net.parameters(), lr=0.05)
        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=30, gamma=0.7)
        
        
    def single_batch_init(self):
        data = next(iter(dataloader))
        inputs, labels = data
        train_inputs = inputs[:validation_index]
        validation_inputs = inputs[validation_index:]
        train_labels = labels[:validation_index]
        validation_labels = labels[validation_index:]
        train_loss = 999
        
    def accuracy(outputs, labels):
        _, predicted = torch.max(outputs.data, 1)
        correct = (predicted == labels).sum().item()
        return correct/labels.shape[0]

class Simple_Batch(Foundation_Classification):
    def __init__(self,dataloader, validation_ratio=0.8):
        super().__init__(dataloader, validation_ratio)
        
a = Simple_Batch(dataloader)
a.simple_net_init()
114/30:
from torch import nn, optim

class Foundation_Classification():
    def __init__(self,dataloader, validation_ratio=0.8):    
        self.batch_size = 64
        self.validation_index = int(self.batch_size * validation_ratio)
        self.stat = {"train_loss":[], "validation_loss":[]}
        self.epoch = 0
        self.results_path = 'results/cnnclassifier50epochs.pt'
        self.dataloader = dataloader
        
    def simple_net_init(self):     
        self.net = Simple_CNN()
        self.loss_fn = nn.CrossEntropyLoss()
        self.optimizer = optim.Adam(self.net.parameters(), lr=0.05)
        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=30, gamma=0.7)
        
        
    def single_batch_init(self):
        data = next(iter(dataloader))
        inputs, labels = data
        train_inputs = inputs[:validation_index]
        validation_inputs = inputs[validation_index:]
        train_labels = labels[:validation_index]
        validation_labels = labels[validation_index:]
        train_loss = 999
        
    def accuracy(outputs, labels):
        _, predicted = torch.max(outputs.data, 1)
        correct = (predicted == labels).sum().item()
        return correct/labels.shape[0]

class Simple_Batch(Foundation_Classification):
    def __init__(self,dataloader, validation_ratio=0.8):
        super().__init__(dataloader, validation_ratio)
        
a = Simple_Batch(dataloader)
a.simple_net_init()
a.optimizer
114/31:
from torch import nn, optim

class Foundation_Classification():
    def __init__(self,dataloader, validation_ratio=0.8):    
        self.batch_size = 64
        self.validation_index = int(self.batch_size * validation_ratio)
        self.stat = {"train_loss":[], "validation_loss":[]}
        self.epoch = 0
        self.results_path = 'results/cnnclassifier50epochs.pt'
        self.dataloader = dataloader
        
    def simple_net_init(self):     
        self.net = Simple_CNN()
        self.loss_fn = nn.CrossEntropyLoss()
        self.optimizer = optim.Adam(self.net.parameters(), lr=0.05)
        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=30, gamma=0.7)
        
        
    def single_batch_init(self):
        data = next(iter(dataloader))
        inputs, labels = data
        train_inputs = inputs[:validation_index]
        validation_inputs = inputs[validation_index:]
        train_labels = labels[:validation_index]
        validation_labels = labels[validation_index:]
        train_loss = 999
        
    def accuracy(outputs, labels):
        _, predicted = torch.max(outputs.data, 1)
        correct = (predicted == labels).sum().item()
        return correct/labels.shape[0]

class Simple_Batch(Foundation_Classification):
    def train():
        pass
        
a = Simple_Batch(dataloader)
114/32:
from torch import nn, optim

class Foundation_Classification():
    def __init__(self,dataloader, validation_ratio=0.8):    
        self.batch_size = 64
        self.validation_index = int(self.batch_size * validation_ratio)
        self.stat = {"train_loss":[], "validation_loss":[]}
        self.epoch = 0
        self.results_path = 'results/cnnclassifier50epochs.pt'
        self.dataloader = dataloader
        
    def simple_fig_init(self):     
        self.net = Simple_CNN()
        self.loss_fn = nn.CrossEntropyLoss()
        self.optimizer = optim.Adam(self.net.parameters(), lr=0.05)
        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=30, gamma=0.7)
        
        
    def single_batch_init(self):
        data = next(iter(dataloader))
        inputs, labels = data
        train_inputs = inputs[:validation_index]
        validation_inputs = inputs[validation_index:]
        train_labels = labels[:validation_index]
        validation_labels = labels[validation_index:]
        train_loss = 999
        
    def accuracy(outputs, labels):
        _, predicted = torch.max(outputs.data, 1)
        correct = (predicted == labels).sum().item()
        return correct/labels.shape[0]
    
    def train(inputs, labels):
        self.optimizer.zero_grad()
        outputs = self.net(inputs)
        loss = self.loss_fn(outputs, labels)
        loss.backward()
        self.optimizer.step()
        return loss.item()
    
    def validate(inputs, labels):
        outputs = self.net(inputs)
        return loss_fn(outputs, labels).item()
114/33:
from torch import nn, optim

class Foundation_Classification():
    def __init__(self,dataloader, validation_ratio=0.8):    
        self.batch_size = 64
        self.validation_index = int(self.batch_size * validation_ratio)
        self.stat = {"train_loss":[], "validation_loss":[]}
        self.results_path = 'results/cnnclassifier50epochs.pt'
        self.dataloader = dataloader
        
    def simple_fig_init(self):     
        self.net = Simple_CNN()
        self.loss_fn = nn.CrossEntropyLoss()
        self.optimizer = optim.Adam(self.net.parameters(), lr=0.05)
        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=30, gamma=0.7)
        
    def accuracy(outputs, labels):
        _, predicted = torch.max(outputs.data, 1)
        correct = (predicted == labels).sum().item()
        return correct/labels.shape[0]
    
    def train(inputs, labels):
        self.optimizer.zero_grad()
        outputs = self.net(inputs)
        loss = self.loss_fn(outputs, labels)
        loss.backward()
        self.optimizer.step()
        return loss.item()
    
    def validate(inputs, labels):
        outputs = self.net(inputs)
        return self.loss_fn(outputs, labels).item()
114/34:
from torch import nn, optim

class Foundation_Classification():
    def __init__(self,dataloader, validation_ratio=0.8):    
        self.batch_size = 64
        self.validation_index = int(self.batch_size * validation_ratio)
        self.stat = {"train_loss":[], "validation_loss":[]}
        self.results_path = 'results/cnnclassifier50epochs.pt'
        self.dataloader = dataloader
        
    def simple_fig_init(self):     
        self.net = Simple_CNN()
        self.loss_fn = nn.CrossEntropyLoss()
        self.optimizer = optim.Adam(self.net.parameters(), lr=0.05)
        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=30, gamma=0.7)
        
    def accuracy(self, outputs, labels):
        _, predicted = torch.max(outputs.data, 1)
        correct = (predicted == labels).sum().item()
        return correct/labels.shape[0]
    
    def train(self, inputs, labels):
        self.optimizer.zero_grad()
        outputs = self.net(inputs)
        loss = self.loss_fn(outputs, labels)
        loss.backward()
        self.optimizer.step()
        return loss.item()
    
    def validate(self, inputs, labels):
        outputs = self.net(inputs)
        return self.loss_fn(outputs, labels).item()
    
    def plot(self):
        plt.plot(self.stat["train_loss"], 'r', label = 'training loss', )
        plt.plot(self.stat["validation_loss"], 'g', label = 'test loss' )
        plt.xlabel('epoch')
        plt.ylabel('loss')
        plt.title('Training and test loss')
114/35:
class Simple_Batch(Foundation_Classification):
    
    def training(): 
        data = next(iter(self.dataloader))
        inputs, labels = data
        train_inputs = inputs[:self.validation_index]
        validation_inputs = inputs[self.validation_index:]
        train_labels = labels[:self.validation_index]
        validation_labels = labels[self.validation_index:]
        train_loss = 999
        
        epoch = 0
        
        while round(train_loss,3) != 0.0:

            train_loss = self.train(train_inputs, train_labels)
            if epoch <= 340:
                sel.fscheduler.step()


            validation_loss = self.validate(validation_inputs, validation_labels)

            self.stat["train_loss"].append(train_loss)
            self.stat["validation_loss"].append(validation_loss)
            print(f"epoch: {epoch} training loss: {train_loss: .3f} validation loss: {validation_loss: .3f}")
            epoch += 1
            
        torch.save({"state_dict": net.state_dict()}, results_path)
114/36:
a = Simple_Batch(dataloader)
a.training()
a.plot()
114/37:
class Simple_Batch(Foundation_Classification):
    
    def training(self): 
        data = next(iter(self.dataloader))
        inputs, labels = data
        train_inputs = inputs[:self.validation_index]
        validation_inputs = inputs[self.validation_index:]
        train_labels = labels[:self.validation_index]
        validation_labels = labels[self.validation_index:]
        train_loss = 999
        
        epoch = 0
        
        while round(train_loss,3) != 0.0:

            train_loss = self.train(train_inputs, train_labels)
            if epoch <= 340:
                sel.fscheduler.step()


            validation_loss = self.validate(validation_inputs, validation_labels)

            self.stat["train_loss"].append(train_loss)
            self.stat["validation_loss"].append(validation_loss)
            print(f"epoch: {epoch} training loss: {train_loss: .3f} validation loss: {validation_loss: .3f}")
            epoch += 1
            
        torch.save({"state_dict": net.state_dict()}, results_path)
114/38:
a = Simple_Batch(dataloader)
a.training()
a.plot()
114/39:
from torch import nn, optim

class Foundation_Classification():
    def __init__(self,dataloader, validation_ratio=0.8):    
        self.batch_size = 64
        self.validation_index = int(self.batch_size * validation_ratio)
        self.stat = {"train_loss":[], "validation_loss":[]}
        self.results_path = 'results/cnnclassifier50epochs.pt'
        self.dataloader = dataloader
        
    def simple_fig_init(self):     
        self.net = Simple_CNN()
        self.loss_fn = nn.CrossEntropyLoss()
        self.optimizer = optim.Adam(self.net.parameters(), lr=0.05)
        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=30, gamma=0.7)
        
    def accuracy(self, outputs, labels):
        _, predicted = torch.max(outputs.data, 1)
        correct = (predicted == labels).sum().item()
        return correct/labels.shape[0]
    
    def train(self, inputs, labels):
        self.optimizer.zero_grad()
        outputs = self.net(inputs)
        loss = self.loss_fn(outputs, labels)
        loss.backward()
        self.optimizer.step()
        return loss.item()
    
    def validate(self, inputs, labels):
        outputs = self.net(inputs)
        return self.loss_fn(outputs, labels).item()
    
    def plot(self):
        plt.plot(self.stat["train_loss"], 'r', label = 'training loss', )
        plt.plot(self.stat["validation_loss"], 'g', label = 'test loss' )
        plt.xlabel('epoch')
        plt.ylabel('loss')
        plt.title('Training and test loss')
114/40:
class Simple_Batch(Foundation_Classification):
    def __init__(self, dataloader, validation_ratio=0.8):
        super().__init__()
        self.simple_fig_init()
    
    def training(self): 
        data = next(iter(self.dataloader))
        inputs, labels = data
        train_inputs = inputs[:self.validation_index]
        validation_inputs = inputs[self.validation_index:]
        train_labels = labels[:self.validation_index]
        validation_labels = labels[self.validation_index:]
        train_loss = 999
        
        epoch = 0
        
        while round(train_loss,3) != 0.0:

            train_loss = self.train(train_inputs, train_labels)
            if epoch <= 340:
                sel.fscheduler.step()


            validation_loss = self.validate(validation_inputs, validation_labels)

            self.stat["train_loss"].append(train_loss)
            self.stat["validation_loss"].append(validation_loss)
            print(f"epoch: {epoch} training loss: {train_loss: .3f} validation loss: {validation_loss: .3f}")
            epoch += 1
            
        torch.save({"state_dict": net.state_dict()}, results_path)
114/41:
a = Simple_Batch(dataloader)
a.training()
a.plot()
114/42:
class Simple_Batch(Foundation_Classification):
    def __init__(self, dataloader, validation_ratio=0.8):
        super().__init__(dataloader, validation_ratio)
        self.simple_fig_init()
    
    def training(self): 
        data = next(iter(self.dataloader))
        inputs, labels = data
        train_inputs = inputs[:self.validation_index]
        validation_inputs = inputs[self.validation_index:]
        train_labels = labels[:self.validation_index]
        validation_labels = labels[self.validation_index:]
        train_loss = 999
        
        epoch = 0
        
        while round(train_loss,3) != 0.0:

            train_loss = self.train(train_inputs, train_labels)
            if epoch <= 340:
                sel.fscheduler.step()


            validation_loss = self.validate(validation_inputs, validation_labels)

            self.stat["train_loss"].append(train_loss)
            self.stat["validation_loss"].append(validation_loss)
            print(f"epoch: {epoch} training loss: {train_loss: .3f} validation loss: {validation_loss: .3f}")
            epoch += 1
            
        torch.save({"state_dict": net.state_dict()}, results_path)
114/43:
a = Simple_Batch(dataloader)
a.training()
a.plot()
114/44:
class Simple_Batch(Foundation_Classification):
    def __init__(self, dataloader, validation_ratio=0.8):
        super().__init__(dataloader, validation_ratio)
        self.simple_fig_init()
    
    def training(self): 
        data = next(iter(self.dataloader))
        inputs, labels = data
        train_inputs = inputs[:self.validation_index]
        validation_inputs = inputs[self.validation_index:]
        train_labels = labels[:self.validation_index]
        validation_labels = labels[self.validation_index:]
        train_loss = 999
        
        epoch = 0
        
        while round(train_loss,3) != 0.0:

            train_loss = self.train(train_inputs, train_labels)
            if epoch <= 340:
                self.scheduler.step()


            validation_loss = self.validate(validation_inputs, validation_labels)

            self.stat["train_loss"].append(train_loss)
            self.stat["validation_loss"].append(validation_loss)
            print(f"epoch: {epoch} training loss: {train_loss: .3f} validation loss: {validation_loss: .3f}")
            epoch += 1
            
        torch.save({"state_dict": net.state_dict()}, results_path)
114/45:
a = Simple_Batch(dataloader)
a.training()
a.plot()
114/46:
from torch import nn, optim

class Foundation_Classification():
    def __init__(self,dataloader, validation_ratio=0.8):    
        self.batch_size = 64
        self.validation_index = int(self.batch_size * validation_ratio)
        self.stat = {"train_loss":[], "validation_loss":[]}
        self.results_path = 'results/cnnclassifier50epochs.pt'
        self.dataloader = dataloader
        
    def simple_fig_init(self):     
        self.net = Simple_CNN()
        self.loss_fn = nn.CrossEntropyLoss()
        self.optimizer = optim.Adam(self.net.parameters(), lr=0.001)
        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=30, gamma=0.7)
        
    def accuracy(self, outputs, labels):
        _, predicted = torch.max(outputs.data, 1)
        correct = (predicted == labels).sum().item()
        return correct/labels.shape[0]
    
    def train(self, inputs, labels):
        self.optimizer.zero_grad()
        outputs = self.net(inputs)
        loss = self.loss_fn(outputs, labels)
        loss.backward()
        self.optimizer.step()
        return loss.item()
    
    def validate(self, inputs, labels):
        outputs = self.net(inputs)
        return self.loss_fn(outputs, labels).item()
    
    def plot(self):
        plt.plot(self.stat["train_loss"], 'r', label = 'training loss', )
        plt.plot(self.stat["validation_loss"], 'g', label = 'test loss' )
        plt.xlabel('epoch')
        plt.ylabel('loss')
        plt.title('Training and test loss')
114/47:
class Simple_Batch(Foundation_Classification):
    def __init__(self, dataloader, validation_ratio=0.8):
        super().__init__(dataloader, validation_ratio)
        self.simple_fig_init()
    
    def training(self): 
        data = next(iter(self.dataloader))
        inputs, labels = data
        train_inputs = inputs[:self.validation_index]
        validation_inputs = inputs[self.validation_index:]
        train_labels = labels[:self.validation_index]
        validation_labels = labels[self.validation_index:]
        train_loss = 999
        
        epoch = 0
        
        while round(train_loss,3) != 0.0:

            train_loss = self.train(train_inputs, train_labels)
            if epoch <= 340:
                self.scheduler.step()


            validation_loss = self.validate(validation_inputs, validation_labels)

            self.stat["train_loss"].append(train_loss)
            self.stat["validation_loss"].append(validation_loss)
            print(f"epoch: {epoch} training loss: {train_loss: .3f} validation loss: {validation_loss: .3f}")
            epoch += 1
            
        torch.save({"state_dict": net.state_dict()}, results_path)
114/48:
a = Simple_Batch(dataloader)
a.training()
a.plot()
114/49:
class Single_Batch(Foundation_Classification):
    def __init__(self, dataloader, validation_ratio=0.8):
        super().__init__(dataloader, validation_ratio)
        self.simple_fig_init()
    
    def training(self): 
        data = next(iter(self.dataloader))
        inputs, labels = data
        train_inputs = inputs[:self.validation_index]
        validation_inputs = inputs[self.validation_index:]
        train_labels = labels[:self.validation_index]
        validation_labels = labels[self.validation_index:]
        train_loss = 999
        
        epoch = 0
        
        while round(train_loss,3) != 0.0:

            train_loss = self.train(train_inputs, train_labels)
            if epoch <= 340:
                self.scheduler.step()


            validation_loss = self.validate(validation_inputs, validation_labels)

            self.stat["train_loss"].append(train_loss)
            self.stat["validation_loss"].append(validation_loss)
            print(f"epoch: {epoch} training loss: {train_loss: .3f} validation loss: {validation_loss: .3f}")
            epoch += 1
            
        torch.save({"state_dict": net.state_dict()}, results_path)
114/50:
a = Single_Batch(dataloader)
a.training()
a.plot()
114/51:
# TO COMPLETE
# define a Model class

class Simple_CNN(nn.Sequential):
    def __init__(self):
        super().__init__()
        layers = [    
            nn.Conv2d(in_channels=3,out_channels=3, kernel_size=3),    
            nn.ReLU(),
            nn.Conv2d(in_channels=3,out_channels=6, kernel_size=3),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(in_channels=6,out_channels=6, kernel_size=3),    
            nn.ReLU(),
            nn.Conv2d(in_channels=6,out_channels=12, kernel_size=3),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Flatten(),
            nn.Linear(12*13*13,512),
            nn.ReLU(),
            nn.Linear(512,200),
            nn.Linear(200,30)
        ]
        for idx, module in enumerate(layers):
            self.add_module(str(idx), module)
114/52:
from torch import nn, optim

class Foundation_Classification():
    def __init__(self,dataloader, validation_ratio=0.8):    
        self.batch_size = 64
        self.validation_index = int(self.batch_size * validation_ratio)
        self.stat = {"train_loss":[], "validation_loss":[]}
        self.results_path = 'results/cnnclassifier50epochs.pt'
        self.dataloader = dataloader
        
    def simple_fig_init(self):     
        self.net = Simple_CNN()
        self.loss_fn = nn.CrossEntropyLoss()
        self.optimizer = optim.Adam(self.net.parameters(), lr=0.001)
        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=30, gamma=0.7)
        
    def accuracy(self, outputs, labels):
        _, predicted = torch.max(outputs.data, 1)
        correct = (predicted == labels).sum().item()
        return correct/labels.shape[0]
    
    def train(self, inputs, labels):
        self.optimizer.zero_grad()
        outputs = self.net(inputs)
        loss = self.loss_fn(outputs, labels)
        loss.backward()
        self.optimizer.step()
        return loss.item()
    
    def validate(self, inputs, labels):
        outputs = self.net(inputs)
        return self.loss_fn(outputs, labels).item()
    
    def plot(self):
        plt.plot(self.stat["train_loss"], 'r', label = 'training loss', )
        plt.plot(self.stat["validation_loss"], 'g', label = 'test loss' )
        plt.xlabel('epoch')
        plt.ylabel('loss')
        plt.title('Training and test loss')
114/53:
class Single_Batch(Foundation_Classification):
    def __init__(self, dataloader, validation_ratio=0.8):
        super().__init__(dataloader, validation_ratio)
        self.simple_fig_init()
    
    def training(self): 
        data = next(iter(self.dataloader))
        inputs, labels = data
        train_inputs = inputs[:self.validation_index]
        validation_inputs = inputs[self.validation_index:]
        train_labels = labels[:self.validation_index]
        validation_labels = labels[self.validation_index:]
        train_loss = 999
        
        epoch = 0
        
        while round(train_loss,3) != 0.0:

            train_loss = self.train(train_inputs, train_labels)
            if epoch <= 340:
                self.scheduler.step()


            validation_loss = self.validate(validation_inputs, validation_labels)

            self.stat["train_loss"].append(train_loss)
            self.stat["validation_loss"].append(validation_loss)
            print(f"epoch: {epoch} training loss: {train_loss: .3f} validation loss: {validation_loss: .3f}")
            epoch += 1
            
        torch.save({"state_dict": net.state_dict()}, results_path)
114/54:
a = Single_Batch(dataloader)
a.training()
a.plot()
114/55:
# print(next(iter(dataloader))[0].shape) 
print(dataloader.shape)
114/56:
# print(next(iter(dataloader))[0].shape) 
print(dataloader.size)
114/57:
# print(next(iter(dataloader))[0].shape) 
print(dataset.shape)
114/58:
# print(next(iter(dataloader))[0].shape) 
print(dataset.size)
114/59:
# print(next(iter(dataloader))[0].shape) 
print(len(dataset))
114/60:
dataset = ImageFolder(root="data/train_set/train_set/", transform=transforms.ToTensor())
dataloader = DataLoader(dataset, batch_size=64, shuffle=True)
114/61:
# print(next(iter(dataloader))[0].shape) 
print(len(dataset))
114/62:
# TO COMPLETE
# define a Model class

class Simple_CNN(nn.Sequential):
    def __init__(self):
        super().__init__()
        layers = [    
            nn.Conv2d(in_channels=3,out_channels=3, kernel_size=3),    
            nn.ReLU(),
            nn.Conv2d(in_channels=3,out_channels=6, kernel_size=3),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(in_channels=6,out_channels=6, kernel_size=3),    
            nn.ReLU(),
            nn.Conv2d(in_channels=6,out_channels=12, kernel_size=3),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Flatten(),
            nn.Linear(12*13*13,512),
            nn.ReLU(),
            nn.Linear(512,200),
            nn.Linear(200,30)
        ]
        for idx, module in enumerate(layers):
            self.add_module(str(idx), module)
114/63:
from torch import nn, optim

class Foundation_Classification():
    def __init__(self,dataloader, validation_ratio=0.8):    
        self.batch_size = 64
        self.validation_index = int(self.batch_size * validation_ratio)
        self.stat = {"train_loss":[], "validation_loss":[]}
        self.results_path = 'results/cnnclassifier50epochs.pt'
        self.dataloader = dataloader
        
    def simple_fig_init(self):     
        self.net = Simple_CNN()
        self.loss_fn = nn.CrossEntropyLoss()
        self.optimizer = optim.Adam(self.net.parameters(), lr=0.001)
        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=30, gamma=0.7)
        
    def accuracy(self, outputs, labels):
        _, predicted = torch.max(outputs.data, 1)
        correct = (predicted == labels).sum().item()
        return correct/labels.shape[0]
    
    def train(self, inputs, labels):
        self.optimizer.zero_grad()
        outputs = self.net(inputs)
        loss = self.loss_fn(outputs, labels)
        loss.backward()
        self.optimizer.step()
        return loss.item()
    
    def validate(self, inputs, labels):
        outputs = self.net(inputs)
        return self.loss_fn(outputs, labels).item()
    
    def plot(self):
        plt.plot(self.stat["train_loss"], 'r', label = 'training loss', )
        plt.plot(self.stat["validation_loss"], 'g', label = 'test loss' )
        plt.xlabel('epoch')
        plt.ylabel('loss')
        plt.title('Training and test loss')
114/64:
from torch import nn, optim

class Foundation_Classification():
    def __init__(self,dataloader, validation_ratio=0.8):    
        self.batch_size = 64
        self.validation_index = int(self.batch_size * validation_ratio)
        self.stat = {"train_loss":[], "validation_loss":[]}
        self.results_path = 'results/cnnclassifier50epochs.pt'
        self.dataloader = dataloader
        
    def simple_fig_init(self):     
        self.net = Simple_CNN()
        self.loss_fn = nn.CrossEntropyLoss()
        self.optimizer = optim.Adam(self.net.parameters(), lr=0.05)
        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=30, gamma=0.7)
        
    def accuracy(self, outputs, labels):
        _, predicted = torch.max(outputs.data, 1)
        correct = (predicted == labels).sum().item()
        return correct/labels.shape[0]
    
    def train(self, inputs, labels):
        self.optimizer.zero_grad()
        outputs = self.net(inputs)
        loss = self.loss_fn(outputs, labels)
        loss.backward()
        self.optimizer.step()
        return loss.item()
    
    def validate(self, inputs, labels):
        outputs = self.net(inputs)
        return self.loss_fn(outputs, labels).item()
    
    def plot(self):
        plt.plot(self.stat["train_loss"], 'r', label = 'training loss', )
        plt.plot(self.stat["validation_loss"], 'g', label = 'test loss' )
        plt.xlabel('epoch')
        plt.ylabel('loss')
        plt.title('Training and test loss')
114/65:
class Single_Batch(Foundation_Classification):
    def __init__(self, dataloader, validation_ratio=0.8):
        super().__init__(dataloader, validation_ratio)
        self.simple_fig_init()
    
    def training(self): 
        data = next(iter(self.dataloader))
        inputs, labels = data
        train_inputs = inputs[:self.validation_index]
        validation_inputs = inputs[self.validation_index:]
        train_labels = labels[:self.validation_index]
        validation_labels = labels[self.validation_index:]
        train_loss = 999
        
        epoch = 0
        
        while round(train_loss,3) != 0.0:

            train_loss = self.train(train_inputs, train_labels)
            if epoch <= 340:
                self.scheduler.step()


            validation_loss = self.validate(validation_inputs, validation_labels)

            self.stat["train_loss"].append(train_loss)
            self.stat["validation_loss"].append(validation_loss)
            print(f"epoch: {epoch} training loss: {train_loss: .3f} validation loss: {validation_loss: .3f}")
            epoch += 1
            
        torch.save({"state_dict": net.state_dict()}, results_path)
114/66:
a = Single_Batch(dataloader)
a.training()
a.plot()
   1:
import cv2
import math

import numpy as np
import torch
import torch.nn as nn
import torchvision.transforms as transforms
from torch.hub import load_state_dict_from_url

from PIL import Image
import matplotlib.pyplot as plt
import os
os.environ["KMP_DUPLICATE_LIB_OK"]="TRUE"

%history
   2:
# TO COMPLETE
# define a Model class

class Simple_CNN(nn.Sequential):
    def __init__(self):
        super(nn.Sequential, self).__init__()
        layers = [    
            nn.Conv2d(in_channels=3,out_channels=3, kernel_size=3),    
            nn.ReLU(),
            nn.Conv2d(in_channels=3,out_channels=6, kernel_size=3),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(in_channels=6,out_channels=6, kernel_size=3),    
            nn.ReLU(),
            nn.Conv2d(in_channels=6,out_channels=12, kernel_size=3),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Flatten(),
            nn.Linear(12*13*13,512),
            nn.ReLU(),
            nn.Linear(512,200),
            nn.Linear(200,30)
        ]
        for idx, module in enumerate(layers):
            self.add_module(str(idx), module)
            

%history
   3:
# TO COMPLETE
# define a Model class

class Simple_CNN(nn.Sequential):
    def __init__(self):
        super(nn.Sequential, self).__init__()
        layers = [    
            nn.Conv2d(in_channels=3,out_channels=3, kernel_size=3),    
            nn.ReLU(),
            nn.Conv2d(in_channels=3,out_channels=6, kernel_size=3),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(in_channels=6,out_channels=6, kernel_size=3),    
            nn.ReLU(),
            nn.Conv2d(in_channels=6,out_channels=12, kernel_size=3),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Flatten(),
            nn.Linear(12*13*13,512),
            nn.ReLU(),
            nn.Linear(512,200),
            nn.Linear(200,30)
        ]
        for idx, module in enumerate(layers):
            self.add_module(str(idx), module)
   4:
from torch import nn, optim
batch_size = 64
validation_index = int(batch_size * 0.8)

data = next(iter(dataloader))
inputs, labels = data
train_inputs = inputs[:validation_index]
validation_inputs = inputs[validation_index:]
train_labels = labels[:validation_index]
validation_labels = labels[validation_index:]

train_loss = 10
epoch = 0

net = Simple_CNN()


results_path = 'results/cnnclassifier50epochs.pt'
statsrec = np.zeros((4,1))

loss_fn = nn.CrossEntropyLoss()
# optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)
optimizer = optim.Adam(net.parameters(), lr=0.001)
# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.7)


%history
   5:
# from torch import nn, optim
# batch_size = 64
# validation_index = int(batch_size * 0.8)

# data = next(iter(dataloader))
# inputs, labels = data
# train_inputs = inputs[:validation_index]
# validation_inputs = inputs[validation_index:]
# train_labels = labels[:validation_index]
# validation_labels = labels[validation_index:]

# train_loss = 10
# epoch = 0

# net = Simple_CNN()


# results_path = 'results/cnnclassifier50epochs.pt'
# statsrec = np.zeros((4,1))

# loss_fn = nn.CrossEntropyLoss()
# # optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)
# optimizer = optim.Adam(net.parameters(), lr=0.001)
# # scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.7)


%history
   6:
# from torch import nn, optim
# batch_size = 64
# validation_index = int(batch_size * 0.8)

# data = next(iter(dataloader))
# inputs, labels = data
# train_inputs = inputs[:validation_index]
# validation_inputs = inputs[validation_index:]
# train_labels = labels[:validation_index]
# validation_labels = labels[validation_index:]

# train_loss = 10
# epoch = 0

# net = Simple_CNN()


# results_path = 'results/cnnclassifier50epochs.pt'
# statsrec = np.zeros((4,1))

# loss_fn = nn.CrossEntropyLoss()
# # optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)
# optimizer = optim.Adam(net.parameters(), lr=0.001)
# # scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.7)


%history -g -f 1.ipyng
