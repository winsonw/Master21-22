{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier using PyTorch\n",
    "\n",
    "In this notebook we train an MLP classifier on the MINST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn, optim\n",
    "from torchvision import transforms, datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# replace with your own root directory\n",
    "ROOT=\"C:/Users/scsdch/OneDrive - University of Leeds/Teaching/ODL Deep Learning/workspace/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the MNIST dataset. This will download a copy to your machine on first use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f79ccfc899b9428ca3e1c0bc9858acf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c58c3ed5349c4746ab6aec8ccad50d40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1172b8893b6d401c9a2a2b12b07fe256",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9736d8f2bbe40e6a40628003265e349",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "Processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scsdch\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py:480: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:141.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Load the datasets\n",
    "train_set = torchvision.datasets.MNIST(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "\n",
    "test_set = torchvision.datasets.MNIST(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect some of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, label = train_set[4]\n",
    "print(data.size())\n",
    "print(label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create some iterable Data Loaders for easy iteration on mini-batches during training and testing. Also, initialise an array with the 10 class IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_set,\n",
    "    batch_size=24, # Forward pass only so batch size can be larger\n",
    "    shuffle=False,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "classes = np.arange(0, 10)\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show some images and labels as a sanity check.\n",
    "Use `torchvision.utils.make_grid` to create one image from a set of images. Note that this function converts single channel (grey-scale) tensors to have three channels. This is done by replicating the values into red, green and blue channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-9948dca45552>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# get a batch of random training examples (images and corresponding labels)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mdataiter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "def timshow(x):\n",
    "    xa = np.transpose(x.numpy(),(1,2,0))\n",
    "    plt.imshow(xa)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    return xa\n",
    "    \n",
    "# get a batch of random training examples (images and corresponding labels)\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# show images and labels\n",
    "print(images.size())\n",
    "timshow(torchvision.utils.make_grid(images))\n",
    "print(*labels.numpy())     # asterisk unpacks the ndarray\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a classifier\n",
    "Instead of defining the classifier function, loss function and parameter updates directly as we did in PyTorch.ipynb, it is convenient to use the `torch.nn` and `torch.optim` packages. These provide a simple way to build networks without losing sight of the iterative steps in gradient descent.\n",
    "\n",
    "First we construct the classifer function using the nn.Sequential wrapper that simply sequences the steps in the classifier function. In the case of a linear classifier there is just one nn.Linear layer. This is preceeded by `nn.Flatten` that vectorises a $28\\times28$ input image into a 1D vector of length $28*28$. We will also experiment with a two layer classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([300, 784])\n",
      "torch.Size([300])\n",
      "torch.Size([10, 300])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    \n",
    "    # single layer\n",
    "    #nn.Linear(28*28, 10)\n",
    "    \n",
    "    # two layers\n",
    "    nn.Linear(28*28, 300),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(300,10)\n",
    ")\n",
    "\n",
    "for param in net.parameters():\n",
    "    print(param.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the network. For the two-layer network you'll need at least 200 epochs. 50 epochs will be more than enough for the one-layer network, but we'll run for the same number of epochs as for the two-layer network to give the full curve in the plot below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nepochs = 200    # number of epochs\n",
    "results_path = ROOT+'results/linear1layer200epochs.pt'\n",
    "\n",
    "# initialise ndarray to store the mean loss in each epoch (on the training data)\n",
    "losses = np.zeros(nepochs)\n",
    "\n",
    "# Use a loss function and optimiser provided as part of PyTorch.\n",
    "# The chosen optimiser (Stochastic Gradient Descent with momentum) needs only to be given the parameters (weights and biases)\n",
    "# of the network and updates these when asked to perform an optimisation step below.\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "for epoch in range(nepochs):  # loop over the dataset multiple times\n",
    "\n",
    "    # initialise variables for mean loss calculation\n",
    "    running_loss = 0.0\n",
    "    n = 0\n",
    "    \n",
    "    for data in train_loader:\n",
    "        inputs, labels = data\n",
    "        \n",
    "        # Zero the parameter gradients to remove accumulated gradient from a previous iteration.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward, backward, and update parameters\n",
    "        outputs = net(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        # accumulate loss and increment minibatches\n",
    "        running_loss += loss.item()\n",
    "        n += 1\n",
    "       \n",
    "    # record the mean loss for this epoch and show progress\n",
    "    losses[epoch] = running_loss / n\n",
    "    print(f\"epoch: {epoch+1} loss: {losses[epoch] : .3f}\")\n",
    "    \n",
    "# save network parameters and losses\n",
    "torch.save({\"state_dict\": net.state_dict(), \"losses\": losses}, results_path)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the first dimension of inputs and outputs corresponds to a minibatch of examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"input size: {inputs.size()}, output size: {outputs.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the history of the loss function during training (mean loss in each epoch) for 1 and 2 layer models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = torch.load(ROOT+'results/linear1layer200epochs.pt')\n",
    "d2 = torch.load(ROOT+'results/linear2layer200epochs.pt')\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "plt.plot(d1[\"losses\"], 'r', label = '1 layer', )\n",
    "plt.plot(d2[\"losses\"], 'g', label = '2 layers' )\n",
    "plt.legend()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.title('Training loss for 1 and 2 layer classifiers')\n",
    "\n",
    "fig.savefig(\"training_loss_MNIST.svg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the trained classifier `net` perform on the test set? First define our performance measures in terms of a given confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(cnfm):\n",
    "    return cnfm.trace()/cnfm.sum((0,1))\n",
    "\n",
    "def recalls(cnfm):\n",
    "    return np.diag(cnfm)/cnfm.sum(1)\n",
    "\n",
    "def precisions(cnfm):\n",
    "    return np.diag(cnfm)/cnfm.sum(0)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the model on test data, build a confusion matrix and compute performance measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = torch.load(ROOT+'results/linear2layer200epochs.pt')\n",
    "# now load the parameter state into the current model (make sure this is the right model).\n",
    "net.load_state_dict(d[\"state_dict\"])\n",
    "\n",
    "# initialise confusion matrix\n",
    "nclasses = classes.shape[0]\n",
    "cnfm = np.zeros((nclasses,nclasses),dtype=int)\n",
    "\n",
    "# work without gradient computation since we are testing (i.e. no optimisation)\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        \n",
    "        # find the class with the highest output.\n",
    "        # note that the outputs are confidence values since we didn't need to apply softmax in our network\n",
    "        # (nn.crossentropyloss takes raw condifence values and does its own softmax)   \n",
    "        _, predicted = torch.max(outputs, 1)    \n",
    "       \n",
    "        \n",
    "        # accumulate into confusion matrix\n",
    "        for i in range(labels.size(0)):\n",
    "            cnfm[labels[i].item(),predicted[i].item()] += 1\n",
    "              \n",
    "print(\"Confusion matrix\")\n",
    "print(cnfm)\n",
    "\n",
    "# show confusion matrix as a grey-level image\n",
    "plt.imshow(cnfm, cmap='gray')\n",
    "\n",
    "# show per-class recall and precision\n",
    "print(f\"Accuracy: {accuracy(cnfm) :.1%}\")\n",
    "r = recalls(cnfm)\n",
    "p = precisions(cnfm)\n",
    "for i in range(nclasses):\n",
    "    print(f\"Class {classes[i]} : Precision {p[i] :.1%}  Recall {r[i] :.1%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a bespoke model class\n",
    "\n",
    "In the above, we have used the 'container' module `nn.Sequential` to define our network. To give more flexibility in the definition of the network, we can replace this with our own `nn.module` as below. Notice here, we have used this flexibility to perform the flattening ourselves instead of using `nn.Flatten` - this will be more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes=10):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = x.reshape(x.size(0), -1) # flatten the input\n",
    "        out = self.fc1(out)\n",
    "        return out\n",
    "    \n",
    "net = Classifier()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "Experiment with the impact on accuracy of (1) adding a third fully-connected linear layer to the network and (2) replacing `nn.Sigmoid` by `nn.ReLU`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
