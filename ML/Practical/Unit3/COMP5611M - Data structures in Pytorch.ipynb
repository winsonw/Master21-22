{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\"><img alt=\"Creative Commons Licence\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by/4.0/88x31.png\" /></a><br /><span xmlns:dct=\"http://purl.org/dc/terms/\" property=\"dct:title\">COMP5611M - Data Structures in Pytorch</span> by <span xmlns:cc=\"http://creativecommons.org/ns#\" property=\"cc:attributionName\">Marc de Kamps and University of Leeds</span> is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\">Creative Commons Attribution 4.0 International License</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "In this notebook we will explore data structures in PyTorch, highlight some of the commonalities and differences with Numpy arrays and will discuss *autograd*, a package that allows automatic differentiation so that gradient\n",
    "based optimisation rules can be derived automatically, rather than by the modeller. This implies that novel loss functions and architectures can be implemented easily without any need to adapt backpropagation algorithms.\n",
    "\n",
    "At the end of the nodebook you should be able to\n",
    "- Define data structures commonly used in neural networks as Pytoch tensors\n",
    "- Explain the main commonalities and differences between Pytorch tensors and numpy arrays\n",
    "- Define mathematical expressions commonly used in neural networks in Pytorch\n",
    "- Combine the expressions in network classes\n",
    "- Define a simple training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch introductory materials\n",
    "\n",
    "Much of the material presented here is a condensed version of the official PyTorch introduction, which can be found\n",
    "here: https://pytorch.org/tutorials/beginner/nlp/pytorch_tutorial.html\n",
    "\n",
    "This is also the authorative source: changes to PyTorch are likely to find their way into that material."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors\n",
    "\n",
    "Take a few minutes to go through this page:\n",
    "https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html\n",
    "\n",
    "We will repeat some of the information below and then use Tensors to implement a network that can implement the XOR problem. The XOR is a logical gate. Like the AND, learning the values of its truth table can be considered\n",
    "a data classification problem, but it cannot be learnt by a single perceptron.\n",
    "\n",
    "The truth table is as follows:\n",
    "\n",
    "\n",
    "| $x_1$ | $x_2$| o |\n",
    "| ------| -----|---|\n",
    "|  0    |    0 | 0 |\n",
    "|  0    |    1 | 1 |\n",
    "|  1    |    0 | 1 |\n",
    "|  1    |    1 | 0 |\n",
    "\n",
    "To create a network that is capable of handling the XOR problem we create a network with two input nodes, two hidden nodes and one output node. Moreover, we need an extra input in the input layer to represent the bias, and ditto for the hidden layer. This creates an architecture of (3, 3, 1), which means 3 nodes for the input, three nodes for the hidden node and 1 for the output.\n",
    "\n",
    "This in turn means that we need a $3 \\times 2$ matrix to connect the input layer to the hidden layer and a $3 \\times 1$ matrix to connect the hidden layer to the output layer.\n",
    "\n",
    "It is not very difficult to design a network by hand that solves this problem.\n",
    "\n",
    "\n",
    "<img src=\"xor.png\" width=\"200\" height=\"200\" />\n",
    "\n",
    "The two decision lines are $x+y = \\frac{1}{2}$ and $x+y = \\frac{3}{2}$. We construct a hidden layer of three nodes.\n",
    "One is a perceptron that implements the first decision line, the second implements the second decision line. The\n",
    "third is clamped to 1, so as to implement a bias for the output layer.\n",
    "\n",
    "We can then build an *OR* gate on top of the hidden layer ('the input lies below $x+y = \\frac{1}{2}$ or\n",
    "above $x+y=\\frac{3}{2}$; we can implement 'below' by multiplying all weights of the decision line by -1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing a Network by Hand\n",
    "\n",
    "Below, we will construct a network by hand, based on the solution principle sketched above. It is general not a good idea to create neural networks by hand, but this example will show that the essential calculations can be performed in PyTorch data structures that are not very different from Numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# we adopt the convention that node 0 is the threshold\n",
    "tensor_v = torch.tensor([[0.5, -1., -1.],[-1.5, 1., 1.]])\n",
    "tensor_w = torch.tensor([[0.5, -1., -1.]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most important operations in neural networks is matrix vector multiplication. Let us calculate the hidden layer values for a given input pattern. We will start with the pattern (0,0). Since we clamp node 0 to the value 1 in order to implement a bias, we create (1,0,0). *torch.matmul* provides matrix vector multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.5000, -1.5000])\n"
     ]
    }
   ],
   "source": [
    "input = torch.tensor([1.,0.,0.])\n",
    "hi=tensor_v@input\n",
    "print(hi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On this output, we need to apply the step, or Heaviside, function.\n",
    "\n",
    "\n",
    "**Exercise**: Research the Pytorch documentation to find out what role the argument *values* plays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 0.])\n"
     ]
    }
   ],
   "source": [
    "values = torch.tensor([0.5])\n",
    "h=torch.heaviside(hi, values)\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's investigate if our hidden layers comes out correctly: we have four input patterns\n",
    "and therefore four cases. You should verify that we expect the following:\n",
    "\n",
    "|input pattern|desired hidden layer response|\n",
    "|-------------|-----------------------------|\n",
    "| (1, 0, 0)   |         (1,0)               |\n",
    "| (1, 0, 1)   |         (0,0)               |\n",
    "| (1, 1, 0)   |         (0,0)               |\n",
    "| (1, 1, 1)   |         (0,1)               |\n",
    "\n",
    "We can interpret the hidden layer information as follows:  of the two nodes the first one expresses\n",
    "'below the line $x+y -\\frac{1}{2}$'. The second node expresses 'above the line $x+y -\\frac{3}{2}$'.\n",
    "\n",
    "We will now implement the hidden layer as a function for a given input pattern, and see if it predicts the correct value for each of the input patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 0.])\n",
      "tensor([0., 0.])\n",
      "tensor([0., 0.])\n",
      "tensor([0., 1.])\n"
     ]
    }
   ],
   "source": [
    "def hidden(input, tensor_v):\n",
    "    hi=tensor_v@input\n",
    "    values = torch.tensor([0.5])\n",
    "    h=torch.heaviside(hi, values)\n",
    "    return h\n",
    "\n",
    "input=torch.Tensor([1., 0., 0.])\n",
    "print(hidden(input,tensor_v))\n",
    "input=torch.Tensor([1., 0., 1.])\n",
    "print(hidden(input,tensor_v))\n",
    "input=torch.Tensor([1., 1., 0.])\n",
    "print(hidden(input,tensor_v))\n",
    "input=torch.Tensor([1., 1., 1.])\n",
    "print(hidden(input,tensor_v))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We have a hidden layer that already implicitly codes all information we need. However, we want a single output that classifies the input pattern with a single value that represents the output of the XOR gate. We can get what we want by implementing an OR gate. An OR gate can be implemented by a single perceptron:\n",
    "$$\n",
    "\\mathcal{H}(w_0\\cdot 1 + w_1 x_1 + w_2 x_2)\n",
    "$$\n",
    "You should verify that $w_0 = -\\frac{1}{2}, w_1 = 1, w_2 =1$ implements an OR gate.\n",
    "\n",
    "\n",
    "We need to extend the hidden node vector. We have calculated two hidden nodes, but node 0 is clamped to 1 by definition. And observe that we already took this into account when we defined the *w* tensor, which expects three nodes. Simply applying *matmul* to *h* will produce an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch picks up on the size mismatch\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    o=tensor_w@h\n",
    "except RuntimeError:\n",
    "    print('Pytorch picks up on the size mismatch')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several solutions are conceivable. One is to pre-create the hidden layer as a tensor of shape (1,3) and\n",
    "to update its last node by using an appropriate slicing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1.])\n",
      "tensor([1., 1., 0.])\n"
     ]
    }
   ],
   "source": [
    "h=torch.ones(3)\n",
    "print(h)\n",
    "h[1:]=torch.heaviside(hi,values)\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can evaluate the last layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.])\n"
     ]
    }
   ],
   "source": [
    "output = torch.heaviside(tensor_w@h,values)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn(input):\n",
    "    h=torch.ones(3)\n",
    "    h[1:]=torch.heaviside(tensor_v@input, values)\n",
    "    return torch.heaviside(tensor_w@h, values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([0.])\n"
     ]
    }
   ],
   "source": [
    "input1 = torch.tensor([1.,0.,0.])\n",
    "print(nn(input1))\n",
    "input2 = torch.tensor([1.,0.,1.])\n",
    "print(nn(input2))\n",
    "input3 = torch.tensor([1.,1.,0.])\n",
    "print(nn(input3))\n",
    "input4 = torch.tensor([1.,1.,1.])\n",
    "print(nn(input4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors and the GPU\n",
    "\n",
    "We absolutely don't recommend that you try to create networks like this. Setting weights by hand is impractical and you don't really want your networks to consists of a loosely coupled set of tensors. We will improve on these things below. Nonetheless, these calculations are representative for what happens under the hood of PyTorch. And already simply using Tensors allows you to involve the GPU, if you have one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Using device: cpu\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.6387, 0.5247, 0.6826, 0.3051, 0.4635, 0.4550, 0.5725, 0.4980, 0.9371,\n",
       "        0.6556])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.current_device()\n",
    "    torch.cuda.device(0)\n",
    "    torch.cuda.device_count()\n",
    "    torch.cuda.get_device_name(0)\n",
    "\n",
    "    # setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "torch.rand(10).to(device)     # move a tensor to device\n",
    "torch.rand(10, device=device) # create a tensor on device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Sigmoid Version of the XOR Network\n",
    "\n",
    "### Using Torch to represent a network\n",
    "\n",
    "To have network represented by a collection of matrices without any further organisation is undesirable. Torch offers a neural network module and it is good practice to derive from that. The code below is adapted from here:\n",
    "https://gist.githubusercontent.com/user01/68514db1127eb007f24d28bfd11dd60e/raw/f9d19c595aaf43dbd23ee1c1457eecbf3be59ae1/torch.xor.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 3, True)\n",
    "        self.fc2 = nn.Linear(3, 1, True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This network derives from an object called nn.Module, which is a neural network base class. If you haven't seen subclassing in Python before, just consider it as a bit of extra magic that you have to include, just as the call to the base class initialisation method, which is done through the *super* statement. Once these statements are included, building the network is straightforward enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This network is able to process input patterns of the right shape. Although the forward method is not called explicitly, it will be used in the evaluation of the input pattern.\n",
    "\n",
    "**Exercise**: Think of a way of convincing yourself (and possibly others) of a way to see that forward is being called in the statement below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8218], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.Tensor([1.,0.])\n",
    "net(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the documentation for nn.Linear, which is here:\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear,\n",
    "we find that Linear applies the following transformation:\n",
    "$$\n",
    "\\boldsymbol{y} = \\boldsymbol{x} \\boldsymbol{A}^T + b\n",
    "$$\n",
    "\n",
    "The dimensions are sepcified in the call to Linear:\n",
    "> <code>torch.nn.Linear(in_features, out_features, bias=True, device=None, dtype=None)</code>\n",
    "\n",
    "This call specifies the dimensions of both $\\boldsymbol{A}$ and $\\boldsymbol{b}$. Next, the documentation states something very important:\n",
    "\n",
    "The shape of the input is $(N, *, H_{in})$, and the output shape is $(N, *, H_{out})$, where $H_{in}$ is equal to\n",
    "<code>in_features</code> and $H_{out}$ is equal to <code>out_features</code>. \n",
    "\n",
    "In our particular example this means that nn.Linear(2,3) is, as expected something which takes  two dimensional\n",
    "pattern and spits out a three dimensional one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.3286,  0.6938, -0.2992], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "layer = nn.Linear(2,3)\n",
    "input = torch.Tensor([0.,0.])\n",
    "output = layer(input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But it also works on a *set* of input patterns of arbitrary size!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3286,  0.6938, -0.2992],\n",
      "        [-1.0009,  1.3147,  0.0034],\n",
      "        [-0.9729,  0.3528, -0.4170],\n",
      "        [-1.6452,  0.9737, -0.1143]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.Tensor([[0.,0.],[0.,1.],[1.,0.],[1.,1.]])\n",
    "output = layer(inputs)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where did we actually set the numerical values of this linear layer? We didn't! The documentation states that the numerical values are set by sampling from a uniform distribution when we create the layers. This makes perfect sense in the context of neural networks where we usually initialise weights by random values.\n",
    "\n",
    "Can we get access to the values though? Yes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.6443, -0.6723],\n",
      "        [-0.3411,  0.6209],\n",
      "        [-0.1178,  0.3026]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.3286,  0.6938, -0.2992], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(layer.weight),\n",
    "print(layer.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Verify by hand on an input pattern of your choice that the calculation of output patterns is as you expect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient information\n",
    "\n",
    "Just in passing, we mention that the data structures carry gradient information. In another notebook, we will discuss PyTorch's *autograd* functionality. Here, we just note that any expression that expression like linear\n",
    "which have been created wih <code>requires_grad=True</code> carry gradient information, and compound statements, such as neural network also carry gradient information. In other words, the neural network we created is not capable of calculating its output in response to input pattern, but also evaluate the gradient. This explains why in the code below optimisers like the <code>SGD</code> object can implement stochastic gradient descent almost for free."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inputs =  [torch.Tensor(input) for input in [\n",
    "    [0, 0],\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "    [1, 1]\n",
    "] ]\n",
    "           \n",
    "targets = [ torch.Tensor(output) for output in [\n",
    "    [0],\n",
    "    [1],\n",
    "    [1],\n",
    "    [0]\n",
    "] ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Training Loop\n",
    "\n",
    "The training loop is very simple. In the notebook on *autograd* we will explain the use of *forward* and *backward* calculations, but the upshot is that <code>loss.backward()</code> calculates the gradient of the loss function, which the optimizer then uses to perform a training step. In its construction, the network parameters have been defined as the thing to be optimised and a learning rate has been set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loop:\n",
      "Epoch        0 Loss: 0.5276403427124023\n",
      "Epoch      500 Loss: 0.2909414768218994\n",
      "Epoch     1000 Loss: 0.22681206464767456\n",
      "Epoch     1500 Loss: 0.0005891557666473091\n",
      "Epoch     2000 Loss: 4.389457686215792e-10\n",
      "Epoch     2500 Loss: 6.417089082333405e-12\n",
      "Epoch     3000 Loss: 6.417089082333405e-12\n",
      "Epoch     3500 Loss: 6.417089082333405e-12\n",
      "Epoch     4000 Loss: 6.417089082333405e-12\n",
      "Epoch     4500 Loss: 6.417089082333405e-12\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "EPOCHS_TO_TRAIN = 5000\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.05)\n",
    "\n",
    "print(\"Training loop:\")\n",
    "for idx in range(0, EPOCHS_TO_TRAIN):\n",
    "    for input, target in zip(inputs, targets):\n",
    "        optimizer.zero_grad()   # zero the gradient buffers\n",
    "        output = net(input)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()    # Does the update\n",
    "    if idx % 500 == 0:\n",
    "        print(\"Epoch {: >8} Loss: {}\".format(idx, loss.data.numpy()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final results:\n",
      "[0. 0.]\n",
      "Input:[0,0] Target:[0] Predicted:[0.0] Error:[0.0]\n",
      "[0. 1.]\n",
      "Input:[0,1] Target:[1] Predicted:[1.0] Error:[0.0]\n",
      "[1. 0.]\n",
      "Input:[1,0] Target:[1] Predicted:[1.0] Error:[0.0]\n",
      "[1. 1.]\n",
      "Input:[1,1] Target:[0] Predicted:[0.0] Error:[0.0]\n"
     ]
    }
   ],
   "source": [
    "print(\"Final results:\")\n",
    "for inputpattern, target in zip(inputs, targets):\n",
    "    output = net(inputpattern)\n",
    "    print(inputpattern.data.numpy())\n",
    "    print(\"Input:[{},{}] Target:[{}] Predicted:[{}] Error:[{}]\".format(\n",
    "        int(inputpattern.data.numpy()[0]),\n",
    "        int(inputpattern.data.numpy()[1]),\n",
    "        int(target.data.numpy()[0]),\n",
    "        round(float(output.data.numpy()[0]), 4),\n",
    "        round(float(abs(target.data.numpy()[0] - output.data.numpy()[0])), 4)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disclaimer\n",
    "\n",
    "XOR is not a very realistic data science problem. And as mentioned in the main text, we would not normally use the MSE loss function in classification.  The purpose of this exercise is to display the data structures that you may encounter in PyTorch and because theoretically steepest gradient descent is easy to understand on this problem - you could easily implement it in even a spreadsheet - it is easy to see what's going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
