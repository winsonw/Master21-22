{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\"><img alt=\"Creative Commons Licence\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by/4.0/88x31.png\" /></a><br /><span xmlns:dct=\"http://purl.org/dc/terms/\" property=\"dct:title\">COMP5611M - Building a Machine Learning Pipeline</span> by <span xmlns:cc=\"http://creativecommons.org/ns#\" property=\"cc:attributionName\">Marc de Kamps and University of Leeds</span> is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\">Creative Commons Attribution 4.0 International License</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Machine Learning Pipeline (Part 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "Here, we finally analyse the data. We will experiment with different regressors, preform cross validation and parameter search and will use the *scikit-learn* interface to do so.\n",
    "\n",
    "In particular, we will \n",
    "- Apply linear regression to make predictions\n",
    "- Apply decision trees to make predictions\n",
    "- Apply random forests to make predictions\n",
    "- Compare these different regression methods ob the mean squared error criterion\n",
    "- Perform cross validation and grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tarfile\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "local_path = 'datasets/housing'\n",
    "\n",
    "\n",
    "def restore():\n",
    "    housing_tgz=tarfile.open(os.path.join(local_path,'./housing.tgz'))\n",
    "    housing_tgz.extractall(path=local_path)\n",
    "    housing_tgz.close()\n",
    "\n",
    "    csv_path=os.path.join(local_path,'./housing.csv')\n",
    "    housing = pd.read_csv(csv_path)\n",
    "\n",
    "    # create test training set with stratified sampling (see previous notebook)\n",
    "    housing[\"income_category\"]=np.ceil(housing[\"median_income\"]/1.5)\n",
    "    housing[\"income_category\"].where(housing[\"income_category\"] < 5, 5.0, inplace = True)\n",
    "\n",
    "    split = StratifiedShuffleSplit(n_splits=1, test_size=0.2,random_state=42)\n",
    "\n",
    "    for train_index, test_index in split.split(housing,housing[\"income_category\"]):\n",
    "        strat_train_set = housing.loc[train_index]\n",
    "        strat_test_set = housing.loc[test_index]\n",
    "    \n",
    "    for set_ in (strat_train_set, strat_test_set):\n",
    "        set_.drop((\"income_category\"),axis=1,inplace=True)\n",
    "        \n",
    "   \n",
    "    return strat_train_set, strat_test_set\n",
    "\n",
    "strat_train_set, strat_test_set = restore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing=strat_train_set.drop(\"median_house_value\",axis=1)\n",
    "housing_labels=strat_train_set[\"median_house_value\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_num=housing.drop(\"ocean_proximity\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_attribs=list(housing_num)\n",
    "cat_attribs=[\"ocean_proximity\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer=SimpleImputer(strategy=\"median\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class CombinedAttributesAdder(BaseEstimator):\n",
    "\n",
    "    def __init__(self, do_add_bedrooms_per_room = False):\n",
    "        \n",
    "        # simply a binary variable per room\n",
    "        self.do_add_bedrooms_per_room = do_add_bedrooms_per_room\n",
    "        \n",
    "        # These are the column indices of the respective columns. OK for illustration purposes.\n",
    "        # For more robust code you would want to extract these values from the DataFrame by name.\n",
    "        self.rooms_ix      = 3\n",
    "        self.bedrooms_ix   = 4\n",
    "        self.population_ix = 5\n",
    "        self.household_ix  = 6\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        # We don't transform the target values here\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        rooms_per_household = X[:,self.rooms_ix]/X[:,self.household_ix]\n",
    "        population_per_household = X[:, self.population_ix]/ X[:,self.rooms_ix]\n",
    "        if self.do_add_bedrooms_per_room:\n",
    "            bedrooms_per_room = X[:,self.bedrooms_ix]/X[:,self.rooms_ix]\n",
    "            return np.c_[X,rooms_per_household, population_per_household,bedrooms_per_room]\n",
    "        else:\n",
    "            return np.c_[X, rooms_per_household, population_per_household]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFrameSelector(BaseEstimator):\n",
    "    \n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names= attribute_names\n",
    "        \n",
    "    def fit(self,X, y = None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X[self.attribute_names].values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipeline= Pipeline([\n",
    "    ('selector', DataFrameSelector(num_attribs)),\n",
    "    ('imputer',SimpleImputer(strategy=\"median\")),\n",
    "    ('attribs_adder',CombinedAttributesAdder()),\n",
    "    ('std_scaler',StandardScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_pipeline = Pipeline([\n",
    "    ('selector',DataFrameSelector(cat_attribs)),\n",
    "    ('one hot',OneHotEncoder())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "full_pipeline = FeatureUnion(transformer_list=[\n",
    "    (\"num_pipeline\",num_pipeline),\n",
    "    (\"cat_pipeline\",cat_pipeline)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_prepared = full_pipeline.fit_transform(housing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_prepared.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression\n",
    "Up until this point, the pipeline is a succcint version of all we did in Part 2, with one small exception that we encourage you to find. *housing_prepared* is fully imputed and processed version of the test set. *housing_labels* the associated labesl.  The code for linear regression is shockingly simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg=LinearRegression()\n",
    "lin_reg.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is always a good idea to see what this looks like on the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_data=housing.iloc[:5]\n",
    "some_labels=housing_labels.iloc[:5]\n",
    "some_data_prepared=full_pipeline.transform(some_data)\n",
    "print(\"Predictions:\", lin_reg.predict(some_data_prepared))\n",
    "print(\"Labels:\", list(some_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "housing_predictions = lin_reg.predict(housing_prepared)\n",
    "lin_mse=mean_squared_error(housing_labels, housing_predictions)\n",
    "lin_rmse=np.sqrt(lin_mse)\n",
    "print(lin_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does something sensible, but the root mean squared error is sizeable. Although some of this is caused by the deviation of prediction and outcome for the more expensive homes, inspection of individual predictions shows substantial deviations for some data points. And this is on the **training set**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a decsion tree prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "tree_reg = DecisionTreeRegressor()\n",
    "\n",
    "tree_reg.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the similarity of interface. Prediction also looks similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_predictions=tree_reg.predict(housing_prepared)\n",
    "tree_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "tree_rsme=np.sqrt(tree_mse)\n",
    "tree_rsme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No error at all! Actually this is somewhat suspect. There is no doubt the tree has learnt the training set perfectly, but this may indicate overfitting.\n",
    "\n",
    "### Cross validation\n",
    "In cross validation the dataset is partitioned. In n-fold cross validation, the following experimnt is repeated n times: set a fraction of n-1/n patterns apart from training and use the rest for evaluation. This gives n different scores. THe implementation is straightforward with only one caveat: *scikit-learn* expects a utility function (lower is better than higher) rather than a cost function, and the scoring function is the negative of the MSE. The code below compensates for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores =cross_val_score(tree_reg, housing_prepared,housing_labels,scoring=\"neg_mean_squared_error\",cv=10)\n",
    "\n",
    "tree_rmse_scores = np.sqrt(-scores)\n",
    "\n",
    "def display_scores(scores):\n",
    "    print('Scores:',scores)\n",
    "    print('Mean:',scores.mean())\n",
    "    print('Standard deviation:',scores.std())\n",
    "    \n",
    "display_scores(tree_rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If anything, the performance is now slightly worse than for linear regression. This is a clear example of overfitting by the original tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Carry out cross validation for the linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will try a random forest regressors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "forest_reg=RandomForestRegressor()\n",
    "forest_reg.fit(housing_prepared,housing_labels)\n",
    "\n",
    "housing_predictions=forest_reg.predict(housing_prepared)\n",
    "forest_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "forest_rsme=np.sqrt(forest_mse)\n",
    "forest_rsme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks encouraging: a better error than linear regression, but no clear overfitting. Again, let's cross validate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores=cross_val_score(forest_reg, housing_prepared,housing_labels,scoring=\"neg_mean_squared_error\",cv=10)\n",
    "forest_rmse_scores=np.sqrt(-scores)\n",
    "\n",
    "display_scores(forest_rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search\n",
    "\n",
    "This is clearly better than the linear regressor. We have used the random forest 'out of the box', not bothering to tweak its parameters. The lectures should have given you some ideas on how different parameter settings can be used to alter decision trees and random forests. In a *grid search* you systematically try out combinations of relevant parameters. As you can check from the *scikit-learn* documentation, you can vary (at least) the following parameters:\n",
    "\n",
    "- n_estimators\n",
    "- max_features\n",
    "- bootstrap\n",
    "\n",
    "Suppose you want to systematically explore a number of parameter settings, then *GridSearchCV* can help you do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid  = [{'n_estimators': [3, 10, 30],'max_features':[2, 4, 6, 8]}, \n",
    "               {'bootstrap': [False],'n_estimators':[3,10],'max_features':[2,3,4]}]\n",
    "\n",
    "forest_reg=RandomForestRegressor()\n",
    "\n",
    "grid_search = GridSearchCV(forest_reg, param_grid, cv=5,scoring='neg_mean_squared_error')\n",
    "grid_search.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this has finished, you need to get the best scoring model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation scores are available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvres = grid_search.cv_results_\n",
    "for mean_score, params in zip(cvres[\"mean_test_score\"],cvres[\"params\"]):\n",
    "    print(np.sqrt(-mean_score),params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should explore the possibility to do a **RandomizedSearchCV**.\n",
    "\n",
    "**Exercise** Why is this sometimes a better option? When would you use it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance\n",
    "\n",
    "It can be useful to explore feature importance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = grid_search.best_estimator_.feature_importances_\n",
    "extra_attribs= [\"rooms_per_hhold\", \"pop_per_hhold\",\"bedrooms_per_room\"]\n",
    "\n",
    "# Here we use the OneHotEncoder class to retrieve the original categories\n",
    "encoder=OneHotEncoder()\n",
    "encoder.fit(housing[cat_attribs])\n",
    "cat_one_hot_attribs = [ el for el in encoder.categories_[0]]\n",
    "print(cat_one_hot_attribs)\n",
    "attributes=num_attribs + extra_attribs + cat_one_hot_attribs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(zip(feature_importances,attributes),reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, the feature that a house must be close to the ocean seems to be the only important categorical feature. You can consider leaving out the the others.\n",
    "\n",
    "\n",
    "## Evaluation on the test set\n",
    "\n",
    "This is a step you always should do. If the results are substantially worse than the cross validated ones, you still must distrust your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model=grid_search.best_estimator_\n",
    "\n",
    "X_test = strat_test_set.drop(\"median_house_value\",axis=1)\n",
    "y_test = strat_test_set[\"median_house_value\"].copy()\n",
    "\n",
    "X_test_prepared = full_pipeline.transform(X_test)\n",
    "\n",
    "final_predictions=final_model.predict(X_test_prepared)\n",
    "\n",
    "final_mse = mean_squared_error(y_test,final_predictions)\n",
    "final_rsme = np.sqrt(final_mse)\n",
    "print(final_rsme)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks reasonably close to the cross validated results on the training set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
