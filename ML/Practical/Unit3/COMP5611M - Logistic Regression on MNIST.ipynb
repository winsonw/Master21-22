{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\"><img alt=\"Creative Commons Licence\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by/4.0/88x31.png\" /></a><br /><span xmlns:dct=\"http://purl.org/dc/terms/\" property=\"dct:title\">Logistic Regression on MNIST</span> by <span xmlns:cc=\"http://creativecommons.org/ns#\" property=\"cc:attributionName\">Marc de Kamps and University of Leeds</span> is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\">Creative Commons Attribution 4.0 International License</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression on MNIST\n",
    "\n",
    "### Introduction\n",
    "\n",
    "This notebook is for exploration. It contains code snippets that you can reuse elsewhere. In particular it\n",
    "- Demonstrates how to obtain the MNIST dataset\n",
    "- Uses a PyTorch frontend for the MNIST dataset that allows easy access to each image and its classification\n",
    "- It demonstrates steepest gradient descent for the multiclass cross entropy loss function (something we will re-examine using PyTorch as a framework.\n",
    "\n",
    "This notebook is used in preparation for Activity 2.4.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST data\n",
    "\n",
    "This is a first exploration of the MNIST dataset, which consists of a large collection of handwritten numerals, i.e images. The images are grey-scale $28 \\times 28$ pixels, and each image is associated with a label, which\n",
    "can take on any of the ten values, '0', '1', ..., '9', representing a human judgement about which numeral the image\n",
    "is supposed to represent.\n",
    "\n",
    "### Warning\n",
    "The first two lines in the following code are a hack after the disappearance of the original dataset website. It is a temporary fix. If you find the code block below doesn't work, contact the module leader. It will not be a major problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget www.di.ens.fr/~lelarge/MNIST.tar.gz\n",
    "!tar -zxvf MNIST.tar.gz\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "transform=transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "        ])\n",
    "dataset1 = datasets.MNIST('.', train=True, download=True,\n",
    "                       transform=transform)\n",
    "dataset2 = datasets.MNIST('.', train=False,\n",
    "                       transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code does nothing but download the dataset and then transform it into a Pytorch native format, which is a Tensor. For practical purposes you can treat Tensor's as Numpy arrays for now. With that in mind, it is easy to inspect the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset1[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Careful inspection of the output of the print statement shows this is a (1,2) tensor, that is a single row of two elements. Let us first consider the second element:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset1[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a single numeral. Possibly the label of the image, which then would be the first element. Let's explore this hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# in general you have to be careful in convering tensors to python arrays\n",
    "# a tensor in general will be created on the gpu, and will have some stuff that doesn't relate to the data, like\n",
    "# gradient information. First convert it to a cpu object, detach the data and then convert it to numpy arrays.\n",
    "np_arr = dataset1[0][0].to('cpu').detach().numpy()\n",
    "print(np_arr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is tempting to try and run imshow directly on np_arr. It won't work and the shape information should give you a clue as to why not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np_arr[0].shape)\n",
    "nr_pix =np_arr[0].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That should be better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np_arr[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's better! Do the image and label match? Let's try a random other pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset1[122][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(dataset1[122][0][0])\n",
    "plt.savefig('two.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "design=np.array([t[0].to('cpu').detach().numpy().flatten() for t in dataset1])\n",
    "labels=[t[1] for t in dataset1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(design.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are the rows indeed images?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img22 = np.array(design[22]) # copy it\n",
    "img22.shape = nr_pix, nr_pix\n",
    "plt.imshow(img22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks that way, so *design* is a design matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the labels into target vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_classes = 10\n",
    "t=np.zeros(design.shape[0]*nr_classes)\n",
    "t.shape=design.shape[0],nr_classes\n",
    "\n",
    "for i,ind in enumerate(labels):\n",
    "    t[i][ind] = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the actual regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a constant one to each pattern\n",
    "ones = np.ones(design.shape[0])\n",
    "ones.shape = design.shape[0],1\n",
    "designbias=np.concatenate((design,ones),axis=1)\n",
    "print(designbias.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the weights. A given image connects to 785 of a single node. There will be 10 nodes, one\n",
    "\n",
    "weights = np.random.normal(0.,1.,size=(nr_pix*nr_pix+1)*nr_classes)\n",
    "weights.shape = nr_pix*nr_pix + 1, nr_classes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main text mentions a numerical issue that may crop up when implementing a naive implementation of softmax. Have a look at the stackoverflow discussion.\n",
    "\n",
    "The gradient is given in the main text by the following equation:\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial w_j} = \\sum^N_{i=1}(y_{ij} - t_{ij}) \\boldsymbol{\\Phi}_i\n",
    "$$\n",
    "As you can see, with the help of *np.outer*, this line can be expressed very tersely in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A suitable softmax implementation was taken from this discussion:\n",
    "# https://stackoverflow.com/questions/42599498/numercially-stable-softmax\n",
    "# (sic)\n",
    "\n",
    "\n",
    "\n",
    "def stableSoftmax(x):\n",
    "    z = x - np.max(x, axis=-1, keepdims=True)\n",
    "    numerator = np.exp(z)\n",
    "    denominator = np.sum(numerator, axis=-1, keepdims=True)\n",
    "    softmax = numerator / denominator\n",
    "    return softmax\n",
    "\n",
    "def update(designbias,weights):\n",
    "    return stableSoftmax(designbias.dot(weights))\n",
    "\n",
    "# The gradient is calculated over a single input pattern. This is a 10 \\time 785 matrix. To do this\n",
    "# over the entire dataset would require 60000 of them, which may lead to memory problems\n",
    "\n",
    "def gradient(target, weights, designbias,i):\n",
    "    ms=np.outer(designbias[i],update(designbias,weights)[i] - target[i])\n",
    "    return ms\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating our Regressor\n",
    "\n",
    "Before we do any training, let's evaluate the regressor on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "designtest=np.array([t[0].to('cpu').detach().numpy().flatten() for t in dataset2])\n",
    "labelstest=np.array([t[1] for t in dataset2])\n",
    "print(labelstest.shape)\n",
    "ones = np.ones(designtest.shape[0])\n",
    "ones.shape = designtest.shape[0],1\n",
    "designbiastest=np.concatenate((designtest,ones),axis=1)\n",
    "\n",
    "def correctlyClassified(labels,designbias,weights):\n",
    "    outcomes=update(designbias,weights)\n",
    "    outcomelabels = np.array([ np.argmax(x) for x in outcomes ])\n",
    "    difftest = outcomelabels - labels\n",
    "    return np.count_nonzero(difftest==0),outcomes.shape[0]\n",
    "\n",
    "correctlyClassified(labelstest,designbiastest,weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a very crude measure: we count how many patterns were actually classified in agreement with their label. As you can see the results are approximately at chance level, as they should be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrdatapoints=designbias.shape[0]\n",
    "r = 0.02\n",
    "for i in range(5000):\n",
    "    if i%1000 == 0: print(i)\n",
    "        \n",
    "    weights -= r*gradient(t, weights, designbias,np.random.randint(0,nrdatapoints-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correctlyClassified(labelstest,designbiastest,weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training things have considerably improved.  A proper analysis should be done with separation of test and training set and cross validation (see Machine Learning pipeline notebooks)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
