{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\"><img alt=\"Creative Commons Licence\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by/4.0/88x31.png\" /></a><br /><span xmlns:dct=\"http://purl.org/dc/terms/\" property=\"dct:title\">COMP5611M - Building a Machine Learning Pipeline (part 2)</span> by <span xmlns:cc=\"http://creativecommons.org/ns#\" property=\"cc:attributionName\">Marc de Kamps and University of Leeds</span> is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\">Creative Commons Attribution 4.0 International License</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a machine learning (part 2)\n",
    "\n",
    "### Objectives\n",
    "\n",
    "We will clean the data, in particular, we will\n",
    "- Impute missing values\n",
    "- Replace alpha-numeric information by one-hot-encoded vectors to arrive at a purely numerical implementation\n",
    "- Add features to the dataset that are likely to boost prediction power\n",
    "- We will use Transformer and Pipeline objects to provide self-documenting implementations of these steps\n",
    "\n",
    "### Caveat\n",
    "\n",
    "There is one small error in this notebook that we will leave in place, but have removed in (Part 3). Once you have run (Part 3), see if you can discover it.\n",
    "**Warning** Use Part 3 as a basis for further coding, not this notebook (part 2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start where we left off in the previous notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-122.23 37.88 41.0 ... 8.3252 452600.0 'NEAR BAY']\n",
      " [-122.22 37.86 21.0 ... 8.3014 358500.0 'NEAR BAY']\n",
      " [-122.24 37.85 52.0 ... 7.2574 352100.0 'NEAR BAY']\n",
      " ...\n",
      " [-121.22 39.43 17.0 ... 1.7 92300.0 'INLAND']\n",
      " [-121.32 39.43 18.0 ... 1.8672 84700.0 'INLAND']\n",
      " [-121.24 39.37 16.0 ... 2.3886 89400.0 'INLAND']]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tarfile\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "local_path = 'datasets/housing'\n",
    "\n",
    "\n",
    "def restore():\n",
    "    housing_tgz=tarfile.open(os.path.join(local_path,'./housing.tgz'))\n",
    "    housing_tgz.extractall(path=local_path)\n",
    "    housing_tgz.close()\n",
    "\n",
    "    csv_path=os.path.join(local_path,'./housing.csv')\n",
    "    housing = pd.read_csv(csv_path)\n",
    "\n",
    "    # create test training set with stratified sampling (see previous notebook)\n",
    "    housing[\"income_category\"]=np.ceil(housing[\"median_income\"]/1.5)\n",
    "    housing[\"income_category\"].where(housing[\"income_category\"] < 5, 5.0, inplace = True)\n",
    "\n",
    "    split = StratifiedShuffleSplit(n_splits=1, test_size=0.2,random_state=42)\n",
    "\n",
    "    for train_index, test_index in split.split(housing,housing[\"income_category\"]):\n",
    "        strat_train_set = housing.loc[train_index]\n",
    "        strat_test_set = housing.loc[test_index]\n",
    "    \n",
    "    for set_ in (strat_train_set, strat_test_set):\n",
    "        set_.drop((\"income_category\"),axis=1,inplace=True)\n",
    "        \n",
    "\n",
    "    housing.drop((\"income_category\"),axis=1,inplace=True)\n",
    "\n",
    "   \n",
    "    return housing, strat_train_set, strat_test_set\n",
    "\n",
    "housing, strat_train_set, start_test_set = restore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing data\n",
    "\n",
    "In many real world data sets there is missing data. Doctors forget to register the temperature at some days, sensors are sometimes faulty and produce no sensible output, etc. Pandas is a considerable step up from comma separated files in that they have an explicit representation for a missing value: *NaN*.\n",
    "\n",
    "In general, machine learning algorithms can't work with NaN, however and this means either removing data where NaN occurs or replacing them by a sensible default value, a process called *imputation*.\n",
    "\n",
    "Here you have three choices:\n",
    "-Drop districts where NaN occurs\n",
    "-Drop the entire attribute (i.e. remove the entire column where NaN occurs)\n",
    "-Impute\n",
    "\n",
    "Many imputation strategies are simple: they replace the missing value by the median (or sometimes mean) of the attribute. This is potentially dangerous: if the pattern of missingness is not random, but has systematic causes it may be wrong to use this strategy. More sophisticated strategies, that we will not consider here, consist of using for example a random forest or decision tree to try and predict the missing values.\n",
    "\n",
    "Pandas offers quick ways for implementing simple imputation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-122.23 37.88 41.0 ... 8.3252 452600.0 'NEAR BAY']\n",
      " [-122.22 37.86 21.0 ... 8.3014 358500.0 'NEAR BAY']\n",
      " [-122.24 37.85 52.0 ... 7.2574 352100.0 'NEAR BAY']\n",
      " ...\n",
      " [-121.22 39.43 17.0 ... 1.7 92300.0 'INLAND']\n",
      " [-121.32 39.43 18.0 ... 1.8672 84700.0 'INLAND']\n",
      " [-121.24 39.37 16.0 ... 2.3886 89400.0 'INLAND']]\n"
     ]
    }
   ],
   "source": [
    "#housing.dropna(subset=[\"total_bedrooms\"]) # drop those districts for which the total_bedrooms value is missing\n",
    "#housing.drop(\"total_bedrooms\",axis=1) # drop the entire column\n",
    "\n",
    "# impute missing values by the median\n",
    "median=housing[\"total_bedrooms\"].median()\n",
    "housing[\"total_bedrooms\"].fillna(median)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*scikit-learn* itself also offers support for simple imputation strategies. And since this nicely dovetails with the use of pipelines, which is an important topic in this notebook, let's check it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer=SimpleImputer(strategy=\"median\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be as simple as calling the fit method on the data frame. So:\n",
    "\n",
    "            imputer.fit(housing)\n",
    "\n",
    "You will find this will cause an exception to be thrown:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot use median strategy with non-numeric data:\ncould not convert string to float: 'NEAR BAY'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-116-48e4448c63e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimputer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhousing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/impute/_base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0mself\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mSimpleImputer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \"\"\"\n\u001b[0;32m--> 277\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_fit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_indicator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/impute/_base.py\u001b[0m in \u001b[0;36m_validate_input\u001b[0;34m(self, X, in_fit)\u001b[0m\n\u001b[1;32m    247\u001b[0m                 new_ve = ValueError(\"Cannot use {} strategy with non-numeric \"\n\u001b[1;32m    248\u001b[0m                                     \"data:\\n{}\".format(self.strategy, ve))\n\u001b[0;32m--> 249\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mnew_ve\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot use median strategy with non-numeric data:\ncould not convert string to float: 'NEAR BAY'"
     ]
    }
   ],
   "source": [
    "imputer.fit(housing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1** Examine the data frame to see what causes this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "      <th>ocean_proximity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-122.23</td>\n",
       "      <td>37.88</td>\n",
       "      <td>41.0</td>\n",
       "      <td>880.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>8.3252</td>\n",
       "      <td>452600.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-122.22</td>\n",
       "      <td>37.86</td>\n",
       "      <td>21.0</td>\n",
       "      <td>7099.0</td>\n",
       "      <td>1106.0</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>8.3014</td>\n",
       "      <td>358500.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-122.24</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1467.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>7.2574</td>\n",
       "      <td>352100.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1274.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>558.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>5.6431</td>\n",
       "      <td>341300.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1627.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>3.8462</td>\n",
       "      <td>342200.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "0    -122.23     37.88                41.0        880.0           129.0   \n",
       "1    -122.22     37.86                21.0       7099.0          1106.0   \n",
       "2    -122.24     37.85                52.0       1467.0           190.0   \n",
       "3    -122.25     37.85                52.0       1274.0           235.0   \n",
       "4    -122.25     37.85                52.0       1627.0           280.0   \n",
       "\n",
       "   population  households  median_income  median_house_value ocean_proximity  \n",
       "0       322.0       126.0         8.3252            452600.0        NEAR BAY  \n",
       "1      2401.0      1138.0         8.3014            358500.0        NEAR BAY  \n",
       "2       496.0       177.0         7.2574            352100.0        NEAR BAY  \n",
       "3       558.0       219.0         5.6431            341300.0        NEAR BAY  \n",
       "4       565.0       259.0         3.8462            342200.0        NEAR BAY  "
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#! Sample answer \n",
    "housing.head()\n",
    "\n",
    "# It is clear that ocean_proximity is a text attribute and that imputer tries to calculate a numerical value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2** The offending data itself does not need to be imputed. So it could be dropped, at least temporary, from the dataframe. Research how you can create a dataframe that doesn't contain the offending data, then fit the imputer to it. Once you have done this, print the\n",
    "\n",
    "                imputer.statistics_\n",
    "                \n",
    "member variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.1849e+02  3.4260e+01  2.9000e+01  2.1270e+03  4.3500e+02  1.1660e+03\n",
      "  4.0900e+02  3.5348e+00  1.7970e+05]\n"
     ]
    }
   ],
   "source": [
    "#! Sample answer\n",
    "housing_num =housing.drop(\"ocean_proximity\",axis=1)\n",
    "imputer.fit(housing_num)\n",
    "print(imputer.statistics_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous question, you should have created a new dataframe that doesn't contain the 'ocean_proximity' attribute. Let's assume this reduced dataframe is called *housing_num* which now is guaranteed to contain numerical data only.\n",
    "\n",
    "The fit method has not changed *housing_num* at all. It only has fed the data of the reduced dataframe to the imputer and allowed it to calculate median values for each of the columns. This is called *training* the imputer.\n",
    "Now the imputer must be used to actually clean up the data of the *housing_num* frame. This is done by calling the imputer's *transform* method on *housing_num*. **Before proceeding, please ensure you have created the reduced dataframe and called it housing_num or the notebook will crash.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=imputer.transform(housing_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3** Examine the form of the data represented by X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.2223e+02  3.7880e+01  4.1000e+01 ...  1.2600e+02  8.3252e+00\n",
      "   4.5260e+05]\n",
      " [-1.2222e+02  3.7860e+01  2.1000e+01 ...  1.1380e+03  8.3014e+00\n",
      "   3.5850e+05]\n",
      " [-1.2224e+02  3.7850e+01  5.2000e+01 ...  1.7700e+02  7.2574e+00\n",
      "   3.5210e+05]\n",
      " ...\n",
      " [-1.2122e+02  3.9430e+01  1.7000e+01 ...  4.3300e+02  1.7000e+00\n",
      "   9.2300e+04]\n",
      " [-1.2132e+02  3.9430e+01  1.8000e+01 ...  3.4900e+02  1.8672e+00\n",
      "   8.4700e+04]\n",
      " [-1.2124e+02  3.9370e+01  1.6000e+01 ...  5.3000e+02  2.3886e+00\n",
      "   8.9400e+04]]\n",
      "(20640, 9)\n"
     ]
    }
   ],
   "source": [
    "#! Sample answer\n",
    "print(X)\n",
    "print(X.shape)\n",
    "\n",
    "# So just one big numpy array, but without Nan!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4** Convert X back into a dataframe with the original column names (minus *ocean_proximity*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-122.23</td>\n",
       "      <td>37.88</td>\n",
       "      <td>41.0</td>\n",
       "      <td>880.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>8.3252</td>\n",
       "      <td>452600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-122.22</td>\n",
       "      <td>37.86</td>\n",
       "      <td>21.0</td>\n",
       "      <td>7099.0</td>\n",
       "      <td>1106.0</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>8.3014</td>\n",
       "      <td>358500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-122.24</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1467.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>7.2574</td>\n",
       "      <td>352100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1274.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>558.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>5.6431</td>\n",
       "      <td>341300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1627.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>3.8462</td>\n",
       "      <td>342200.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "0    -122.23     37.88                41.0        880.0           129.0   \n",
       "1    -122.22     37.86                21.0       7099.0          1106.0   \n",
       "2    -122.24     37.85                52.0       1467.0           190.0   \n",
       "3    -122.25     37.85                52.0       1274.0           235.0   \n",
       "4    -122.25     37.85                52.0       1627.0           280.0   \n",
       "\n",
       "   population  households  median_income  median_house_value  \n",
       "0       322.0       126.0         8.3252            452600.0  \n",
       "1      2401.0      1138.0         8.3014            358500.0  \n",
       "2       496.0       177.0         7.2574            352100.0  \n",
       "3       558.0       219.0         5.6431            341300.0  \n",
       "4       565.0       259.0         3.8462            342200.0  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#! Sample answer\n",
    "housing_tr = pd.DataFrame(X,columns=housing_num.columns)\n",
    "housing_tr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text and Categorical Variables\n",
    "\n",
    "In general machine learning algorithms need numerical values to work and *scikit-learn* expects data in two dimensional numpy arrays (like X) in the previous example. It is therefore necessary to convert text which often indicates a categorical variable (like, '<1H OCEAN', 'INLAND', 'NEAR OCEAN' etc.) into numerical values. (An exception to this would be a project that tries to apply NLP to text fragments, but that is not the case here; the column\n",
    "*ocean_proximity* clearly contains categories). It would be possible to convert  them into numerical values as in '<1H OCEAN' = 0, 'INLAND' = 1, etc.. There is a potential problem with this as there is an impcit concept of nearness in this coding: 0 and 1 are closer together than 1 and 4, for example. If used in a clustering algorithm this nearness may be used without this being intended by the investigator. A one-hot encoding does not have this problem.\n",
    "\n",
    "A one-hot coding would code 5 categories as follows:\n",
    "(1,0,0,0,0)\n",
    "(0,1,0,0,0)\n",
    "   ...\n",
    "(0,0,0,0,1)\n",
    "\n",
    "There is no unintended concept of proximity in this coding and it is to be preferred for coding categorical variables numerically.\n",
    "\n",
    "Again *scikit-learn* offers a custom-made class for transforming textual categorical variables into one-hot encoded ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(['<1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN'],\n",
      "      dtype=object)]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc=OneHotEncoder()\n",
    "enc.fit(housing[\"ocean_proximity\"].values.reshape(-1,1))\n",
    "print(enc.categories_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*reshape*  is necessary because a transformer (such as Imputer or OneHotEncoder) expects a 2D numpy array. **housing[\"ocean_proximity\"]** is a *pandas.Series*, hence the use of values. The -1 is a neat little syntactic trick to prevent having to know or calculate the length of the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       ...,\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.transform(housing[\"ocean_proximity\"].values.reshape(-1,1)).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers\n",
    "\n",
    "*Imputer* and *OneHotEncoder* are so-called **transformers**. Whenever you start to realise that you need to do a lot of repetitive processing, you should either use a tranformer or design one of your own, in case *scikit-learn*  doesn't have one that is necessary for your processing.\n",
    "\n",
    "You can design a class of your own that encapsulates your preprocessing code. All you need to do is provide functions: \n",
    "\n",
    "                fit()\n",
    "                transform()\n",
    "                fit_transform()\n",
    "                \n",
    "*fit_transform* simply first calls *fit* and then *transform*\n",
    "\n",
    "As an example, lets add some preprocessing that adds the number of rooms per house hold and optionally also\n",
    "the number of rooms per household. Earlier we have seen that the number rooms per house hold correlates quite strongly with the median house value and it may be a variable that we want to represent explicitly in the dataset, meaning that we have to transform it. Optionally, we may want the number of bedrooms per room to the dataset as well. We will make the transformer configurable so that later in your analysis pipeline you can investigate both options, simply by setting a binary flag, which then becomes a hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-122.23 37.88 41.0 ... 6.984126984126984 0.3659090909090909\n",
      "  0.14659090909090908]\n",
      " [-122.22 37.86 21.0 ... 6.238137082601054 0.3382166502324271\n",
      "  0.15579659106916466]\n",
      " [-122.24 37.85 52.0 ... 8.288135593220339 0.338104976141786\n",
      "  0.12951601908657123]\n",
      " ...\n",
      " [-121.22 39.43 17.0 ... 5.20554272517321 0.44676131322094054\n",
      "  0.21517302573203195]\n",
      " [-121.32 39.43 18.0 ... 5.329512893982808 0.39838709677419354\n",
      "  0.21989247311827956]\n",
      " [-121.24 39.37 16.0 ... 5.254716981132075 0.4980251346499102\n",
      "  0.22118491921005387]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class CombinedAttributesAdder(BaseEstimator):\n",
    "\n",
    "    def __init__(self, do_add_bedrooms_per_room = False):\n",
    "        \n",
    "        # simply a binary variable per room\n",
    "        self.do_add_bedrooms_per_room = do_add_bedrooms_per_room\n",
    "        \n",
    "        # These are the column indices of the respective columns. OK for illustration purposes.\n",
    "        # For more robust code you would want to extract these values from the DataFrame by name.\n",
    "        self.rooms_ix      = 3\n",
    "        self.bedrooms_ix   = 4\n",
    "        self.population_ix = 5\n",
    "        self.household_ix  = 6\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        # We don't transform the target values here\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        rooms_per_household = X[:,self.rooms_ix]/X[:,self.household_ix]\n",
    "        population_per_household = X[:, self.population_ix]/ X[:,self.rooms_ix]\n",
    "        if self.do_add_bedrooms_per_room:\n",
    "            bedrooms_per_room = X[:,self.bedrooms_ix]/X[:,self.rooms_ix]\n",
    "            return np.c_[X,rooms_per_household, population_per_household,bedrooms_per_room]\n",
    "        else:\n",
    "            return np.c_[X, rooms_per_household, population_per_household]\n",
    "        \n",
    "attr_adder=CombinedAttributesAdder(do_add_bedrooms_per_room=True)\n",
    "housing_extra_attribs=attr_adder.transform(housing.values)\n",
    "print(housing_extra_attribs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The extra attributes have now been added to the data, which are spit out as a 2D numpy array without further structure.  Ultimately, this is what you need if you want to do numerical processing and exactly what *scikit-learn* algorithms expect.\n",
    "\n",
    "The *np.c_* is a numpy shorthand for grouping vectors into matrices. There are many ways of achieving the same effect, some people prefer *hstack* , *vstack* constructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 4]\n",
      " [2 5]\n",
      " [3 6]]\n",
      "[[1 4]\n",
      " [2 5]\n",
      " [3 6]]\n"
     ]
    }
   ],
   "source": [
    "a=np.array([1,2,3]).T\n",
    "b=np.array([4,5,6]).T\n",
    "print(np.c_[a,b])\n",
    "\n",
    "print(np.vstack([a,b]).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "\n",
    "Most machine learning algorithms perform better if the numerical values of the attributes are of comparable magnitude. The total number of rooms varies between 0 and 39320, whereas income is between 0 and 15. Typically, the attributes - but not the target variable, here median house value - are rescaled. An exception are decision tree based methods where the data is taken as is.\n",
    "\n",
    "#### Min-max scaling (normalisation)\n",
    "\n",
    "The data are shifted and rescaled so that their minimum value corresponds to 0 and their maximum value corresponds to 1. *scikit-learn* provides the **MinMaxScaler** to achieve this. It can be recongured to other target ranges than $[0,1]$ where desired.\n",
    "\n",
    "#### Standardisation\n",
    "The mean is substracted of all data points and then divided by the variance so that a zero-mean unit variance distribution results.\n",
    "\n",
    "There are pros and cons to each of these methods. Some machine learning algorithms (neural networks) expect input values to be within a certain range, meaning that you have to use min-max scaling. But a single outlier can compress the other data points to heap up around 0, something that doesn't happen to the same extent in standardisation.\n",
    "\n",
    "*scikit-learn* provides the *StandardScalar* transformer to implement this method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines\n",
    "\n",
    "Pipelines group transformers sequentially and provide a single access point for running the entire preprocessing chain. The transformations we have discussed so far crop up almost in every machine learning analysis. Each transformation step is usually simple, and it may be that initially you have cobbled an ad hoc preprocessing step to get going with analysis. Such code tends to obscure your programmes. If you find yourself bored because you're constantly writing relatively simple proeprocessing steps, you're not using pipelines properly. The individual\n",
    "Transformers can be hidden in python modules that you can import. The Pipelines define a very short definition\n",
    "of the preprocessing steps and are almost self-documenting. An example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer',SimpleImputer(strategy=\"median\")),\n",
    "    ('attribs_adder',CombinedAttributesAdder()),\n",
    "    ('std_scaler',StandardScaler())\n",
    "])\n",
    "\n",
    "housing_num_tr = num_pipeline.fit_transform(housing_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This piece of code is very readable, moreover since imputing and scaling occur so often, similar pipelines can\n",
    "often be constructed for other analyses, and the indvidual components can be re-used with a minimum of new programming effors. \n",
    "\n",
    "The names are practical for documentation, but also allow named access to the elements of the pipeline in case they're needed downstream in the analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleImputer(strategy='median')"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_pipeline['imputer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you call the pipeline *fit* method, it calls the *fit_transform* method of the elements of the pipeline in order. All but the last elements of the pipeline must be *transformers*, the last can be an *estimator* (an *estimator*) has *fit* method, but not necessarily a *transform* or *fit_transform* method. )."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Seperate Pipelines for Numerical and Categorical Values\n",
    "\n",
    "We have seen that we need to handle numerical and categorical values differently. This effectively boils down to the creation of two pipelines that each run on mutually exclusive columns. We would like to run these pipelines in parallel and combine their joint output in one 2D numpy array which contains the conversion of all attributes to sensible numerical values.\n",
    "\n",
    "To this end we will write a custom transformer that selects only certain columns of the dataset, and drops the other ones. We will use that transformer, called **DataFrameSelector** to create two parallel pipelines and then\n",
    "combined the two resulting datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFrameSelector(BaseEstimator):\n",
    "    \n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names= attribute_names\n",
    "        \n",
    "    def fit(self,X, y = None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X[self.attribute_names].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that although there appears to be no pandas code, this works because X is a panda DataFrame, which can take a list of column names and select the appropriate columns!\n",
    "\n",
    "The two pipelines can now be created as follows **from the training set**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_attribs=list(housing) # using pandas to get the names of attributes in housing_num, which we made by dropping ocean_proximity\n",
    "num_attribs.remove(\"ocean_proximity\")\n",
    "cat_attribs=[\"ocean_proximity\"]\n",
    "\n",
    "num_pipeline= Pipeline([\n",
    "    ('selector', DataFrameSelector(num_attribs)),\n",
    "    ('imputer',SimpleImputer(strategy=\"median\")),\n",
    "    ('attribs_adder',CombinedAttributesAdder()),\n",
    "    ('std_scaler',StandardScaler())\n",
    "])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    ('selector',DataFrameSelector(cat_attribs)),\n",
    "    ('one hot',OneHotEncoder())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These pipelines can be run independently ('in parallel'). Their results now need to be combined. *scikit-learn* offers the **FeatureUnion** for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "full_pipeline = FeatureUnion(transformer_list=[\n",
    "    (\"num_pipeline\",num_pipeline),\n",
    "    (\"cat_pipeline\",cat_pipeline)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole pipeline can now be run in one go on the original *housing* DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16512, 15)\n",
      "  (0, 0)\t-1.1560428086829155\n",
      "  (0, 1)\t0.7719496164846016\n",
      "  (0, 2)\t0.7433308916510305\n",
      "  (0, 3)\t-0.49323393384425046\n",
      "  (0, 4)\t-0.4454382074687401\n",
      "  (0, 5)\t-0.6362114070375079\n",
      "  (0, 6)\t-0.4206984222235789\n",
      "  (0, 7)\t-0.6149374443958345\n",
      "  (0, 8)\t-0.31205451913809157\n",
      "  (0, 9)\t-0.05357520795873586\n",
      "  (0, 10)\t1.0\n",
      "  (1, 0)\t-1.1760248286103931\n",
      "  (1, 1)\t0.6596947951050618\n",
      "  (1, 2)\t-1.165317203353399\n",
      "  (1, 3)\t-0.9089665536785813\n",
      "  (1, 4)\t-1.036927797035715\n",
      "  (1, 5)\t-0.998331346653321\n",
      "  (1, 6)\t-1.0222270483369085\n",
      "  (1, 7)\t1.336459363533279\n",
      "  (1, 8)\t0.217683376839971\n",
      "  (1, 9)\t-0.054238379827919576\n",
      "  (1, 10)\t1.0\n",
      "  (2, 0)\t1.186849027813462\n",
      "  (2, 1)\t-1.3421828528300457\n",
      "  (2, 2)\t0.1866418639414053\n",
      "  :\t:\n",
      "  (16509, 8)\t0.3469341967290955\n",
      "  (16509, 9)\t-0.05997192431397317\n",
      "  (16509, 11)\t1.0\n",
      "  (16510, 0)\t0.7822131242821033\n",
      "  (16510, 1)\t-0.8510680092945644\n",
      "  (16510, 2)\t0.1866418639414053\n",
      "  (16510, 3)\t-0.30991876289367937\n",
      "  (16510, 4)\t-0.37484891488667316\n",
      "  (16510, 5)\t-0.05717803824588582\n",
      "  (16510, 6)\t-0.37545069371062934\n",
      "  (16510, 7)\t0.0981213859630686\n",
      "  (16510, 8)\t0.024994881724019452\n",
      "  (16510, 9)\t0.020381881634194854\n",
      "  (16510, 10)\t1.0\n",
      "  (16511, 0)\t-1.435791087667559\n",
      "  (16511, 1)\t0.9964592592436813\n",
      "  (16511, 2)\t1.8567089470702811\n",
      "  (16511, 3)\t0.22085347952697917\n",
      "  (16511, 4)\t0.3602533733817481\n",
      "  (16511, 5)\t-0.13515931237107331\n",
      "  (16511, 6)\t0.3777909044755312\n",
      "  (16511, 7)\t-0.15779864628309584\n",
      "  (16511, 8)\t-0.2285294723443894\n",
      "  (16511, 9)\t-0.0668146061262357\n",
      "  (16511, 13)\t1.0\n"
     ]
    }
   ],
   "source": [
    "housing.head()\n",
    "#housing=strat_train_set.drop(\"median_house_value\",axis=1)\n",
    "#housing_labels=strat_train_set[\"median_house_value\"]\n",
    "housing_prepared = full_pipeline.fit_transform(housing)\n",
    "\n",
    "print(housing_prepared.shape)\n",
    "print(housing_prepared)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
