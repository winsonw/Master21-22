{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## COMP5623M Assessment Coursework 1 - Image Classification [100 marks]\n\nThe maximum number of marks for each part are shown in the section headers. As indicated in the main heading above, the overall assessment carries a maximum of 100 marks.\n\nThis summative assessment is weighted 25% of the final grade for the module.\n\n### Motivation \n\nThrough this coursework, you will:\n\n> 1. Practice building, evaluating, and finetuning a convolutional neural network on an image dataset from development to testing. \n> 2. Gain a deeper understanding of feature maps and filters by visualizing some from a pre-trained network. \n\n\n### Setup and resources \n\nYou must work using this provided template notebook.\n\nHaving a GPU will speed up the training process, especially for Question 1.3. See the provided document on Minerva about setting up a working environment for various ways to access a GPU.\n\nPlease implement the coursework using **Python and PyTorch**, and refer to the notebooks and exercises provided.\n\nThis coursework will use a subset of images from Tiny ImageNet, which is a subset of the ImageNet dataset [https://image-net.org/]. Our subset of Tiny ImageNet contains 30 different categories, we will refer to it as TinyImageNet30. The training set has 450 resized images (64x64 pixels) for each category (13,500 images in total). You can download the training and test set from the Kaggle website:\n\n>[Private class Kaggle competition and data](https://www.kaggle.com/t/9b703e0d71824a658e186d5f69960e27)\n\nTo access the dataset, you will need an account on the Kaggle website. Even if you have an existing Kaggle account, please carefully adhere to these instructions, or we may not be able to locate your entries:\n\n> 1. Use your **university email** to register a new account.\n> 2. Set your **Kaggle account NAME** to your university username, for example, ``sc15jb``.\n\nThe class Kaggle competition also includes a blind test set, which will be used in Question 1 for evaluating your custom model's performance on a test set. The competition website will compute the test set accuracy, as well as position your model on the class leaderboard.\n\n### Submission\n\nPlease submit the following:\n\n> 1. Your completed Jupyter notebook file, without removing anything in the template, in **.ipynb format.**\n> 2. The **.html version** of your notebook; File > Download as > HTML (.html). Check that all cells have been run and all outputs (including all graphs you would like to be marked) displayed in the .html for marking.\n> 3. Your selected image from section 2.4.2 \"Failure analysis\"\n\nFinal note:\n\n> **Please display everything that you would like to be marked. Under each section, put the relevant code containing your solution. You may re-use functions you defined previously, but any new code must be in the relevant section.** Feel free to add as many code cells as you need under each section.\n\nYour student username (for example, ```sc15jb```):\n","metadata":{"id":"LR4bovYL4CJz"}},{"cell_type":"markdown","source":"sc212zw","metadata":{"id":"LKZdFXsuW0bb"}},{"cell_type":"markdown","source":"Your full name:","metadata":{"id":"6lXJZadGW0bc"}},{"cell_type":"markdown","source":"Ziwei Wang","metadata":{"id":"As-rXIWJW0bc"}},{"cell_type":"markdown","source":"## Imports\n\nFeel free to add to this section as needed.\n\nYou may need to download `cv2` using [pip](https://pypi.org/project/opencv-python/) or [conda](https://anaconda.org/conda-forge/opencv).","metadata":{"id":"iSStRP1dW0bd"}},{"cell_type":"code","source":"import cv2\nimport math\n\nimport torch\nimport numpy as np\nimport torch.nn as nn\nfrom torch import optim\n\nimport torchvision.transforms as transforms\nfrom torch.hub import load_state_dict_from_url\n\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import DataLoader\n\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport os\nos.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nimport pandas as pd\nfrom torchvision.io import read_image\nfrom torch.utils.data import Dataset\n! pip install natsort\nfrom natsort import natsorted\n\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns","metadata":{"id":"mxlzGxQEW0bd","execution":{"iopub.status.busy":"2022-02-28T00:46:28.677379Z","iopub.execute_input":"2022-02-28T00:46:28.677755Z","iopub.status.idle":"2022-02-28T00:46:41.754981Z","shell.execute_reply.started":"2022-02-28T00:46:28.677711Z","shell.execute_reply":"2022-02-28T00:46:41.753968Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## QUESTION 1 [55 marks]\n\nOne challenge of building a deep learning model is to choose an architecture that can learn the features in the dataset without being unnecessarily complex. The first part of the coursework involves building a CNN and training it on TinyImageNet30. \n\n### **Overview:**\n*   **1.1.1** PyTorch ```Dataset``` and ```DataLoader``` classes\n*   **1.1.2** PyTorch ```Model``` class for simple CNN model\n*   **1.1.3** Overfitting on a single batch\n*   **1.2.1** Training on complete dataset\n*   **1.2.2** Fine-tuning model\n*   **1.2.3** Generating confusion matrices\n*   **1.3**   Testing on test set on Kaggle\n\n\n## 1.1 Single-batch training [14 marks]\n\nWe will use a method of development called “single-batch training”, or \"overfitting a single batch\", in which we check that our model and the training code is working properly and can overfit a single training batch (i.e., we can drive the training loss to zero). Then we move on to training on the complete training set and adjust for any overfitting and fine-tune the model via regularisation.\n\n### 1.1.1 Dataset class [3 marks]\n\nWrite a PyTorch ```Dataset``` class (an example [here](https://www.askpython.com/python-modules/pytorch-custom-datasets) for reference) which loads the TinyImage30 dataset and ```DataLoaders``` for training and validation sets.\n","metadata":{"id":"kfR--uYXHdIi"}},{"cell_type":"code","source":"# from google.colab import drive\n# drive.mount('/content/gdrive')","metadata":{"id":"GAx7ey00W0bg","outputId":"61543360-722e-42bd-a673-1383f87b5ac1","execution":{"iopub.status.busy":"2022-02-27T17:02:20.146117Z","iopub.execute_input":"2022-02-27T17:02:20.146354Z","iopub.status.idle":"2022-02-27T17:02:20.152791Z","shell.execute_reply.started":"2022-02-27T17:02:20.146327Z","shell.execute_reply":"2022-02-27T17:02:20.152051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ! cp \"/content/gdrive/MyDrive/Colab Notebooks/data.zip\" \"data.zip\"\n# ! unzip data.zip","metadata":{"id":"JLZjwS3PW0bg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dataset = ImageFolder(root=\"data/train_set/train_set/\", transform=transforms.ToTensor())\ndataset = ImageFolder(root=\"../input/something/train_set/train_set\", transform=transforms.ToTensor())\ndataloader = DataLoader(dataset, batch_size=64, shuffle=True)","metadata":{"id":"A_F_A1l3W0bg","execution":{"iopub.status.busy":"2022-02-27T17:10:09.406249Z","iopub.execute_input":"2022-02-27T17:10:09.406890Z","iopub.status.idle":"2022-02-27T17:10:10.802514Z","shell.execute_reply.started":"2022-02-27T17:10:09.406848Z","shell.execute_reply":"2022-02-27T17:10:10.801816Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"### 1.1.2 Define a CNN model [3 marks]\n\nCreate a new model class using a combination of convolutional and fully connected layers, ReLU, and max-pool. ","metadata":{"id":"-h_03GZ3W0bh"}},{"cell_type":"code","source":"# TO COMPLETE\n# define a Model class\n\nclass Simple_CNN(nn.Sequential):\n    def __init__(self):\n        super().__init__()\n        layers = [    \n            nn.Conv2d(in_channels=3,out_channels=32, kernel_size=3),    \n            nn.ReLU(),\n            nn.Conv2d(in_channels=32,out_channels=32, kernel_size=3),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Conv2d(in_channels=32,out_channels=64, kernel_size=3),    \n            nn.ReLU(),\n            nn.Conv2d(in_channels=64,out_channels=64, kernel_size=3),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Flatten(),\n            nn.Linear(64*13*13,512),\n            nn.ReLU(),\n            nn.Linear(512,30),\n            nn.LogSoftmax(dim=1),\n        ]\n        for idx, module in enumerate(layers):\n            self.add_module(str(idx), module)","metadata":{"id":"k4teLMReW0bh","execution":{"iopub.status.busy":"2022-02-27T17:10:15.181125Z","iopub.execute_input":"2022-02-27T17:10:15.181678Z","iopub.status.idle":"2022-02-27T17:10:15.190704Z","shell.execute_reply.started":"2022-02-27T17:10:15.181637Z","shell.execute_reply":"2022-02-27T17:10:15.189985Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"### 1.1.3 Single-batch training [8 marks]\n\nWrite the foundational code which trains your network given **one single batch** of training data and computes the loss on the complete validation set for each epoch. Set ```batch_size = 64```. \n\nDisplay the graph of the training and validation loss over training epochs, showing as long as necessary to show you can drive the training loss to zero.\n\n> Please leave all graphs and code you would like to be marked clearly displayed without needing to run code cells or wait for training.\n","metadata":{"id":"4T4m0R30W0bi"}},{"cell_type":"code","source":"class Foundation_Classification():\n    def __init__(self, dataloader):    \n        self.batch_size = 64\n        self.stat = {\"train_loss\":[], \"validation_loss\":[]}\n        self.results_path = 'single_batch.pt'\n        self.dataloader = dataloader\n        self.epoch_size = len(self.dataloader)\n        \n    def simple_fig_init(self):     \n        self.net = Simple_CNN().to(device)\n        self.loss_fn = nn.CrossEntropyLoss().to(device)\n        self.optimizer = optim.Adam(self.net.parameters(), lr=0.001)\n        \n    def accuracy(self, outputs, labels):\n        _, predicted = torch.max(outputs.data, 1)\n        correct = (predicted == labels).sum().item()\n        return correct/labels.shape[0]\n    \n    def train(self, inputs, labels):\n        self.optimizer.zero_grad()\n        outputs = self.net(inputs.to(device))\n        loss = self.loss_fn(outputs, labels.to(device))\n        loss.backward()\n        self.optimizer.step()\n        return loss.item()\n\n    def predict(self, inputs):\n        with torch.no_grad():\n          output = int(torch.argmax(self.net(inputs.to(device))))\n        return output\n\n    def validate(self, inputs, labels):\n        with torch.no_grad():\n          outputs = self.net(inputs.to(device)).to(device)\n          loss = self.loss_fn(outputs, labels.to(device)).item()\n        return loss\n    \n    def plot(self):\n        plt.plot(self.stat[\"train_loss\"], 'r', label = 'training loss', )\n        plt.plot(self.stat[\"validation_loss\"], 'g', label = 'validation loss' )\n        plt.xlabel('epoch')\n        plt.ylabel('loss')\n        plt.title('Training and test loss')","metadata":{"id":"0Lr69ExUW0bi","execution":{"iopub.status.busy":"2022-02-27T17:34:20.779034Z","iopub.execute_input":"2022-02-27T17:34:20.779644Z","iopub.status.idle":"2022-02-27T17:34:20.792851Z","shell.execute_reply.started":"2022-02-27T17:34:20.779606Z","shell.execute_reply":"2022-02-27T17:34:20.791764Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"class Single_Batch(Foundation_Classification):\n    def __init__(self, dataloader, validation_ratio=0.2):\n        super().__init__(dataloader)\n        self.validation_ratio = validation_ratio\n        self.validation_index = int(self.batch_size * (1 - self.validation_ratio))\n        self.simple_fig_init()\n    \n    def training(self): \n        data = next(iter(self.dataloader))\n        inputs, labels = data\n        train_inputs = inputs[:self.validation_index]\n        validation_inputs = inputs[self.validation_index:]\n        train_labels = labels[:self.validation_index]\n        validation_labels = labels[self.validation_index:]\n        train_loss = 999\n        \n        epoch = 0\n        \n        while round(train_loss,3) != 0.0:\n\n            train_loss = self.train(train_inputs, train_labels)\n            validation_loss = self.validate(validation_inputs, validation_labels)\n\n            self.stat[\"train_loss\"].append(train_loss)\n            self.stat[\"validation_loss\"].append(validation_loss)\n            # print(f\"epoch: {epoch} training loss: {train_loss: .3f} validation loss: {validation_loss: .3f}\")\n            epoch += 1\n            \n        torch.save({\"single_state_dict\": self.net.state_dict()}, self.results_path)","metadata":{"id":"8W87-pEbW0bi","execution":{"iopub.status.busy":"2022-02-27T17:34:24.727760Z","iopub.execute_input":"2022-02-27T17:34:24.728260Z","iopub.status.idle":"2022-02-27T17:34:24.739869Z","shell.execute_reply.started":"2022-02-27T17:34:24.728216Z","shell.execute_reply":"2022-02-27T17:34:24.738992Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"s = Single_Batch(dataloader)\ns.training()\ns.plot()","metadata":{"scrolled":true,"id":"tcRSKyeJW0bj","outputId":"8e75a4f6-fd42-4fe7-c8d6-a64849f67f66","execution":{"iopub.status.busy":"2022-02-27T17:10:26.382574Z","iopub.execute_input":"2022-02-27T17:10:26.383229Z","iopub.status.idle":"2022-02-27T17:10:36.248964Z","shell.execute_reply.started":"2022-02-27T17:10:26.383187Z","shell.execute_reply":"2022-02-27T17:10:36.247998Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"\n\n## 1.2 Training on complete dataset [23 marks]\n\n### 1.2.1 Train CNN and show loss graph [6 marks]\n\nTrain your model on the complete training dataset, and use the validation set to determine when to stop training.\n\nDisplay the graph of training and validation loss over epochs to show how you determined the optimal number of training epochs.\n\n> As in previous sections, please leave the graph clearly displayed.\n","metadata":{"id":"4yA2DsAhW0bj"}},{"cell_type":"code","source":"# TO COMPLETE","metadata":{"id":"J9fSJbOSW0bj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Complete_Dataset(Foundation_Classification):\n    def __init__(self, dataloader, fig=False):\n        super().__init__(dataloader)\n        if not fig:\n            self.complete_fig_init(lr=0.004)\n\n    def complete_fig_init(self, lr=0.001, step_size=3):     \n        self.net = Simple_CNN().to(device)\n        self.loss_fn = nn.CrossEntropyLoss().to(device)\n        self.optimizer = optim.Adam(self.net.parameters(), lr=lr)\n        self.step_size = step_size\n        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=step_size, gamma=0.5)\n\n    def training(self, epoch_max, k_fold=8, total_step=12):\n\n        validation_size = int(self.epoch_size / k_fold)\n        max_scheduler_step = total_step\n        last_loss = None\n\n        for epoch in range(epoch_max):\n            batch_count = 0\n            train_loss = 0.0\n            validation_loss = 0.0\n            validation_idx1 = (epoch % k_fold) * validation_size\n            validation_idx2 = ((epoch % k_fold) + 1) * validation_size\n\n            for inputs, labels in self.dataloader:\n                if batch_count < validation_idx1 or batch_count >= validation_idx2:\n                    loss = self.train(inputs, labels)\n                    train_loss += loss/ (self.epoch_size - validation_size)\n                else:\n                    loss = self.validate(inputs, labels)\n                    validation_loss += loss/ validation_size\n                batch_count +=1\n\n            if last_loss and last_loss <= train_loss and max_scheduler_step >= 0:\n                self.scheduler.step()\n                max_scheduler_step -= 1\n\n            last_loss = train_loss\n\n            self.stat[\"train_loss\"].append(train_loss)\n            self.stat[\"validation_loss\"].append(validation_loss)\n            if epoch % 10 == 0:\n                print(f\"epoch: {epoch} training loss: {train_loss: .3f} validation loss: {validation_loss: .3f}\")\n            if round(train_loss,3) == 0:\n                break\n            else:\n                epoch += 1\n        torch.save({\"state_dict\": self.net.state_dict()}, self.results_path)","metadata":{"id":"MbP_z9lPW0bj","execution":{"iopub.status.busy":"2022-02-27T17:35:38.372817Z","iopub.execute_input":"2022-02-27T17:35:38.373246Z","iopub.status.idle":"2022-02-27T17:35:38.385168Z","shell.execute_reply.started":"2022-02-27T17:35:38.373210Z","shell.execute_reply":"2022-02-27T17:35:38.384432Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"c = Complete_Dataset(dataloader)\nc.training(300)\nc.plot()","metadata":{"id":"yKUrIqUYW0bk","outputId":"7766010c-71b8-49f0-faca-42f1bcb82310","execution":{"iopub.status.busy":"2022-02-27T17:35:41.242149Z","iopub.execute_input":"2022-02-27T17:35:41.242849Z","iopub.status.idle":"2022-02-27T17:56:02.770556Z","shell.execute_reply.started":"2022-02-27T17:35:41.242812Z","shell.execute_reply":"2022-02-27T17:56:02.769786Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"\n### 1.2.2 Finetuning [6 marks]\n\nNow finetune your architecture by implementing at least 2 methods of reducing overfitting and increasing the model's ability to generalise. You are encouraged to further adjust the model after you have done the minimum requirement, to increase your model performance. Please do not use any pre-trained weights from a model trained on ImageNet.\n","metadata":{"id":"z1AKoQUXW0bk"}},{"cell_type":"markdown","source":"**Method 1:** Data augmentation of your choice","metadata":{"id":"3QpfqOB8W0bk"}},{"cell_type":"markdown","source":"**Method 2:** Adding dropout and/or batch normalisation to the model","metadata":{"id":"nSfel3xJW0bk"}},{"cell_type":"markdown","source":"If you adjust the Model class, redefine it below and instantiate it as ```model_122a```, ```model_122b```, and so on.\n\n","metadata":{"id":"lOFVbI1eW0bk"}},{"cell_type":"code","source":"# TO COMPLETE\nclass model_122a(nn.Sequential):\n  def __init__(self):\n    super().__init__()\n    layers = [    \n      nn.Conv2d(in_channels=3,out_channels=32, kernel_size=3),    \n      nn.ReLU(),\n      nn.Conv2d(in_channels=32,out_channels=32, kernel_size=3),\n      nn.ReLU(),\n      nn.MaxPool2d(kernel_size=2, stride=2),\n      nn.Conv2d(in_channels=32,out_channels=64, kernel_size=3),    \n      nn.ReLU(),\n      nn.Conv2d(in_channels=64,out_channels=64, kernel_size=3),\n      nn.ReLU(),\n      nn.MaxPool2d(kernel_size=2, stride=2),\n      nn.Flatten(),\n      nn.Dropout(p=0.25),\n      nn.Linear(64*13*13,512),\n      nn.ReLU(),\n      nn.Dropout(p=0.25),\n      nn.Linear(512,30),\n      nn.LogSoftmax(dim=1)\n    ]\n    for idx, module in enumerate(layers):\n      self.add_module(str(idx), module)\n\n    \nclass model_122b(nn.Sequential):\n  def __init__(self):\n    super().__init__()\n    layers = [    \n      nn.Conv2d(in_channels=3,out_channels=64, kernel_size=5),    \n      nn.ReLU(),\n      nn.Conv2d(in_channels=64,out_channels=64, kernel_size=5),\n      nn.ReLU(),\n      nn.MaxPool2d(kernel_size=2, stride=2),\n      nn.Conv2d(in_channels=64,out_channels=64, kernel_size=3),\n      nn.ReLU(),\n      nn.Conv2d(in_channels=64,out_channels=64, kernel_size=3),\n      nn.ReLU(),\n      nn.Conv2d(in_channels=64,out_channels=64, kernel_size=3),\n      nn.ReLU(),\n      nn.MaxPool2d(kernel_size=2, stride=2),\n      nn.Conv2d(in_channels=64,out_channels=128, kernel_size=3),\n      nn.ReLU(),\n      nn.Flatten(),\n      nn.Linear(128*9*9,256),\n      nn.ReLU(),\n      nn.Dropout(0.5),\n      nn.Linear(256,256),\n      nn.ReLU(),\n      nn.Dropout(0.5),\n      nn.Linear(256,30),\n      nn.LogSoftmax(dim=1)\n    ]\n    for idx, module in enumerate(layers):\n      self.add_module(str(idx), module)\n","metadata":{"id":"33qx0nIJW0bk","execution":{"iopub.status.busy":"2022-02-27T19:58:07.883736Z","iopub.execute_input":"2022-02-27T19:58:07.884474Z","iopub.status.idle":"2022-02-27T19:58:07.900373Z","shell.execute_reply.started":"2022-02-27T19:58:07.884430Z","shell.execute_reply":"2022-02-27T19:58:07.899648Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"class Fine_tune(Complete_Dataset):\n    def __init__(self, dataloader):\n        super().__init__(dataloader, fig=True)\n        self.fine_fig_init()\n\n    def fine_fig_init(self, lr=0.002, step_size=20):     \n        self.net = model_122a().to(device)\n        self.loss_fn = nn.CrossEntropyLoss().to(device)\n        self.optimizer = optim.Adam(self.net.parameters(), lr=lr)\n        self.step_size = step_size\n        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=step_size, gamma=0.5)\n","metadata":{"id":"PnsgrWiBW0bl","execution":{"iopub.status.busy":"2022-02-27T20:09:01.725707Z","iopub.execute_input":"2022-02-27T20:09:01.726564Z","iopub.status.idle":"2022-02-27T20:09:01.733893Z","shell.execute_reply.started":"2022-02-27T20:09:01.726509Z","shell.execute_reply":"2022-02-27T20:09:01.732972Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"f_transforms = transforms.Compose([\n      transforms.ColorJitter(hue=0.2, saturation=0.2, brightness=0.2),\n      transforms.RandomRotation(degrees=180),\n      transforms.ToTensor()\n    ])\nf_dataset = ImageFolder(root=\"../input/something/train_set/train_set\", transform=f_transforms)\nf_dataloader = DataLoader(f_dataset, batch_size=64, shuffle=True)\nf = Fine_tune(f_dataloader)\nf.training(400, total_step=60)","metadata":{"id":"TzoBVJgjW0bl","outputId":"bc3adfa7-1432-48c9-e73f-8d0fcdb7e16d","execution":{"iopub.status.busy":"2022-02-27T20:09:03.984076Z","iopub.execute_input":"2022-02-27T20:09:03.984462Z","iopub.status.idle":"2022-02-27T23:01:07.289963Z","shell.execute_reply.started":"2022-02-27T20:09:03.984429Z","shell.execute_reply":"2022-02-27T23:01:07.289099Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"# instantiate your new Model class\n# model_122\nf.plot()","metadata":{"id":"IOQeDkcsW0bl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n### 1.2.3 Training comparison [4 marks]\n\nDisplay, side-by-side or on one single graph, the training and validation loss graphs for the single-batch training (section 1.1.3), on the full training set (1.2.1) and your final fine-tuned model (1.2.2). ","metadata":{"id":"32ScB8PFW0bl"}},{"cell_type":"code","source":"def plot_comparsison(s, c, f):\n    fig, axs = plt.subplots(3, sharex=True, sharey=True)\n\n    plt.title('training comparision')\n    plt.xlabel('epoch')\n    plt.ylabel('loss')\n    axs[0].plot(s.stat[\"train_loss\"], 'r', label = 'training loss')\n    axs[0].plot(s.stat[\"validation_loss\"], 'b', label = 'validation_loss')\n\n    axs[1].plot(c.stat[\"train_loss\"], 'r', label = 'training loss')\n    axs[1].plot(c.stat[\"validation_loss\"], 'b', label = 'validation_loss')\n\n    axs[2].plot(f.stat[\"train_loss\"], 'r', label = 'training loss')\n    axs[2].plot(f.stat[\"validation_loss\"], 'b', label = 'validation_loss')\n\n    plt.plot()","metadata":{"id":"qLBiPDhwW0bl","execution":{"iopub.status.busy":"2022-02-27T23:27:09.675233Z","iopub.execute_input":"2022-02-27T23:27:09.675549Z","iopub.status.idle":"2022-02-27T23:27:09.683727Z","shell.execute_reply.started":"2022-02-27T23:27:09.675517Z","shell.execute_reply":"2022-02-27T23:27:09.682791Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"plot_comparsison(s, c, f)","metadata":{"id":"EAxEgSDTW0bl","outputId":"85931b75-f564-442a-e478-885f03c6c131","execution":{"iopub.status.busy":"2022-02-27T23:27:11.759124Z","iopub.execute_input":"2022-02-27T23:27:11.759487Z","iopub.status.idle":"2022-02-27T23:27:11.776740Z","shell.execute_reply.started":"2022-02-27T23:27:11.759453Z","shell.execute_reply":"2022-02-27T23:27:11.775849Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"In the single batch training, the validation loss goes the opposite direction as the training loss reach zero in the relatively short amount of epoch taken.\n\n\nWhile using the complete dataset, the validation loss has similar trends with the corresponding training loss. Nevertheless, there is a shape reduction in both loss curves in the early stage of the training process.\n\n\nWith the implementation of finetuning, the training loss and validation loss go down with a smoother curve. In contrast, those two loss curves do not reach zero and fluctuate at a float number close to zero.\n\n","metadata":{"id":"VXX8UHi9W0bm"}},{"cell_type":"markdown","source":"\n### 1.2.4 Confusion matrices [7 marks]\n\nUse your architecture with best accuracy to generate two confusion matrices, one for the training set and one for the validation set. Remember to use the whole validation and training sets, and to include all your relevant code. Display the confusion matrices in a meaningful way which clearly indicates what percentage of the data is represented in each position.\n\n","metadata":{"id":"61XYAMVjW0bm"}},{"cell_type":"code","source":"# TO COMPLETE\ndef plot_conf(model, dataloader, figsize=(25,10), train=True) :\n    validation_batch = int(211 * (1 - 0.2))\n    train_pred = np.array([])\n    train_true = np.array([])\n    validation_pred = np.array([])\n    validation_true = np.array([])\n    batch_count = 0\n    with torch.no_grad():\n        for inputs, labels in dataloader:\n            outputs = model.net(inputs.to(device)).to(device)\n            _, pred = torch.max(outputs.data, 1)\n            if batch_count <= validation_batch:\n                train_pred = np.append(train_pred, pred)\n                train_true = np.append(train_true, labels)\n            else:\n                validation_pred = np.append(validation_pred, pred)\n                validation_true = np.append(validation_true, labels)\n            batch_count +=1\n\n    if train:\n        cm = confusion_matrix(train_true, train_pred)\n        fig_name = \"training\"\n    else:\n        cm = confusion_matrix(validation_true, validation_pred)\n        fig_name = \"validation\"\n        \n    fig, ax = plt.subplots(figsize=figsize)         # Sample figsize in inches\n    fig.suptitle(fig_name + \"confusion matrix\")\n    f = sns.heatmap(cm, annot=True, linewidths=.5, ax=ax)\n\nplot_conf(c, dataloader, train=True)","metadata":{"id":"1WQEPddsW0bm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_conf(c, dataloader, train=False)","metadata":{"id":"N6dcqksQW0bm","outputId":"f359bf62-0bb7-40cb-d59f-a975b40367ca"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"What conclusions can be drawn from the confusion matrices?","metadata":{"id":"elOrOU8gW0bm"}},{"cell_type":"markdown","source":"The architecture used in the graph is already reached zero in both training and validation loss. In addition, I use cross-validation to split the train and validation set. In this case, all the inputs are corrected. As a result, the confusion matrix can only show the distribution of the image classes. The confusion matrix is shown as an identity matrix with main diagonal filled with non-zero attributes while others are zero.","metadata":{"id":"DS8a9iyDW0bm"}},{"cell_type":"markdown","source":"\n\n## 1.3 Testing on test data [18 marks]\n\n### 1.3.1 Dataset and generating predictions [6 marks]\n\nCreate a PyTorch ```Dataset``` for the unlabeled test data in the test_set folder of the Kaggle competition and generate predictions using your final model. \n","metadata":{"id":"LXFWH_KPW0bm"}},{"cell_type":"code","source":"# TO COMPLETE","metadata":{"id":"HNktSeKQW0bn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Test_Dataset(Dataset):\n    def __init__(self, main_dir, transform):\n        self.main_dir = main_dir\n        self.transform = transform\n        all_imgs = os.listdir(main_dir)\n        self.total_imgs = natsorted(all_imgs)\n\n    def __len__(self):\n        return len(self.total_imgs)\n\n    def __getitem__(self, idx):\n        img_loc = os.path.join(self.main_dir, self.total_imgs[idx])\n\n        image = Image.open(img_loc).convert(\"RGB\")\n        id = self.total_imgs[idx]\n        tensor_image = self.transform(image)\n        return tensor_image, self.total_imgs[idx]\n","metadata":{"id":"jTOZAvhlW0bn","execution":{"iopub.status.busy":"2022-02-27T17:56:54.131903Z","iopub.execute_input":"2022-02-27T17:56:54.132635Z","iopub.status.idle":"2022-02-27T17:56:54.141263Z","shell.execute_reply.started":"2022-02-27T17:56:54.132599Z","shell.execute_reply":"2022-02-27T17:56:54.140520Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"test_dataset = Test_Dataset(\"../input/something/test_set/test_set\", transform=transforms.ToTensor())\ntest_dataloader = DataLoader(test_dataset)","metadata":{"id":"0cy_f5XfW0bn","execution":{"iopub.status.busy":"2022-02-27T17:57:08.995784Z","iopub.execute_input":"2022-02-27T17:57:08.996585Z","iopub.status.idle":"2022-02-27T17:57:09.239873Z","shell.execute_reply.started":"2022-02-27T17:57:08.996549Z","shell.execute_reply":"2022-02-27T17:57:09.239183Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"\n### 1.3.2 CSV file and test set accuracy [12 marks]\n\nSave all test predictions to a CSV file and submit it to the private class Kaggle competition. **Please save your test CSV file submissions using your student username (the one with letters, ie., ``sc15jb``, not the ID with only numbers)**, for example, `sc15jb.csv`. That will help us to identify your submissions.\n\nThe CSV file must contain only two columns: ‘Id’ and ‘Category’ (predicted class ID) as shown below:\n\n```txt\nId,Category\n28d0f5e9_373c.JPEG,2\nbbe4895f_40bf.JPEG,18\n```\n\nThe ‘Id’ column should include the name of the image. It is important to keep the same name as the one on the test set. Do not include any path, just the name of file (with extension). Your csv file must contain 1501 rows, one for each image on test set and 1 row for the headers.\n\n> You may submit multiple times. We will use your personal top entry for allocating marks for this [10 marks]. The class leaderboard will not affect marking (brownie points!).\n\n","metadata":{"id":"ACX_XSfDW0bn"}},{"cell_type":"code","source":"# TO COMPLETE\nimport csv","metadata":{"id":"HmN8ca3JW0bn","execution":{"iopub.status.busy":"2022-02-27T17:57:14.183660Z","iopub.execute_input":"2022-02-27T17:57:14.183914Z","iopub.status.idle":"2022-02-27T17:57:14.187877Z","shell.execute_reply.started":"2022-02-27T17:57:14.183886Z","shell.execute_reply":"2022-02-27T17:57:14.187059Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"def test(model, dataloader):\n    with open(\"sc212zw.csv\", \"w\") as output_file:\n        output_writer = csv.writer(output_file)\n        output_writer.writerow([\"Id\",\"Category\"])\n\n        for test_inputs, test_id in dataloader:\n            tets_output = model.predict(test_inputs)\n            output_writer.writerow([test_id[0], tets_output])","metadata":{"id":"wHEASjUKW0bn","execution":{"iopub.status.busy":"2022-02-27T17:57:18.847827Z","iopub.execute_input":"2022-02-27T17:57:18.848436Z","iopub.status.idle":"2022-02-27T17:57:18.857800Z","shell.execute_reply.started":"2022-02-27T17:57:18.848383Z","shell.execute_reply":"2022-02-27T17:57:18.857191Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"test(c, test_dataloader)","metadata":{"id":"RuHooT2QW0bn","execution":{"iopub.status.busy":"2022-02-27T17:57:22.317248Z","iopub.execute_input":"2022-02-27T17:57:22.317522Z","iopub.status.idle":"2022-02-27T17:57:28.460436Z","shell.execute_reply.started":"2022-02-27T17:57:22.317493Z","shell.execute_reply":"2022-02-27T17:57:28.459743Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"\n\n## QUESTION 2 [40 marks]\n\n\n\nIn this question, you will visualize the filters and feature maps of a fully-trained CNN (AlexNet) on the full ImageNet 2012 dataset.\n\n> Please do not alter the name of the function or the number and type of its arguments and return values, otherwise the automatic grading function will not work correctly. You are welcome to import other modules (though the simplest solution only requires the ones below).\n","metadata":{"id":"C_sR8Gc04CJ2"}},{"cell_type":"markdown","source":"### **Overview:**\n*   **2.1.1** Extract filters from model: ``fetch_filters(layer_idx, model)``\n*   **2.2.1** Load test image\n*   **2.2.2** Extract feature maps for given test image: ``fetch_feature_maps(image, model)``\n*   **2.2.3** Display feature maps\n*   **2.3.1** Generate Grad-CAM heatmaps: ``generate_heatmap(output, class_id, model, image)``\n*   **2.3.2** Display heatmaps: add code to cell\n*   **2.3.3** Generate heatmaps for failure analysis\n","metadata":{"id":"r-HIUgQ-HK8Y"}},{"cell_type":"markdown","source":"### Loading a pre-trained model\n\nRun the cell below to load an AlexNet model with pre-trained weights.","metadata":{"id":"-gYdjXng4CJ5"}},{"cell_type":"code","source":"model = torch.hub.load('pytorch/vision:v0.6.0', 'alexnet', pretrained=True)\nmodel.eval()","metadata":{"id":"ik9dzD4S4CJ6","outputId":"e5173823-4d22-43f5-9b4c-ae0e18930429","execution":{"iopub.status.busy":"2022-02-28T00:49:19.605531Z","iopub.execute_input":"2022-02-28T00:49:19.606222Z","iopub.status.idle":"2022-02-28T00:49:22.959765Z","shell.execute_reply.started":"2022-02-28T00:49:19.606159Z","shell.execute_reply":"2022-02-28T00:49:22.958730Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"model.features","metadata":{"id":"JL8i4VrlW0bo","outputId":"38cf5112-d76a-4df3-81c3-9effe0eee100","execution":{"iopub.status.busy":"2022-02-27T23:17:06.943045Z","iopub.execute_input":"2022-02-27T23:17:06.943306Z","iopub.status.idle":"2022-02-27T23:17:06.948209Z","shell.execute_reply.started":"2022-02-27T23:17:06.943276Z","shell.execute_reply":"2022-02-27T23:17:06.947563Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"model.features[0]","metadata":{"id":"NP5pqV1MW0bp","outputId":"2c48d190-7f71-4b39-ccdf-9a6b9a93b492","execution":{"iopub.status.busy":"2022-02-27T23:17:09.009701Z","iopub.execute_input":"2022-02-27T23:17:09.010651Z","iopub.status.idle":"2022-02-27T23:17:09.016602Z","shell.execute_reply.started":"2022-02-27T23:17:09.010594Z","shell.execute_reply":"2022-02-27T23:17:09.015892Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"model.features[0].weight.shape","metadata":{"id":"-KNuCmNRW0bp","outputId":"5c6b81bf-0afd-4b32-a150-3d53ec9cf65b","execution":{"iopub.status.busy":"2022-02-27T23:17:10.902151Z","iopub.execute_input":"2022-02-27T23:17:10.902726Z","iopub.status.idle":"2022-02-27T23:17:10.907623Z","shell.execute_reply.started":"2022-02-27T23:17:10.902687Z","shell.execute_reply":"2022-02-27T23:17:10.906951Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"markdown","source":"\n## 2.1 Extract and visualize the filters [6 marks]\n\nIn this section you will extract and visualize the filters from the pre-trained AlexNet.","metadata":{"id":"GXzlLbstCE7f"}},{"cell_type":"markdown","source":"### 2.1.1 Extract filters [4 marks]\n\nComplete the following function ```fetch_filters``` to return all the filters from the convolutional layers at the given index in ```model.features``` (see printed model above for reference). \n\n\n\n\n\n> We will not test the behaviour of your function using invalid indices.\n\n","metadata":{"id":"_vOrqr2J4CJ7"}},{"cell_type":"code","source":"def fetch_filters(layer_idx, model):\n    \"\"\" \n        Args:\n            layer_idx (int): the index of model.features specifying which conv layer\n            model (AlexNet): PyTorch AlexNet object\n        Return:\n            filters (Tensor):      \n    \"\"\"\n    # TO COMPLETE\n    # return filters\n    return model.features[layer_idx].weight","metadata":{"id":"sdbDXckn4CJ8","execution":{"iopub.status.busy":"2022-02-28T00:49:29.865111Z","iopub.execute_input":"2022-02-28T00:49:29.865438Z","iopub.status.idle":"2022-02-28T00:49:29.871772Z","shell.execute_reply.started":"2022-02-28T00:49:29.865407Z","shell.execute_reply":"2022-02-28T00:49:29.870672Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# all the indices of the conv layers\nconv_layer_idx = [0, 3, 6, 8, 10]\n\nfilters = []\n\nfor layer_idx in conv_layer_idx:\n    filters.append(fetch_filters(layer_idx, model))","metadata":{"id":"HNxPI5y-4CJ8","execution":{"iopub.status.busy":"2022-02-27T23:29:47.159864Z","iopub.execute_input":"2022-02-27T23:29:47.160731Z","iopub.status.idle":"2022-02-27T23:29:47.165038Z","shell.execute_reply.started":"2022-02-27T23:29:47.160691Z","shell.execute_reply":"2022-02-27T23:29:47.164204Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"For your testing purposes, the following code blocks test the dimensions of the function output.","metadata":{"id":"PWSpX94c4CKD"}},{"cell_type":"code","source":"filters[0].shape","metadata":{"id":"TXeQtKkK4CKE","outputId":"35ce6aa7-bd58-4498-f441-2b08a7c9c621","execution":{"iopub.status.busy":"2022-02-27T23:17:19.283815Z","iopub.execute_input":"2022-02-27T23:17:19.284452Z","iopub.status.idle":"2022-02-27T23:17:19.289911Z","shell.execute_reply.started":"2022-02-27T23:17:19.284407Z","shell.execute_reply":"2022-02-27T23:17:19.289136Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"assert list(filters[0].shape) == [64, 3, 11, 11]","metadata":{"id":"QZnC8Eth4CKF","execution":{"iopub.status.busy":"2022-02-27T23:17:21.201505Z","iopub.execute_input":"2022-02-27T23:17:21.202079Z","iopub.status.idle":"2022-02-27T23:17:21.206547Z","shell.execute_reply.started":"2022-02-27T23:17:21.202039Z","shell.execute_reply":"2022-02-27T23:17:21.205823Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"markdown","source":"\n\n### 2.1.2 Display filters [2 marks]\n\nThe following code will visualize some of the filters from each layer. Play around with viewing filters at different depths into the network. Note that ```filters[0]``` could be viewed in colour if you prefer, whereas the subsequent layers must be viewed one channel at a time in grayscale. \n\n","metadata":{"id":"ECx7Ktsg4CKG"}},{"cell_type":"code","source":"# limit how many filters to show\nto_show = 16\n\n# compute the dimensions of the plot\nplt_dim = int(math.sqrt(to_show))\n\n# plot the first channel of each filter in a grid\nfor i, filt in enumerate(filters[0].detach().numpy()[:to_show]):\n    plt.subplot(plt_dim, plt_dim, i+1)\n    plt.imshow(filt[0], cmap=\"gray\")\n    plt.axis('off')\nplt.show()","metadata":{"id":"7K6N3ThU4CKG","outputId":"6c9417c4-98a7-4219-9ca0-10f7765faf9d","execution":{"iopub.status.busy":"2022-02-27T23:17:23.543552Z","iopub.execute_input":"2022-02-27T23:17:23.544010Z","iopub.status.idle":"2022-02-27T23:17:24.016413Z","shell.execute_reply.started":"2022-02-27T23:17:23.543972Z","shell.execute_reply":"2022-02-27T23:17:24.015743Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"markdown","source":"\n\n## 2.2 Extract and visualize feature maps [10 marks]\n\nIn this section, you will pass a test image through the AlexNet and extract and visualize the resulting convolutional layer feature maps.\n\nComplete the following code cell to load the test image ```man_bike.JPEG```.\n\n","metadata":{"id":"yCJF9IIF4CKI"}},{"cell_type":"markdown","source":"### 2.2.1 Load test image [1 mark]\n","metadata":{"id":"bVfEgbC4I_dE"}},{"cell_type":"code","source":"# TO COMPLETE\n# ! mkdir image\n# ! cp \"/content/gdrive/MyDrive/Colab Notebooks/man_bike.JPEG\" \"image/man_bike.JPEG\"\n\nim = Image.open(\"../input/something-2/man_bike.JPEG\").convert(\"RGB\")","metadata":{"id":"xypfUN7y4CKI","execution":{"iopub.status.busy":"2022-02-28T00:49:36.054214Z","iopub.execute_input":"2022-02-28T00:49:36.055156Z","iopub.status.idle":"2022-02-28T00:49:36.087462Z","shell.execute_reply.started":"2022-02-28T00:49:36.055093Z","shell.execute_reply":"2022-02-28T00:49:36.086613Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"Run the code cell below to apply the image transformation expected by the model.","metadata":{"id":"aF2t9uOk4CKJ"}},{"cell_type":"code","source":"# ImageNet normalisation values, to apply to the image transform\nnorm_mean = [0.485, 0.456, 0.406]\nnorm_std = [0.229, 0.224, 0.225]\n\ndata_transform = transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(256),\n        transforms.ToTensor(),\n        transforms.Normalize(norm_mean, norm_std),\n    ])\n\nim = data_transform(im)","metadata":{"id":"Jt0tJQsM4CKO","execution":{"iopub.status.busy":"2022-02-28T00:49:37.874802Z","iopub.execute_input":"2022-02-28T00:49:37.875802Z","iopub.status.idle":"2022-02-28T00:49:37.926411Z","shell.execute_reply.started":"2022-02-28T00:49:37.875749Z","shell.execute_reply":"2022-02-28T00:49:37.925360Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"\n### 2.2.2 Extract feature maps [5 marks]\n\nComplete the function below to pass the test image through a single forward pass of the network. We are interested in the outputs of the max pool layers (outputs of conv layers at model.features indices 0, 3, and 10) for best visualization. Note that the input should pass through *every layer* of the model.","metadata":{"id":"5VQKNo384CKP"}},{"cell_type":"code","source":"def fetch_feature_maps(image, model):\n    \"\"\"\n    Args:\n        image (Tensor): a single input image with transform applied\n        model (AlexNet): PyTorch AlexNet object\n        \n    Return:\n        feature_maps (Tensor): all the feature maps from conv layers \n                    at indices 0, 3, and 10 (outputs of the MaxPool layers)\n    \"\"\"\n\n    # TO COMPLETE\n    conv_layer_idx = [2, 5, 11]\n\n    filters = []\n\n    for layer_idx in range(len(model.features)):\n        image = model.features[layer_idx](image)\n        if layer_idx in conv_layer_idx:\n            filters.append(image)\n    return filters","metadata":{"id":"kmqQ_mJ54CKP","execution":{"iopub.status.busy":"2022-02-28T00:49:40.066011Z","iopub.execute_input":"2022-02-28T00:49:40.066604Z","iopub.status.idle":"2022-02-28T00:49:40.074682Z","shell.execute_reply.started":"2022-02-28T00:49:40.066539Z","shell.execute_reply":"2022-02-28T00:49:40.073738Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"feature_maps = fetch_feature_maps(im.unsqueeze(0), model)","metadata":{"id":"ORkRxCVo4CKQ","execution":{"iopub.status.busy":"2022-02-27T23:30:04.030156Z","iopub.execute_input":"2022-02-27T23:30:04.031084Z","iopub.status.idle":"2022-02-27T23:30:04.064332Z","shell.execute_reply.started":"2022-02-27T23:30:04.031036Z","shell.execute_reply":"2022-02-27T23:30:04.063570Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"For your testing purposes, the following code block tests the dimensions of part of the function output. Note that the first dimension is the batch size.","metadata":{"id":"Pf3SZoFu4CKQ"}},{"cell_type":"code","source":"assert len(feature_maps) == 3\nassert list(feature_maps[0].shape) == [1, 64, 31, 31]","metadata":{"id":"Ow7jGdQ94CKR","execution":{"iopub.status.busy":"2022-02-27T23:17:54.758542Z","iopub.execute_input":"2022-02-27T23:17:54.758797Z","iopub.status.idle":"2022-02-27T23:17:54.763201Z","shell.execute_reply.started":"2022-02-27T23:17:54.758768Z","shell.execute_reply":"2022-02-27T23:17:54.762512Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"markdown","source":"\n\n### 2.2.3 Display feature maps [4 marks]\n\nUsing the code for displaying filters as reference, write code in the block below to display the outputs of the first **16 feature maps from each of the 3 max-pool layers**.","metadata":{"id":"yDmwrp-w4CKR"}},{"cell_type":"code","source":"# TO COMPLETE\n# limit how many filters to show\nto_show = 16\n\n# compute the dimensions of the plot\nplt_dim = int(math.sqrt(to_show))\n\nplt.rcParams[\"figure.figsize\"] = (20,20)\n\n# plot the first channel of each filter in a grid\nfor feature_idx in range(3):\n  for i, filt in enumerate(feature_maps[feature_idx][0].detach().numpy()[:to_show]):\n    plt.subplot(3*plt_dim, plt_dim , (16* feature_idx + i+1))\n    plt.imshow(filt, cmap=\"gray\")\n    plt.axis('off')\nplt.show()","metadata":{"id":"Y2O8TZG74CKS","outputId":"f1ffa331-1381-45d9-c045-c80c7aba8e38","execution":{"iopub.status.busy":"2022-02-27T23:17:56.852432Z","iopub.execute_input":"2022-02-27T23:17:56.853129Z","iopub.status.idle":"2022-02-27T23:17:58.730461Z","shell.execute_reply.started":"2022-02-27T23:17:56.853089Z","shell.execute_reply":"2022-02-27T23:17:58.729683Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"markdown","source":"\n\n\n## 2.3 Understanding of filters and feature maps [7 marks]\n\nRespond in detail to the questions below. (Note that all text boxes can be formatted using Markdown if desired).\n\n### 2.3.1 [3 marks]\nDescribe what the three filters at indices 0, 4, and 6 from the first convolutional layer are detecting (reference the corresponding feature maps to support your discussion).\n","metadata":{"id":"BZNGf5WQ4CKG"}},{"cell_type":"markdown","source":"The first filters detect the shape or the outlines of the image. We can recognise the shape of the people in the middle with his bicycle in most of the filters from the first three rows of filters shown above.\n\nProcessing to the second filters, in this level, most of the filters are blurred or shown as mosaic particles. We can still detect the board outline of the people. For example, the filter at the fifth-row third column can see the head as the brightest part and his body in the slightly darker parts. \n\nAs for the last filters, there is an only main focus in each filter. In the filter of the first column, tenth row, the light area at the middle indicts the people.","metadata":{"id":"m9CzUSC9W0bw"}},{"cell_type":"markdown","source":"### 2.3.2 [2 marks]\nDiscuss how the filters change with depth into the network.","metadata":{"id":"lxRVrqVsW0bw"}},{"cell_type":"markdown","source":"The number of filters grows from 64 to 384 in the early stage then decreases from 384 to 256. Less detail is shown in each filter, while each focuses on one specific detail.","metadata":{"id":"fvGQpE3TW0bw"}},{"cell_type":"markdown","source":"### 2.3.3 [2 marks]\nDiscuss how the feature maps change with depth into the network.","metadata":{"id":"_8vsPpLPW0bw"}},{"cell_type":"markdown","source":"The feature maps are the output of filters. That is, the more specific focus, the more blurred image. Inrelvence details are being discarded as going deeper into the networks, and the feature maps are shown as focus on specific details.","metadata":{"id":"VM1zZ90AW0bw"}},{"cell_type":"markdown","source":"\n## 2.4 Gradient-weighted Class Activation Mapping (Grad-CAM) [17 marks]\n\nIn this section, we will explore using Gradient-weighted Class Activation Mapping (Grad-CAM) to generate coarse localization maps highlighting the important regions in the test images guiding the model's prediction. We will continue using the pre-trained AlexNet.\n\n#### Preparation\n>It is recommended to first read the relevant paper [Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization](https://arxiv.org/abs/1610.02391), and refer to relevant course material.\n\n#### The AlexNet class\n\n>To implement Grad-CAM, we need to edit the AlexNet ```module``` class itself, so instead of loading the AlexNet model from ```torch.hub``` as we did above, we will use the official PyTorch AlexNet class code ([taken from here](https://pytorch.org/vision/stable/_modules/torchvision/models/alexnet.html)). In addition to the class definition, there is also a function below called ```alexnet()``` which allows you to specify whether you want the pretrained version or not, and if so, loads the weights. \n\n#### The hook\n\n>[Hooks](https://pytorch.org/tutorials/beginner/former_torchies/nnft_tutorial.html#forward-and-backward-function-hooks) in PyTorch are functions which can be registered, or attached, to a ```Module``` or ```Tensor```. Hooks can be *forward* hooks or *backward* hooks; forward hooks are called with ```forward()``` and backward hooks with ```backward()```. In the model below, we register a forward hook that saves the **gradients of the activations** to the Tensor output of ```model.features```. The gradients are saved to a class variable so we can easily access them.\n\nCarefully read the code block below. You do not need to add anything to the model.","metadata":{"id":"ZXeO_agI4CKS"}},{"cell_type":"code","source":"# defining where to load the pre-trained weights from\nmodel_urls = {\n    'alexnet': 'https://download.pytorch.org/models/alexnet-owt-7be5be79.pth',\n}\n\n# the class definition\nclass AlexNet(nn.Module):\n\n    def __init__(self, num_classes=1000):\n        super(AlexNet, self).__init__()\n        \n        # a placeholder for storing the gradients\n        self.gradients = None\n        \n        self.features = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n        )\n        \n        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n        self.classifier = nn.Sequential(\n            nn.Dropout(),\n            nn.Linear(256 * 6 * 6, 4096),\n            nn.ReLU(inplace=True),\n            nn.Dropout(),\n            nn.Linear(4096, 4096),\n            nn.ReLU(inplace=True),\n            nn.Linear(4096, num_classes),\n        )\n        \n    # the hook for the gradients of the activations\n    def activations_hook(self, grad):\n        # stores the gradients of the hook's tensor to our placeholder variable\n        self.gradients = grad\n\n    # a method for extracting the activations of the last conv layer only (when we're \n    # not interested in a full forward pass)\n    def get_activations(self, x):\n        return self.features(x)\n    \n    def forward(self, x):\n        x = self.features(x)\n        \n        # we register the hook here to save the gradients of the last convolutional\n        # layer outputs\n        hook = x.register_hook(self.activations_hook)\n        \n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.classifier(x)\n        return x\n\n\ndef alexnet(pretrained=False, progress=True, **kwargs) -> AlexNet:\n    \"\"\"AlexNet model architecture from the\n    `\"One weird trick...\" <https://arxiv.org/abs/1404.5997>`_ paper.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    \"\"\"\n    model = AlexNet(**kwargs)\n    if pretrained:\n        state_dict = load_state_dict_from_url(model_urls['alexnet'],\n                                              progress=progress)\n        model.load_state_dict(state_dict)\n    return model","metadata":{"id":"74G4wPeG4CKS","execution":{"iopub.status.busy":"2022-02-28T00:49:55.300163Z","iopub.execute_input":"2022-02-28T00:49:55.300718Z","iopub.status.idle":"2022-02-28T00:49:55.319582Z","shell.execute_reply.started":"2022-02-28T00:49:55.300658Z","shell.execute_reply":"2022-02-28T00:49:55.318753Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"model = alexnet(pretrained=True)","metadata":{"id":"8FZEiLFv4CKT","execution":{"iopub.status.busy":"2022-02-28T00:50:12.706602Z","iopub.execute_input":"2022-02-28T00:50:12.706969Z","iopub.status.idle":"2022-02-28T00:50:13.503445Z","shell.execute_reply.started":"2022-02-28T00:50:12.706931Z","shell.execute_reply":"2022-02-28T00:50:13.502437Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# pass our test image through our new model with the hook\noutput = model(im.unsqueeze(0))\n\n# save the predicted class\n_, pred_cls = output.max(dim=1, keepdim=True)","metadata":{"id":"WiBjgrST4CKT","execution":{"iopub.status.busy":"2022-02-28T00:50:27.687447Z","iopub.execute_input":"2022-02-28T00:50:27.688026Z","iopub.status.idle":"2022-02-28T00:50:27.895175Z","shell.execute_reply.started":"2022-02-28T00:50:27.687984Z","shell.execute_reply":"2022-02-28T00:50:27.894013Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"Examine and understand the values stored in ```output``` and ```pred_cls```. What does AlexNet classify the test image as?","metadata":{"id":"GzCYhGyI4CKT"}},{"cell_type":"code","source":"pred_cls","metadata":{"id":"TU5QjsuP4CKU","outputId":"00f29596-e2a8-48c8-a8be-ceed6813d929","execution":{"iopub.status.busy":"2022-02-28T00:15:31.307009Z","iopub.execute_input":"2022-02-28T00:15:31.307472Z","iopub.status.idle":"2022-02-28T00:15:31.316094Z","shell.execute_reply.started":"2022-02-28T00:15:31.307433Z","shell.execute_reply":"2022-02-28T00:15:31.315282Z"},"trusted":true},"execution_count":102,"outputs":[]},{"cell_type":"markdown","source":"### 2.4.1 Generate Grad-CAM heatmaps [8 marks]\n\nWith the hooks in place, now implement the code to generate Grad-CAM heatmaps, by following the guiding comments in the code block below.","metadata":{"id":"Mr4svXcg4CKU"}},{"cell_type":"code","source":"def generate_heatmap(output, class_id, model, image):\n    \n    # 1. compute the gradient of the score for the predicted class (logit)\n    # with respect to the feature map activations of the last convolutional layer\n    # Hint: calling .backward() on a Tensor computes its gradients\n    # TO COMPLETE\n    loss = nn.CrossEntropyLoss()(output, class_id[0]).backward()\n\n    # 2. get the gradients from the model placeholder variable\n    # TO COMPLETE\n\n    gradients = model.gradients\n\n    assert list(gradients.shape) == [1, 256, 7, 7]\n    \n    # pool the gradients across the channels\n    pooled_gradients = torch.mean(gradients, dim=[0, 2, 3])\n    assert list(pooled_gradients.shape) == [256]\n    \n    # 3. get the activations of the last convolutional layer\n    # TO COMPLETE\n    activations = model.get_activations(image)\n\n    assert list(activations.shape) == [1, 256, 7, 7]\n    \n    # 4. weight (multiply) the channels (dim=1 of activations) by the corresponding\n    # gradients (pooled_gradients)\n    # TO COMPLETE\n\n\n    # average the channels of the activations and squeeze out the extra dimension\n    heatmap = torch.mean(activations, dim=1).squeeze()\n    assert list(heatmap.shape) == [7, 7]\n    \n    # 5. apply a ReLU to the linear combination of maps because we are only \n    # interested in the features that have a positive influence on the class of \n    # interest, i.e. pixels whose intensity should be increased in order to increase y\n    # Hint: you can use np.maximum() and torch.max() to perform ReLU if you prefer.\n\n    zeros = torch.tensor(np.zeros([7,7]))\n    heatmap = torch.max(heatmap, zeros)\n    \n    # TO COMPLETE\n    return heatmap","metadata":{"id":"Rdlr69tw4CKU","execution":{"iopub.status.busy":"2022-02-28T00:50:19.770655Z","iopub.execute_input":"2022-02-28T00:50:19.771077Z","iopub.status.idle":"2022-02-28T00:50:19.782779Z","shell.execute_reply.started":"2022-02-28T00:50:19.771034Z","shell.execute_reply":"2022-02-28T00:50:19.781361Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"heatmap = generate_heatmap(output, pred_cls, model, im.unsqueeze(0))","metadata":{"id":"4PEmi-aM4CKU","execution":{"iopub.status.busy":"2022-02-28T00:50:31.288894Z","iopub.execute_input":"2022-02-28T00:50:31.289307Z","iopub.status.idle":"2022-02-28T00:50:31.484620Z","shell.execute_reply.started":"2022-02-28T00:50:31.289259Z","shell.execute_reply":"2022-02-28T00:50:31.483556Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"Check the dimensions of ```heatmap```. Do they make sense?","metadata":{"id":"eaiH3MIO4CKV"}},{"cell_type":"code","source":"heatmap.shape","metadata":{"id":"qigb0A9F4CKV","outputId":"d943e61b-583f-4ff6-ffaa-5abb5a18f3df","execution":{"iopub.status.busy":"2022-02-27T23:18:26.610174Z","iopub.execute_input":"2022-02-27T23:18:26.611104Z","iopub.status.idle":"2022-02-27T23:18:26.617022Z","shell.execute_reply.started":"2022-02-27T23:18:26.611051Z","shell.execute_reply":"2022-02-27T23:18:26.616159Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"markdown","source":"### 2.4.2 Display heatmaps [4 marks]\n\nDisplay ```heatmap``` as a coloured heatmap super-imposed onto the original image. To get results as shown in the paper, we recommend the following steps:\n\n1. Resize the heatmap to match the size of the image.\n2. Rescale the image to a 0-255 integer range.\n3. Apply a colormap to the heatmap using ```cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)```.\n4. Multiply all values of heatmap by 0.4 to reduce colour saturation.\n5. Superimpose the heatmap onto the original image (Note: please perform cv2's addition - addition of two cv2 images, not numpy addition. See [here](https://opencv24-python-tutorials.readthedocs.io/en/latest/py_tutorials/py_core/py_image_arithmetics/py_image_arithmetics.html#:~:text=addWeighted()%20etc.-,Image%20Addition,OpenCV%20addition%20and%20Numpy%20addition.) for explanation.)\n6. Normalize the image between 0-255 again.\n7. Display the resulting image.","metadata":{"id":"rvyCYUpw4CKW"}},{"cell_type":"code","source":"# TO COMPLETE\nimport torchvision.transforms as T\n# from google.colab.patches import cv2_imshow\n\ndef display(heatmap, image):\n    rescale_hm = heatmap / torch.max(heatmap) * 256\n\n    resize_im = T.Resize([image.shape[0],image.shape[1]])(T.ToPILImage()(rescale_hm.detach().numpy().astype(np.uint8)))\n    resize_hm = T.ToTensor()(resize_im)\n\n    rescale_hm_2 = (1 - resize_hm / torch.max(resize_hm)) * 256\n#     rescale_hm_2 = resize_hm / torch.max(resize_hm) * 256\n\n\n    modified_hm = (rescale_hm_2).numpy().astype(np.uint8)\n    \n    colormap_hm = cv2.applyColorMap(modified_hm[0], cv2.COLORMAP_JET)\n\n    merge_img = image.copy()\n    alpha = 0.4\n    cv2.addWeighted(image, alpha, merge_img, 1-alpha, 0, merge_img)\n    cv2.addWeighted(colormap_hm, alpha, merge_img, 1-alpha, 0, merge_img)\n    plt.matshow(merge_img)\n","metadata":{"id":"GwTIJoh84CKX","execution":{"iopub.status.busy":"2022-02-28T00:11:46.419438Z","iopub.execute_input":"2022-02-28T00:11:46.420061Z","iopub.status.idle":"2022-02-28T00:11:46.429170Z","shell.execute_reply.started":"2022-02-28T00:11:46.420018Z","shell.execute_reply":"2022-02-28T00:11:46.428212Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"markdown","source":"Show the heatmap for class ```'seashore, coast, seacoast, sea-coast'``` (```class_id = 978```), super-imposed onto the original image.","metadata":{"id":"Nf9aa-2V4CKX"}},{"cell_type":"code","source":"# TO COMPLETE\nim = Image.open(\"../input/something-2/man_bike.JPEG\").convert(\"RGB\")\nim = data_transform(im)\ndisplay(generate_heatmap(model(im.unsqueeze(0)), torch.tensor([[978]]), model, im.unsqueeze(0)), cv2.imread(\"../input/something-2/man_bike.JPEG\"))\n# display(heatmap, cv2.imread(\"image/man_bike.JPEG\"))","metadata":{"id":"IeEeEQPrW0by","outputId":"3302b91a-ff98-40e1-f014-fa9ff1b73675","execution":{"iopub.status.busy":"2022-02-28T00:14:25.418914Z","iopub.execute_input":"2022-02-28T00:14:25.419915Z","iopub.status.idle":"2022-02-28T00:14:26.002778Z","shell.execute_reply.started":"2022-02-28T00:14:25.419861Z","shell.execute_reply":"2022-02-28T00:14:26.001988Z"},"trusted":true},"execution_count":94,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"RQUx0x41W0bz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.4.3 Failure analysis using Grad-CAM [5 marks]\n\nFind an image (online, or from ImageNet or another dataset) which AlexNet classifies *incorrectly*. Display the image below, and show the model's predicted class. Then, generate the Grad-CAM heatmap and display it super-imposed onto the image.","metadata":{"id":"WbmpeRnjW0bz"}},{"cell_type":"code","source":"# TO COMPLETE\nf_dir = \"../input/something-3/saint_j.jpg\"\n\nf_im = Image.open(f_dir).convert(\"RGB\")\nf_im = data_transform(f_im)\n\noutput = model(f_im.unsqueeze(0))\n_, pred_cls = output.max(dim=1, keepdim=True)\n\npred_cls","metadata":{"id":"Y0vOVQqYW0bz","execution":{"iopub.status.busy":"2022-02-28T00:13:46.618455Z","iopub.execute_input":"2022-02-28T00:13:46.618711Z","iopub.status.idle":"2022-02-28T00:13:46.752549Z","shell.execute_reply.started":"2022-02-28T00:13:46.618682Z","shell.execute_reply":"2022-02-28T00:13:46.751839Z"},"trusted":true},"execution_count":91,"outputs":[]},{"cell_type":"code","source":"display(generate_heatmap(model(f_im.unsqueeze(0)), pred_cls, model, f_im.unsqueeze(0)), cv2.imread(f_dir))","metadata":{"id":"-vIYaU-NW0bz","execution":{"iopub.status.busy":"2022-02-28T00:13:49.493732Z","iopub.execute_input":"2022-02-28T00:13:49.494297Z","iopub.status.idle":"2022-02-28T00:13:50.291179Z","shell.execute_reply.started":"2022-02-28T00:13:49.494256Z","shell.execute_reply":"2022-02-28T00:13:50.287986Z"},"trusted":true},"execution_count":92,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"jBc0P7WqW0bz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Briefly describe what explanation the Grad-CAM heatmap provides about why the model has failed to correctly classify your test image.","metadata":{"id":"gauvu5RdW0bz"}},{"cell_type":"markdown","source":"--> Double click to respond herefilters","metadata":{"id":"ylDsVa18W0bz"}},{"cell_type":"code","source":"","metadata":{"id":"etaikfQPW0bz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3 Overall quality [2 marks]\n\nMarks awarded for overall degree of code readibility and omission of unnecessary messy outupts (for example, please avoid printed losses for every batch of a long training process, large numpy arrays, etc.) throughout the work.","metadata":{"id":"tUq5aCULW0bz"}},{"cell_type":"markdown","source":"**Please refer to the submission section at the top of this notebook to prepare your submission.**\n","metadata":{"id":"I1NWRA2QW0b0"}},{"cell_type":"code","source":"","metadata":{"id":"Dp-DrHRxW0b0"},"execution_count":null,"outputs":[]}]}